{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Conv2d:\n",
    "    def __init__(self, F, C, FH, FW, P, S, initializer=None, optimizer=None, activation=None):\n",
    "        self.F = F  # Number of filters\n",
    "        self.C = C  # Number of input channels\n",
    "        self.FH = FH # Filter height\n",
    "        self.FW = FW # Filter width\n",
    "        self.P = P  # Padding size\n",
    "        self.S = S  # Stride size\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.W = self.initializer.W(F, C, FH, FW) if initializer else np.random.randn(F, C, FH, FW) * 0.01\n",
    "        self.B = self.initializer.B(F) if initializer else np.zeros(F)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.dB = np.zeros_like(self.B)\n",
    "        self.X = None\n",
    "\n",
    "    def output_shape2d(self, H, W):\n",
    "        OH = (H + 2 * self.P - self.FH) // self.S + 1\n",
    "        OW = (W + 2 * self.P - self.FW) // self.S + 1\n",
    "        return OH, OW\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        N, C, H, W = X.shape\n",
    "        OH, OW = self.output_shape2d(H, W)\n",
    "        A = np.zeros((N, self.F, OH, OW))\n",
    "        X_padded = np.pad(X, ((0, 0), (0, 0), (self.P, self.P), (self.P, self.P)), 'constant')\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(self.F):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h_start = oh * self.S\n",
    "                        h_end = h_start + self.FH\n",
    "                        w_start = ow * self.S\n",
    "                        w_end = w_start + self.FW\n",
    "                        A[n, f, oh, ow] = np.sum(X_padded[n, :, h_start:h_end, w_start:w_end] * self.W[f, :, :, :]) + self.B[f]\n",
    "\n",
    "        return self.activation.forward(A) if self.activation else A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        if self.activation:\n",
    "            dA = self.activation.backward(dA)\n",
    "\n",
    "        N, F, OH, OW = dA.shape\n",
    "        C = self.C\n",
    "        H, W = self.X.shape[2:]\n",
    "        FH, FW = self.FH, self.FW\n",
    "        P, S = self.P, self.S\n",
    "\n",
    "        dX = np.zeros_like(self.X)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dB = np.zeros_like(self.B)\n",
    "        X_padded = np.pad(self.X, ((0, 0), (0, 0), (P, P), (P, P)), 'constant')\n",
    "        dX_padded = np.pad(dX, ((0, 0), (0, 0), (P, P), (P, P)), 'constant')\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                for c in range(C):\n",
    "                    for fh in range(FH):\n",
    "                        for fw in range(FW):\n",
    "                            for oh in range(OH):\n",
    "                                for ow in range(OW):\n",
    "                                    dX_padded[n, c, oh * S + fh, ow * S + fw] += dA[n, f, oh, ow] * self.W[f, c, fh, fw]\n",
    "                                    dW[f, c, fh, fw] += dA[n, f, oh, ow] * X_padded[n, c, oh * S + fh, ow * S + fw]\n",
    "                dB[f] += np.sum(dA[n, f, :, :])\n",
    "\n",
    "        if P > 0:\n",
    "            dX = dX_padded[:, :, P:-P, P:-P]\n",
    "        else:\n",
    "            dX = dX_padded\n",
    "\n",
    "        self.dW = dW / N\n",
    "        self.dB = dB / N\n",
    "\n",
    "        if self.optimizer:\n",
    "            self.W = self.optimizer.update(self.W, self.dW)\n",
    "            self.B = self.optimizer.update(self.B, self.dB)\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flowing CNN2 forwards\n",
    "x = np.array([[[[ 1,  2,  3,  4],\n",
    "                [ 5,  6,  7,  8],\n",
    "                [ 9, 10, 11, 12],\n",
    "                [13, 14, 15, 16]]]])\n",
    "\n",
    "#\n",
    "w = np.array([[[[ 0.,  0.,  0.],\n",
    "               [ 0.,  1.,  0.],\n",
    "               [ 0., -1.,  0.]]],\n",
    "\n",
    "              [[[ 0.,  0.,  0.],\n",
    "               [ 0., -1.,  1.],\n",
    "               [ 0.,  0.,  0.]]]])\n",
    "\n",
    "#\n",
    "class SimpleInitializerConv2d:\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "    def W(self, F, C, FH, FW):\n",
    "        return self.sigma * np.random.randn(F,C,FH,FW)\n",
    "    def B(self, F):\n",
    "        return np.zeros(F)\n",
    "\n",
    "# Conv2d layer\n",
    "conv2d = Conv2d(F=2, C=1, FH=3, FW=3, P=0, S=1, initializer=SimpleInitializerConv2d())\n",
    "conv2d.W = w \n",
    "\n",
    "# Forward propagation\n",
    "output_forward = conv2d.forward(x)\n",
    "print(\"Forward Propagation Output:\")\n",
    "print(output_forward)\n",
    "\n",
    "# Backpropagation\n",
    "delta = np.array([[[[ -4,  -4],\n",
    "                   [ 10,  11]]],\n",
    "\n",
    "                  [[[  1,  -7],\n",
    "                   [  1, -11]]]])\n",
    "\n",
    "# \n",
    "delta_reshaped = delta.reshape(1, 2, 2, 2)\n",
    "\n",
    "output_backward = conv2d.backward(delta_reshaped)\n",
    "print(\"\\nBackward Propagation Output (dX):\")\n",
    "print(output_backward)\n",
    "print(\"\\nBackward Propagation dW:\")\n",
    "print(conv2d.dW)\n",
    "print(\"\\nBackward Propagation dB:\")\n",
    "print(conv2d.dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_output_size(input_height, input_width, padding, filter_height, filter_width, stride):\n",
    "    output_height = (input_height + 2 * padding - filter_height) // stride + 1\n",
    "    output_width = (input_width + 2 * padding - filter_width) // stride + 1\n",
    "    return int(output_height), int(output_width)\n",
    "\n",
    "#######\n",
    "input_h, input_w = 6, 6\n",
    "padding_h, padding_w = 0, 0\n",
    "filter_h, filter_w = 3, 3\n",
    "stride_h, stride_w = 1, 1\n",
    "\n",
    "output_h, output_w = calculate_output_size(input_h, input_w, padding_h, filter_h, filter_w, stride_h)\n",
    "print(f\"Output size for input ({input_h}x{input_w}), padding {padding_h}, filter ({filter_h}x{filter_w}), stride {stride_h}: ({output_h}x{output_w})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, pool_size, stride=None):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride if stride is not None else pool_size\n",
    "        self.X = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        N, C, H, W = X.shape\n",
    "        PH, PW = self.pool_size\n",
    "        SH, SW = self.stride\n",
    "        OH = (H - PH) // SH + 1\n",
    "        OW = (W - PW) // SW + 1\n",
    "        out = np.zeros((N, C, OH, OW))\n",
    "        arg_max = np.zeros((N, C, OH, OW, 2), dtype=int)\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h_start = oh * SH\n",
    "                        h_end = h_start + PH\n",
    "                        w_start = ow * SW\n",
    "                        w_end = w_start + PW\n",
    "                        pool_window = X[n, c, h_start:h_end, w_start:w_end]\n",
    "                        max_val = np.max(pool_window)\n",
    "                        out[n, c, oh, ow] = max_val\n",
    "                        arg_max_index = np.unravel_index(np.argmax(pool_window), pool_window.shape)\n",
    "                        arg_max[n, c, oh, ow] = (h_start + arg_max_index[0], w_start + arg_max_index[1])\n",
    "\n",
    "        self.arg_max = arg_max\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dX = np.zeros_like(self.X)\n",
    "        N, C, OH, OW = dout.shape\n",
    "        PH, PW = self.pool_size\n",
    "        SH, SW = self.stride\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h_idx, w_idx = self.arg_max[n, c, oh, ow]\n",
    "                        dX[n, c, h_idx, w_idx] += dout[n, c, oh, ow]\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool2D:\n",
    "    def __init__(self, pool_size, stride=None):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride if stride is not None else pool_size\n",
    "        self.X = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        N, C, H, W = X.shape\n",
    "        PH, PW = self.pool_size\n",
    "        SH, SW = self.stride\n",
    "        OH = (H - PH) // SH + 1\n",
    "        OW = (W - PW) // SW + 1\n",
    "        out = np.zeros((N, C, OH, OW))\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h_start = oh * SH\n",
    "                        h_end = h_start + PH\n",
    "                        w_start = ow * SW\n",
    "                        w_end = ow * SW + PW\n",
    "                        pool_window = X[n, c, h_start:h_end, w_start:w_end]\n",
    "                        avg_val = np.mean(pool_window)\n",
    "                        out[n, c, oh, ow] = avg_val\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dX = np.zeros_like(self.X)\n",
    "        N, C, OH, OW = dout.shape\n",
    "        PH, PW = self.pool_size\n",
    "        SH, SW = self.stride\n",
    "        pool_area = PH * PW\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h_start = oh * SH\n",
    "                        h_end = h_start + PH\n",
    "                        w_start = ow * SW\n",
    "                        w_end = ow * SW + PW\n",
    "                        dX[n, c, h_start:h_end, w_start:w_end] += dout[n, c, oh, ow] / pool_area\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(len(X), -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# \n",
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size=20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = (X.shape[0] + self.batch_size - 1) // self.batch_size  \n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item >= self._stop:\n",
    "            raise IndexError(\"Mini-batch index out of range\")\n",
    "        p0 = item * self.batch_size\n",
    "        p1 = min(p0 + self.batch_size, len(self._X))\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.maximum(0, A)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        return dZ * (self.A > 0)\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        self.Z = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        return self.Z / np.sum(self.Z, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = self.Z * dZ\n",
    "        s = np.sum(dA, axis=1, keepdims=True)\n",
    "        dA -= self.Z * s\n",
    "        return dA\n",
    "\n",
    "class FC:\n",
    "    def __init__(self, n_in, n_out, initializer, optimizer, activation):\n",
    "        self.W = initializer.W(n_in, n_out)\n",
    "        self.B = initializer.B(n_out)\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = np.dot(X, self.W) + self.B\n",
    "        return self.activation.forward(self.Z)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        dZ = self.activation.backward(dA)\n",
    "        self.dW = np.dot(self.X.T, dZ)\n",
    "        self.dB = np.sum(dZ, axis=0)\n",
    "        dX = np.dot(dZ, self.W.T)\n",
    "        self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "class HeInitializer:\n",
    "    def W(self, *args):\n",
    "        if len(args) == 2:\n",
    "            n_in, n_out = args\n",
    "            return np.random.randn(n_in, n_out) * np.sqrt(2 / n_in)\n",
    "        elif len(args) == 4:\n",
    "            F, C, FH, FW = args\n",
    "            return np.random.randn(F, C, FH, FW) * np.sqrt(2 / (C * FH * FW))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid shape\")\n",
    "\n",
    "    def B(self, n_out):\n",
    "        return np.zeros(n_out)\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(len(X), -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.shape)\n",
    "\n",
    "class Conv2d:\n",
    "    def __init__(self, F, C, FH, FW, P, S, initializer=None, optimizer=None, activation=None):\n",
    "        self.F, self.C, self.FH, self.FW, self.P, self.S = F, C, FH, FW, P, S\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.W = initializer.W(F, C, FH, FW)\n",
    "        self.B = initializer.B(F)\n",
    "\n",
    "    def output_shape2d(self, H, W):\n",
    "        OH = (H + 2 * self.P - self.FH) // self.S + 1\n",
    "        OW = (W + 2 * self.P - self.FW) // self.S + 1\n",
    "        return OH, OW\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        N, C, H, W = X.shape\n",
    "        OH, OW = self.output_shape2d(H, W)\n",
    "        A = np.zeros((N, self.F, OH, OW))\n",
    "        X_pad = np.pad(X, ((0, 0), (0, 0), (self.P, self.P), (self.P, self.P)), 'constant')\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(self.F):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h0 = oh * self.S\n",
    "                        w0 = ow * self.S\n",
    "                        window = X_pad[n, :, h0:h0+self.FH, w0:w0+self.FW]\n",
    "                        A[n, f, oh, ow] = np.sum(window * self.W[f]) + self.B[f]\n",
    "\n",
    "        return self.activation.forward(A) if self.activation else A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        if self.activation:\n",
    "            dA = self.activation.backward(dA)\n",
    "\n",
    "        N, F, OH, OW = dA.shape\n",
    "        _, _, H, W = self.X.shape\n",
    "        FH, FW, P, S = self.FH, self.FW, self.P, self.S\n",
    "        dX = np.zeros_like(self.X)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dB = np.zeros_like(self.B)\n",
    "\n",
    "        X_pad = np.pad(self.X, ((0,0), (0,0), (P,P), (P,P)), 'constant')\n",
    "        dX_pad = np.pad(dX, ((0,0), (0,0), (P,P), (P,P)), 'constant')\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h0 = oh * S\n",
    "                        w0 = ow * S\n",
    "                        window = X_pad[n, :, h0:h0+FH, w0:w0+FW]\n",
    "                        dW[f] += dA[n, f, oh, ow] * window\n",
    "                        dX_pad[n, :, h0:h0+FH, w0:w0+FW] += dA[n, f, oh, ow] * self.W[f]\n",
    "                dB[f] += np.sum(dA[n, f])\n",
    "\n",
    "        dX = dX_pad[:, :, P:-P, P:-P] if P > 0 else dX_pad\n",
    "        self.dW = dW / N\n",
    "        self.dB = dB / N\n",
    "        self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "class MaxPool2D:\n",
    "    def __init__(self, pool_size, stride=None):\n",
    "        self.pool_size = (pool_size, pool_size) if isinstance(pool_size, int) else pool_size\n",
    "        self.stride = self.pool_size if stride is None else (stride, stride)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        N, C, H, W = X.shape\n",
    "        PH, PW = self.pool_size\n",
    "        SH, SW = self.stride\n",
    "        OH = (H - PH) // SH + 1\n",
    "        OW = (W - PW) // SW + 1\n",
    "        out = np.zeros((N, C, OH, OW))\n",
    "        self.arg_max = np.zeros((N, C, OH, OW, 2), dtype=int)\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h0 = oh * SH\n",
    "                        w0 = ow * SW\n",
    "                        pool = X[n, c, h0:h0+PH, w0:w0+PW]\n",
    "                        out[n, c, oh, ow] = np.max(pool)\n",
    "                        idx = np.unravel_index(np.argmax(pool), pool.shape)\n",
    "                        self.arg_max[n, c, oh, ow] = (h0 + idx[0], w0 + idx[1])\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dX = np.zeros_like(self.X)\n",
    "        N, C, OH, OW = dout.shape\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h_idx, w_idx = self.arg_max[n, c, oh, ow]\n",
    "                        dX[n, c, h_idx, w_idx] += dout[n, c, oh, ow]\n",
    "        return dX\n",
    "\n",
    "# --- CNN Classifier ---\n",
    "class Scratch2dCNNClassifier():\n",
    "    def __init__(self, CNN, NN, n_epoch=1, batch_size=32, verbose=False):\n",
    "        self.CNN = CNN\n",
    "        self.NN = NN\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        self.log_acc = np.zeros(self.n_epoch)\n",
    "\n",
    "    def loss_function(self, y, yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt * np.log(y + delta))\n",
    "\n",
    "    def accuracy(self, Z, Y):\n",
    "        return accuracy_score(Y, np.argmax(Z, axis=1))\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        n_batches = int(np.ceil(X.shape[0] / self.batch_size))\n",
    "        for epoch in range(self.n_epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            self.loss = 0\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                forward_data = mini_X[:, np.newaxis, :, :]\n",
    "                for layer in self.CNN.values():\n",
    "                    forward_data = layer.forward(forward_data)\n",
    "\n",
    "                flatten = Flatten()\n",
    "                forward_data = flatten.forward(forward_data)\n",
    "\n",
    "                for layer in self.NN.values():\n",
    "                    forward_data = layer.forward(forward_data)\n",
    "\n",
    "                Z = forward_data\n",
    "                dout = (Z - mini_y) / self.batch_size\n",
    "                for layer in reversed(self.NN.values()):\n",
    "                    dout = layer.backward(dout)\n",
    "\n",
    "                dout = flatten.backward(dout)\n",
    "                for layer in reversed(self.CNN.values()):\n",
    "                    dout = layer.backward(dout)\n",
    "\n",
    "                self.loss += self.loss_function(Z, mini_y)\n",
    "\n",
    "            avg_loss = self.loss / n_batches\n",
    "            y_pred_train = self.predict(X)\n",
    "            train_accuracy = self.accuracy(y_pred_train, np.argmax(y, axis=1))\n",
    "            self.log_loss[epoch] = avg_loss\n",
    "            self.log_acc[epoch] = train_accuracy\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epoch}, Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self.predict(X_val)\n",
    "                val_accuracy = self.accuracy(y_pred_val, np.argmax(y_val, axis=1))\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epoch}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_data = X[:, np.newaxis, :, :]\n",
    "        for layer in self.CNN.values():\n",
    "            pred_data = layer.forward(pred_data)\n",
    "        flatten = Flatten()\n",
    "        pred_data = flatten.forward(pred_data)\n",
    "        for layer in self.NN.values():\n",
    "            pred_data = layer.forward(pred_data)\n",
    "        return pred_data\n",
    "\n",
    "####\n",
    "if __name__ == '__main__':\n",
    "    mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
    "    X, y = mnist['data'], mnist['target'].astype(int)\n",
    "\n",
    "    num_samples = 1000\n",
    "    X = X[:num_samples]\n",
    "    y = y[:num_samples]\n",
    "    X = X / 255.0\n",
    "    X = X.reshape(-1, 28, 28)\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    y_onehot = enc.fit_transform(y[:, np.newaxis])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "    initializer = HeInitializer()\n",
    "    optimizer = SGD(lr=0.01)\n",
    "\n",
    "    CNN = {\n",
    "        'conv1': Conv2d(F=6, C=1, FH=5, FW=5, P=2, S=1, initializer=initializer, optimizer=optimizer, activation=ReLU()),\n",
    "        'pool1': MaxPool2D(pool_size=2, stride=2)\n",
    "    }\n",
    "\n",
    "    NN = {\n",
    "        'fc1': FC(n_in=6 * 14 * 14, n_out=100, initializer=initializer, optimizer=optimizer, activation=ReLU()),\n",
    "        'fc2': FC(n_in=100, n_out=10, initializer=initializer, optimizer=optimizer, activation=Softmax())\n",
    "    }\n",
    "\n",
    "    model = Scratch2dCNNClassifier(CNN=CNN, NN=NN, n_epoch=1, batch_size=64, verbose=True)\n",
    "    model.fit(X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#####\n",
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size=20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = (X.shape[0] + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item >= self._stop:\n",
    "            raise IndexError(\"Mini-batch index out of range\")\n",
    "        p0 = item * self.batch_size\n",
    "        p1 = min(p0 + self.batch_size, len(self._X))\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.maximum(0, A)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        return dZ * (self.A > 0)\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        self.Z = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        return self.Z / np.sum(self.Z, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = self.Z * dZ\n",
    "        s = np.sum(dA, axis=1, keepdims=True)\n",
    "        dA -= self.Z * s\n",
    "        return dA\n",
    "\n",
    "class FC:\n",
    "    def __init__(self, n_in, n_out, initializer, optimizer, activation):\n",
    "        self.W = initializer.W(n_in, n_out)\n",
    "        self.B = initializer.B(n_out)\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = np.dot(X, self.W) + self.B\n",
    "        return self.activation.forward(self.Z)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        dZ = self.activation.backward(dA)\n",
    "        self.dW = np.dot(self.X.T, dZ)\n",
    "        self.dB = np.sum(dZ, axis=0)\n",
    "        dX = np.dot(dZ, self.W.T)\n",
    "        self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "class HeInitializer:\n",
    "    def W(self, *args):\n",
    "        if len(args) == 2:\n",
    "            n_in, n_out = args\n",
    "            return np.random.randn(n_in, n_out) * np.sqrt(2 / n_in)\n",
    "        elif len(args) == 4:\n",
    "            F, C, FH, FW = args\n",
    "            return np.random.randn(F, C, FH, FW) * np.sqrt(2 / (C * FH * FW))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid shape\")\n",
    "\n",
    "    def B(self, n_out):\n",
    "        return np.zeros(n_out)\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(len(X), -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.shape)\n",
    "\n",
    "class Conv2d:\n",
    "    def __init__(self, F, C, FH, FW, P, S, initializer=None, optimizer=None, activation=None):\n",
    "        self.F, self.C, self.FH, self.FW, self.P, self.S = F, C, FH, FW, P, S\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.W = initializer.W(F, C, FH, FW)\n",
    "        self.B = initializer.B(F)\n",
    "\n",
    "    def output_shape2d(self, H, W):\n",
    "        OH = (H + 2 * self.P - self.FH) // self.S + 1\n",
    "        OW = (W + 2 * self.P - self.FW) // self.S + 1\n",
    "        return OH, OW\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        N, C, H, W = X.shape\n",
    "        OH, OW = self.output_shape2d(H, W)\n",
    "        A = np.zeros((N, self.F, OH, OW))\n",
    "        X_pad = np.pad(X, ((0, 0), (0, 0), (self.P, self.P), (self.P, self.P)), 'constant')\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(self.F):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h0 = oh * self.S\n",
    "                        w0 = ow * self.S\n",
    "                        window = X_pad[n, :, h0:h0+self.FH, w0:w0+self.FW]\n",
    "                        A[n, f, oh, ow] = np.sum(window * self.W[f]) + self.B[f]\n",
    "\n",
    "        return self.activation.forward(A) if self.activation else A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        if self.activation:\n",
    "            dA = self.activation.backward(dA)\n",
    "\n",
    "        N, F, OH, OW = dA.shape\n",
    "        _, _, H, W = self.X.shape\n",
    "        FH, FW, P, S = self.FH, self.FW, self.P, self.S\n",
    "        dX = np.zeros_like(self.X)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dB = np.zeros_like(self.B)\n",
    "\n",
    "        X_pad = np.pad(self.X, ((0,0), (0,0), (P,P), (P,P)), 'constant')\n",
    "        dX_pad = np.pad(dX, ((0,0), (0,0), (P,P), (P,P)), 'constant')\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h0 = oh * S\n",
    "                        w0 = ow * S\n",
    "                        window = X_pad[n, :, h0:h0+FH, w0:w0+FW]\n",
    "                        dW[f] += dA[n, f, oh, ow] * window\n",
    "                        dX_pad[n, :, h0:h0+FH, w0:w0+FW] += dA[n, f, oh, ow] * self.W[f]\n",
    "                dB[f] += np.sum(dA[n, f])\n",
    "\n",
    "        dX = dX_pad[:, :, P:-P, P:-P] if P > 0 else dX_pad\n",
    "        self.dW = dW / N\n",
    "        self.dB = dB / N\n",
    "        self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "class MaxPool2D:\n",
    "    def __init__(self, pool_size, stride=None):\n",
    "        self.pool_size = (pool_size, pool_size) if isinstance(pool_size, int) else pool_size\n",
    "        self.stride = self.pool_size if stride is None else (stride, stride)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        N, C, H, W = X.shape\n",
    "        PH, PW = self.pool_size\n",
    "        SH, SW = self.stride\n",
    "        OH = (H - PH) // SH + 1\n",
    "        OW = (W - PW) // SW + 1\n",
    "        out = np.zeros((N, C, OH, OW))\n",
    "        self.arg_max = np.zeros((N, C, OH, OW, 2), dtype=int)\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h0 = oh * SH\n",
    "                        w0 = ow * SW\n",
    "                        pool = X[n, c, h0:h0+PH, w0:w0+PW]\n",
    "                        out[n, c, oh, ow] = np.max(pool)\n",
    "                        idx = np.unravel_index(np.argmax(pool), pool.shape)\n",
    "                        self.arg_max[n, c, oh, ow] = (h0 + idx[0], w0 + idx[1])\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dX = np.zeros_like(self.X)\n",
    "        N, C, OH, OW = dout.shape\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for oh in range(OH):\n",
    "                    for ow in range(OW):\n",
    "                        h_idx, w_idx = self.arg_max[n, c, oh, ow]\n",
    "                        dX[n, c, h_idx, w_idx] += dout[n, c, oh, ow]\n",
    "        return dX\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(len(X), -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.shape)\n",
    "\n",
    "###\n",
    "class LeNetCNNClassifier():\n",
    "    def __init__(self, CNN, NN, n_epoch=1, batch_size=32, verbose=False):\n",
    "        self.CNN = CNN\n",
    "        self.NN = NN\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        self.log_acc = np.zeros(self.n_epoch)\n",
    "\n",
    "    def loss_function(self, y, yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt * np.log(y + delta))\n",
    "\n",
    "    def accuracy(self, Z, Y):\n",
    "        return accuracy_score(Y, np.argmax(Z, axis=1))\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        n_batches = int(np.ceil(X.shape[0] / self.batch_size))\n",
    "        for epoch in range(self.n_epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            self.loss = 0\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                forward_data = mini_X[:, np.newaxis, :, :]\n",
    "                for layer in self.CNN.values():\n",
    "                    forward_data = layer.forward(forward_data)\n",
    "\n",
    "                flatten = Flatten()\n",
    "                forward_data = flatten.forward(forward_data)\n",
    "\n",
    "                for layer in self.NN.values():\n",
    "                    forward_data = layer.forward(forward_data)\n",
    "\n",
    "                Z = forward_data\n",
    "                dout = (Z - mini_y) / self.batch_size\n",
    "                for layer in reversed(self.NN.values()):\n",
    "                    dout = layer.backward(dout)\n",
    "\n",
    "                dout = flatten.backward(dout)\n",
    "                for layer in reversed(self.CNN.values()):\n",
    "                    dout = layer.backward(dout)\n",
    "\n",
    "                self.loss += self.loss_function(Z, mini_y)\n",
    "\n",
    "            avg_loss = self.loss / n_batches\n",
    "            y_pred_train = self.predict(X)\n",
    "            train_accuracy = self.accuracy(y_pred_train, np.argmax(y, axis=1))\n",
    "            self.log_loss[epoch] = avg_loss\n",
    "            self.log_acc[epoch] = train_accuracy\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epoch}, Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self.predict(X_val)\n",
    "                val_accuracy = self.accuracy(y_pred_val, np.argmax(y_val, axis=1))\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epoch}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_data = X[:, np.newaxis, :, :]\n",
    "        for layer in self.CNN.values():\n",
    "            pred_data = layer.forward(pred_data)\n",
    "        flatten = Flatten()\n",
    "        pred_data = flatten.forward(pred_data)\n",
    "        for layer in self.NN.values():\n",
    "            pred_data = layer.forward(pred_data)\n",
    "        return pred_data\n",
    "\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    #MNIST data\n",
    "    mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
    "    X, y = mnist['data'], mnist['target'].astype(int)\n",
    "\n",
    "    ####\n",
    "    num_samples = 1000\n",
    "    X = X[:num_samples]\n",
    "    y = y[:num_samples]\n",
    "\n",
    "    ###\n",
    "    X = X / 255.0\n",
    "    X = X.reshape(-1, 28, 28)\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    y_onehot = enc.fit_transform(y[:, np.newaxis])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "    #LeNet architecture\n",
    "    initializer = HeInitializer()\n",
    "    optimizer = SGD(lr=0.01)\n",
    "\n",
    "    CNN_lenet = {\n",
    "        'conv1': Conv2d(F=6, C=1, FH=5, FW=5, P=2, S=1, initializer=initializer, optimizer=optimizer, activation=ReLU()),\n",
    "        'pool1': MaxPool2D(pool_size=2, stride=2),\n",
    "        'conv2': Conv2d(F=16, C=6, FH=5, FW=5, P=0, S=1, initializer=initializer, optimizer=optimizer, activation=ReLU()),\n",
    "        'pool2': MaxPool2D(pool_size=2, stride=2)\n",
    "    }\n",
    "\n",
    "    #\n",
    "    dummy_input = np.zeros((1, 1, 28, 28))\n",
    "    output_cnn = dummy_input\n",
    "    for layer in CNN_lenet.values():\n",
    "        output_cnn = layer.forward(output_cnn)\n",
    "    flatten_size = np.prod(output_cnn.shape[1:])\n",
    "\n",
    "    NN_lenet = {\n",
    "        'fc1': FC(n_in=flatten_size, n_out=120, initializer=initializer, optimizer=optimizer, activation=ReLU()),\n",
    "        'fc2': FC(n_in=120, n_out=84, initializer=initializer, optimizer=optimizer, activation=ReLU()),\n",
    "        'fc3': FC(n_in=84, n_out=10, initializer=initializer, optimizer=optimizer, activation=Softmax())\n",
    "    }\n",
    "\n",
    "    #\n",
    "    lenet_model = LeNetCNNClassifier(CNN=CNN_lenet, NN=NN_lenet, n_epoch=1, batch_size=64, verbose=True)\n",
    "    lenet_model.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    #\n",
    "    y_pred_val_lenet = lenet_model.predict(X_val)\n",
    "    accuracy_lenet = lenet_model.accuracy(y_pred_val_lenet, np.argmax(y_val, axis=1))\n",
    "    print(f\"\\nLeNet Validation Accuracy (1000 samples, 1 epoch): {accuracy_lenet:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both AlexNet and VGG16 are historically significant Convolutional Neural Network (CNN) architectures that played crucial roles in the advancement of deep learning for image recognition. They demonstrated the power of deep CNNs in achieving state-of-the-art results on large-scale image classification tasks, particularly in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\n",
    "\n",
    "AlexNet (2012):\n",
    "\n",
    "Key Contributions:\n",
    "\n",
    "AlexNet is widely considered to be the model that ignited the deep learning revolution in computer vision. It achieved a breakthrough performance in the ILSVRC 2012 competition, significantly outperforming previous methods.\n",
    "\n",
    "Architecture: \n",
    "It consists of 8 layers: 5 convolutional layers and 3 fully connected layers. Notably, it was one of the first CNNs to utilize:\n",
    "\n",
    "ReLU (Rectified Linear Unit) activation function: This non-saturating activation function accelerated training compared to traditional sigmoid and tanh functions.\n",
    "\n",
    "Multiple GPUs: The model was trained on two GPUs in parallel, which was crucial for handling the computational demands of the deeper network and the large ImageNet dataset.\n",
    "\n",
    "Local Response Normalization (LRN): A normalization technique that helped in generalization.\n",
    "\n",
    "Overlapping Pooling: Max pooling layers with overlapping windows, which slightly reduced the error rate.\n",
    "\n",
    "Dropout: A regularization technique applied to the fully connected layers to prevent overfitting.\n",
    "\n",
    "Data Augmentation: Techniques like random cropping, scaling, and horizontal flipping were used to increase the size of the training data and improve the model's robustness.\n",
    "\n",
    "Impact:\n",
    "\n",
    "AlexNet's success demonstrated the effectiveness of deep CNNs for complex image recognition tasks and inspired a surge of research and development in the field.\n",
    "\n",
    "VGG16 (2014):\n",
    "\n",
    "Key Contributions:\n",
    "\n",
    "Developed by the Visual Geometry Group (VGG) at the University of Oxford, VGG16 (and its variant VGG19) emphasized the importance of network depth. It achieved excellent performance in the ILSVRC 2014 competition.\n",
    "\n",
    "Architecture: \n",
    "\n",
    "VGG16 is characterized by its very deep architecture, consisting of 16 layers with learnable weights (13 convolutional layers and 3 fully connected layers). Its main architectural characteristics include:\n",
    "\n",
    "Small Convolutional Filters: It exclusively uses 3x3 convolutional filters throughout the network. Multiple stacked 3x3 convolutions have the same receptive field as larger filters but with more non-linearities and fewer parameters.\n",
    "\n",
    "Max Pooling: 2x2 max pooling layers with a stride of 2 are used for downsampling.\n",
    "\n",
    "Uniformity: The architecture has a very uniform structure, making it relatively easy to understand and implement.\n",
    "\n",
    "Large Number of Parameters: Due to its depth and fully connected layers with a large number of neurons, VGG16 has a significantly larger number of parameters compared to AlexNet.\n",
    "\n",
    "Impact: \n",
    "\n",
    "VGG16 demonstrated that increasing the depth of a CNN can lead to significant improvements in performance. Its simple and effective architecture became a popular choice for many image recognition tasks and served as a foundation for subsequent, even deeper networks. It also became a widely used feature extractor for transfer learning.\n",
    "\n",
    "AlexNet was a pioneering work that brought deep CNNs to the forefront of image recognition, while VGG16 further solidified the importance of depth in CNN architectures through its simple yet effective design. Both models are considered foundational in the history of deep learning for computer vision and have paved the way for more advanced and efficient architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layer Calculations\n",
    "\n",
    "\n",
    "Formulae:\n",
    "\n",
    "Output Height ($O_H$): $\\lfloor \\frac{I_H - F_H + 2P}{S} \\rfloor + 1$\n",
    "\n",
    "Output Width ($O_W$): $\\lfloor \\frac{I_W - F_W + 2P}{S} \\rfloor + 1$\n",
    "\n",
    "Number of Parameters: $(F_H \\times F_W \\times C_{in} + 1) \\times C_{out}$\n",
    "\n",
    "    $F_H$: Filter Height\n",
    "\n",
    "    $F_W$: Filter Width\n",
    "\n",
    "    $C_{in}$: Number of Input Channels\n",
    "\n",
    "    $P$: Padding\n",
    "\n",
    "    $S$: Stride\n",
    "\n",
    "    $C_{out}$: Number of Output Channels (number of filters)\n",
    "\n",
    "    The '+ 1' accounts for the bias term for each filter.\n",
    "\n",
    "Layer 1:\n",
    "\n",
    "Input size: 144 x 144 x 3\n",
    "\n",
    "Filter size: 3 x 3 x 6\n",
    "\n",
    "Stride: 1\n",
    "\n",
    "Padding: none\n",
    "\n",
    "Output size: 142 x 142 x 6\n",
    "\n",
    "Number of Parameters: (3 x 3 x 3 + 1) x 6 = 168\n",
    "\n",
    "Layer 2:\n",
    "\n",
    "Input size: 60 x 60 x 24\n",
    "\n",
    "Filter size: 3 x 3 x 48\n",
    "\n",
    "Stride: 1\n",
    "\n",
    "Padding: none\n",
    "\n",
    "Output size: 58 x 58 x 48\n",
    "\n",
    "Number of Parameters: (3 x 3 x 24 + 1) x 48 = 10416\n",
    "\n",
    "Layer 3:\n",
    "\n",
    "Input size: 20 x 20 x 10\n",
    "\n",
    "Filter size: 3 x 3 x 20\n",
    "\n",
    "Stride: 2\n",
    "\n",
    "Padding: none\n",
    "\n",
    "Output size: 9 x 9 x 20\n",
    "\n",
    "Number of Parameters: (3 x 3 x 10 + 1) x 20 = 1820\n",
    "\n",
    "for Layer 3:\n",
    "\n",
    "The convolution in Layer 3 doesn't perfectly cover the input boundaries due to the stride of 2 and no padding. This can lead to some edge information being missed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why 3x3 Filters Are Commonly Used Instead of Larger Ones Such as 7x7\n",
    "\n",
    "\n",
    "Computational Efficiency: Larger filters (e.g., 7x7) require significantly more computations than smaller ones. For a single convolution operation, a 7x7 filter involves 49 multiplications per location, whereas a 3x3 filter only requires 9. This difference becomes substantial in deep networks with many convolutional layers, impacting training and inference speed.\n",
    "\n",
    "Deeper Networks and Increased Non-linearity: Stacking multiple 3x3 filters can achieve the same effective receptive field as a larger filter but with greater depth. For instance, two 3x3 filters stacked together have a receptive field of 5x5, and three 3x3 filters have a receptive field of 7x7. Deeper networks can learn more complex and hierarchical representations of the input data. Additionally, each convolutional layer is typically followed by a non-linear activation function (like ReLU). Stacking multiple 3x3 filters introduces more non-linearities, enabling the network to learn more intricate features.\n",
    "\n",
    "Fewer Parameters: Using multiple 3x3 filters instead of a single large filter reduces the number of parameters in the network. This helps to mitigate overfitting, especially when dealing with limited training data. For example, consider a convolutional layer with C input channels and C output channels. A single 7x7 filter would require C x 7 x 7 x C parameters, while two stacked 3x3 filters would require 2 x C x 3 x 3 x C parameters. It can be easily shown that the latter has fewer parameters.\n",
    "\n",
    "In essence, 3x3 filters offer a good balance between computational efficiency, representational power, and parameter efficiency.\n",
    "\n",
    "The Effect of a 1x1 Filter with No Height or Width\n",
    "\n",
    "\n",
    "Channel-wise Transformation: A 1x1 filter operates across the depth (channel) dimension of the input. It performs a linear transformation of the input channels, allowing the network to increase or decrease the number of channels.\n",
    "\n",
    "Dimensionality Reduction: 1x1 convolutions can be used to reduce the number of channels, which can help to decrease the computational cost of subsequent layers. For example, if an input has 256 channels, a 1x1 convolution can reduce it to 64 channels before feeding it into a computationally expensive 3x3 convolution. This is a key technique used in architectures like Inception and ResNet.\n",
    "\n",
    "Increased Non-linearity: When used in conjunction with non-linear activation functions, 1x1 convolutions can introduce additional non-linearities into the network. This can enhance the network's ability to learn complex functions.\n",
    "\n",
    "1x1 filters, despite their simplicity, are powerful tools for manipulating the channel dimension, reducing computational cost, and increasing non-linearity in CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
