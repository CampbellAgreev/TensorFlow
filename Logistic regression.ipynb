{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Scratch implementation of logistic regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      Number of iterations\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    bias : bool\n",
    "      False if no bias term is included\n",
    "    verbose : bool\n",
    "      True to output the learning process\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : The following form of ndarray, shape (n_features,)\n",
    "      Parameters\n",
    "    self.loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record losses on training data\n",
    "    self.val_loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record loss on validation data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False):\n",
    "        #hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        #arrays to record loss\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid function\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Add bias term to features\"\"\"\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    def _initialize_weights(self, n_features):\n",
    "        \"\"\"Initialize weights with small random values\"\"\"\n",
    "        return np.random.randn(n_features) * 0.01\n",
    "\n",
    "    def _compute_loss(self, y, y_pred):\n",
    "        \"\"\"Compute binary cross-entropy loss\"\"\"\n",
    "        epsilon = 1e-15  # to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn logistic regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "            if X_val is not None:\n",
    "                X_val = self._add_bias(X_val)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coef_ = self._initialize_weights(n_features)\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            # \n",
    "            z = np.dot(X, self.coef_)\n",
    "            #sigmoid function\n",
    "            y_pred = self._sigmoid(z)\n",
    "            \n",
    "            #gradient\n",
    "            error = y_pred - y\n",
    "            gradient = np.dot(X.T, error) / n_samples\n",
    "            \n",
    "            # Update weights\n",
    "            self.coef_ -= self.lr * gradient\n",
    "            \n",
    "            # loss\n",
    "            self.loss[i] = self._compute_loss(y, y_pred)\n",
    "            \n",
    "            #validation loss if validation data is provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self._sigmoid(np.dot(X_val, self.coef_))\n",
    "                self.val_loss[i] = self._compute_loss(y_val, val_pred)\n",
    "            \n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\", end=\"\")\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    print(f\", Validation Loss = {self.val_loss[i]:.4f}\")\n",
    "                else:\n",
    "                    print()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the probability using logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples,)\n",
    "            Estimated probability by logistic regression\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "        return self._sigmoid(np.dot(X, self.coef_))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Estimate the label using logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        threshold : float\n",
    "            Threshold for classification (default 0.5)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples,)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
