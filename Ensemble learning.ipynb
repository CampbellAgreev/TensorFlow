{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# \n",
    "data = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\train.csv\")\n",
    "\n",
    "#features and target\n",
    "X = data[['GrLivArea', 'YearBuilt']]\n",
    "y = data['SalePrice']\n",
    "\n",
    "# Split into train and validation sets (80/20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "#\n",
    "\n",
    "# \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_val_scaled)\n",
    "mse_lr = mean_squared_error(y_val, y_pred_lr)\n",
    "\n",
    "# \n",
    "svr = SVR(kernel='rbf', C=100, gamma=0.1)\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "y_pred_svr = svr.predict(X_val_scaled)\n",
    "mse_svr = mean_squared_error(y_val, y_pred_svr)\n",
    "\n",
    "# \n",
    "dt = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt.predict(X_val_scaled)\n",
    "mse_dt = mean_squared_error(y_val, y_pred_dt)\n",
    "\n",
    "print(f\"\\nIndividual Model Performance:\")\n",
    "print(f\"- Linear Regression MSE: {mse_lr:.2f}\")\n",
    "print(f\"- SVR MSE: {mse_svr:.2f}\")\n",
    "print(f\"- Decision Tree MSE: {mse_dt:.2f}\")\n",
    "\n",
    "# \n",
    "y_pred_avg = (y_pred_lr + y_pred_svr + y_pred_dt) / 3\n",
    "mse_avg = mean_squared_error(y_val, y_pred_avg)\n",
    "\n",
    "# \n",
    "weights_manual = [0.4, 0.3, 0.3]  # Giving more weight to Linear Regression\n",
    "y_pred_weighted = (weights_manual[0]*y_pred_lr + \n",
    "                   weights_manual[1]*y_pred_svr + \n",
    "                   weights_manual[2]*y_pred_dt)\n",
    "mse_weighted = mean_squared_error(y_val, y_pred_weighted)\n",
    "\n",
    "# \n",
    "def blending_mse(weights, preds, true):\n",
    "    blended_pred = np.zeros_like(preds[0])\n",
    "    for w, p in zip(weights, preds):\n",
    "        blended_pred += w * p\n",
    "    return mean_squared_error(true, blended_pred)\n",
    "\n",
    "# Initial guess (equal weights)\n",
    "initial_weights = [1/3, 1/3, 1/3]\n",
    "\n",
    "# weights sum to 1 and each weight between 0 and 1\n",
    "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "bounds = [(0, 1) for _ in range(3)]\n",
    "\n",
    "# \n",
    "preds_val = [y_pred_lr, y_pred_svr, y_pred_dt]\n",
    "\n",
    "#\n",
    "result = minimize(blending_mse, initial_weights, \n",
    "                 args=(preds_val, y_val),\n",
    "                 bounds=bounds,\n",
    "                 constraints=constraints)\n",
    "\n",
    "optimized_weights = result.x\n",
    "y_pred_optimized = (optimized_weights[0]*y_pred_lr + \n",
    "                    optimized_weights[1]*y_pred_svr + \n",
    "                    optimized_weights[2]*y_pred_dt)\n",
    "mse_optimized = mean_squared_error(y_val, y_pred_optimized)\n",
    "\n",
    "# \n",
    "\n",
    "print(f\"- Average Blending MSE: {mse_avg:.2f} (Improvement: {(1 - mse_avg/min(mse_lr, mse_svr, mse_dt))*100:.1f}%)\")\n",
    "print(f\"- Weighted Blending MSE: {mse_weighted:.2f} (Improvement: {(1 - mse_weighted/min(mse_lr, mse_svr, mse_dt))*100:.1f}%)\")\n",
    "print(f\"- Optimized Blending MSE: {mse_optimized:.2f} (Improvement: {(1 - mse_optimized/min(mse_lr, mse_svr, mse_dt))*100:.1f}%)\")\n",
    "print(f\"  Optimized Weights: Linear={optimized_weights[0]:.3f}, SVR={optimized_weights[1]:.3f}, Tree={optimized_weights[2]:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# \n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\train.csv\")\n",
    "\n",
    "#features and target\n",
    "X = data[['GrLivArea', 'YearBuilt']]\n",
    "y = data['SalePrice']\n",
    "\n",
    "# Split into train and validation sets (80/20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# \n",
    "\n",
    "single_tree = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "single_tree.fit(X_train_scaled, y_train)\n",
    "y_pred_single = single_tree.predict(X_val_scaled)\n",
    "mse_single = mean_squared_error(y_val, y_pred_single)\n",
    "print(f\"Single Decision Tree MSE: {mse_single:.2f}\")\n",
    "\n",
    "#Implement Bagging \n",
    "\n",
    "\n",
    "n_estimators = 10  # Number of trees in the ensemble\n",
    "bootstrap_ratio = 0.8  # Size of bootstrap sample relative to original data\n",
    "predictions = []\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    # Create bootstrap sample (random selection with replacement)\n",
    "    n_samples = int(bootstrap_ratio * len(X_train_scaled))\n",
    "    indices = np.random.choice(len(X_train_scaled), size=n_samples, replace=True)\n",
    "    \n",
    "    X_boot = X_train_scaled[indices]\n",
    "    y_boot = y_train.iloc[indices]\n",
    "    \n",
    "    # Train tree on bootstrap sample\n",
    "    tree = DecisionTreeRegressor(\n",
    "        max_depth=5,\n",
    "        random_state=42 + i  # Different seed for each tree\n",
    "    )\n",
    "    tree.fit(X_boot, y_boot)\n",
    "    \n",
    "    # Store predictions on validation set\n",
    "    pred = tree.predict(X_val_scaled)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Average predictions from all trees\n",
    "y_pred_bagging = np.mean(predictions, axis=0)\n",
    "mse_bagging = mean_squared_error(y_val, y_pred_bagging)\n",
    "\n",
    "#\n",
    "print(f\"\\nBagging Results (with {n_estimators} trees):\")\n",
    "print(f\"- Bagging MSE: {mse_bagging:.2f}\")\n",
    "print(f\"- Single Tree MSE: {mse_single:.2f}\")\n",
    "print(f\"Improvement: {(1 - mse_bagging/mse_single)*100:.1f}%\")\n",
    "\n",
    "# \n",
    "print(\"\\nIndividual Tree Performance on Validation Set:\")\n",
    "tree_mses = [mean_squared_error(y_val, pred) for pred in predictions]\n",
    "print(f\"- Best individual tree MSE: {min(tree_mses):.2f}\")\n",
    "print(f\"- Worst individual tree MSE: {max(tree_mses):.2f}\")\n",
    "print(f\"- Average individual tree MSE: {np.mean(tree_mses):.2f}\")\n",
    "print(f\"- Bagging MSE: {mse_bagging:.2f} (better than average by {(1 - mse_bagging/np.mean(tree_mses))*100:.1f}%)\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# \n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\train.csv\")\n",
    "\n",
    "#  features and target\n",
    "X = data[['GrLivArea', 'YearBuilt']]\n",
    "y = data['SalePrice']\n",
    "\n",
    "# Split into train and test sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Configuration\n",
    "K0 = 3  # Number of folds for Stage 0\n",
    "M0 = 2  # Number of models for Stage 0\n",
    "K1 = 2  # Number of folds for Stage 1\n",
    "M1 = 1  # Number of models for Stage 1 \n",
    "\n",
    "#\n",
    "blend_data = np.zeros((len(X_train_scaled), M0))\n",
    "blend_test = np.zeros((len(X_test_scaled), M0))\n",
    "\n",
    "#\n",
    "stage0_models = [\n",
    "    ('LinearRegression', LinearRegression()),\n",
    "    ('DecisionTree', DecisionTreeRegressor(max_depth=5, random_state=42))\n",
    "]\n",
    "\n",
    "# K-fold cross-validation \n",
    "kf = KFold(n_splits=K0, shuffle=True, random_state=42)\n",
    "\n",
    "for m_idx, (m_name, model) in enumerate(stage0_models):\n",
    "    print(f\"\\nTraining {m_name} in Stage 0...\")\n",
    "    test_preds = np.zeros((len(X_test_scaled), K0))\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # \n",
    "        blend_data[val_idx, m_idx] = model.predict(X_val_fold)\n",
    "        \n",
    "        # \n",
    "        test_preds[:, fold_idx] = model.predict(X_test_scaled)\n",
    "    \n",
    "    # \n",
    "    blend_test[:, m_idx] = np.mean(test_preds, axis=1)\n",
    "\n",
    "# \n",
    "meta_model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "\n",
    "# K-fold cross-validation \n",
    "kf1 = KFold(n_splits=K1, shuffle=True, random_state=42)\n",
    "test_preds_stage1 = np.zeros((len(X_test_scaled), K1))\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf1.split(blend_data)):\n",
    "    #\n",
    "    X_meta_train, X_meta_val = blend_data[train_idx], blend_data[val_idx]\n",
    "    y_meta_train, y_meta_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # \n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "    \n",
    "    # \n",
    "    val_pred = meta_model.predict(X_meta_val)\n",
    "    val_mse = mean_squared_error(y_meta_val, val_pred)\n",
    "    print(f\"Stage 1 Fold {fold_idx+1} MSE: {val_mse:.2f}\")\n",
    "    \n",
    "    # \n",
    "    test_preds_stage1[:, fold_idx] = meta_model.predict(blend_test)\n",
    "\n",
    "# \n",
    "final_predictions = np.mean(test_preds_stage1, axis=1)\n",
    "\n",
    "# \n",
    "print(\"\\nSingle Model Performance:\")\n",
    "for m_name, model in stage0_models:\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(f\"- {m_name} MSE: {mse:.2f}\")\n",
    "\n",
    "# Stacking performance\n",
    "stacking_mse = mean_squared_error(y_test, final_predictions)\n",
    "print(f\"\\nStacking Performance:\")\n",
    "print(f\"- Final Stacking MSE: {stacking_mse:.2f}\")\n",
    "\n",
    "# \n",
    "best_single_mse = min(\n",
    "    mean_squared_error(y_test, LinearRegression().fit(X_train_scaled, y_train).predict(X_test_scaled)),\n",
    "    mean_squared_error(y_test, DecisionTreeRegressor(max_depth=5, random_state=42).fit(X_train_scaled, y_train).predict(X_test_scaled))\n",
    ")\n",
    "print(f\"\\nImprovement over best single model: {(1 - stacking_mse/best_single_mse)*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
