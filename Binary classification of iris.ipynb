{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading data\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using  sepal_length and sepal_width for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Binary classification (versicolor vs virginica)\n",
    "X = X[(y == 1) | (y == 2)]\n",
    "y = y[(y == 1) | (y == 2)]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "print(f\"k-NN Accuracy: {knn.score(X_test, y_test):.2f}\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(f\"LogReg Accuracy: {lr.score(X_test, y_test):.2f}\")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "print(f\"SVM Accuracy: {svm.score(X_test, y_test):.2f}\")\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "print(f\"Tree Accuracy: {tree.score(X_test, y_test):.2f}\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "print(f\"RF Accuracy: {rf.score(X_test, y_test):.2f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Plot for each method\n",
    "for clf, title in zip([knn, lr, svm, tree, rf], \n",
    "                      ['k-NN', 'LogReg', 'SVM', 'Decision Tree', 'Random Forest']):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# \n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target_names[iris.target]\n",
    "df_binary = df[df['species'].isin(['versicolor', 'virginica'])].copy()\n",
    "\n",
    "# new features \n",
    "df_binary['petal_sepal_ratio'] = df_binary['petal length (cm)'] / df_binary['sepal length (cm)']\n",
    "df_binary['sepal_petal_area'] = df_binary['sepal length (cm)'] * df_binary['petal length (cm)']\n",
    "\n",
    "# \n",
    "original_features = ['sepal length (cm)', 'petal length (cm)']\n",
    "enhanced_features = original_features + ['petal_sepal_ratio', 'sepal_petal_area']\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_binary[enhanced_features], \n",
    "    df_binary['species'],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_binary, x='sepal length (cm)', y='petal length (cm)', hue='species')\n",
    "plt.title(\"Versicolor vs Virginica (Sepal Length vs Petal Length)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=df_binary, x='species', y='sepal length (cm)')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=df_binary, x='species', y='petal length (cm)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.violinplot(data=df_binary, x='species', y='sepal length (cm)')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.violinplot(data=df_binary, x='species', y='petal length (cm)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# \n",
    "df_binary['petal_sepal_ratio'] = df_binary['petal length (cm)'] / df_binary['sepal length (cm)']\n",
    "\n",
    "# \n",
    "df_binary['sepal_petal_area'] = df_binary['sepal length (cm)'] * df_binary['petal length (cm)']\n",
    "\n",
    "# \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=df_binary, x='species', y='petal_sepal_ratio')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=df_binary, x='species', y='sepal_petal_area')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scale features\n",
    "scaler_orig = StandardScaler().fit(X_train[original_features])\n",
    "scaler_enh = StandardScaler().fit(X_train[enhanced_features])\n",
    "\n",
    "# Transforming features\n",
    "X_train_orig = scaler_orig.transform(X_train[original_features])\n",
    "X_test_orig = scaler_orig.transform(X_test[original_features])\n",
    "\n",
    "X_train_enh = scaler_enh.transform(X_train[enhanced_features])\n",
    "X_test_enh = scaler_enh.transform(X_test[enhanced_features])\n",
    "\n",
    "#models training\n",
    "model_orig = LogisticRegression(max_iter=1000).fit(X_train_orig, y_train)\n",
    "model_enh = LogisticRegression(max_iter=1000).fit(X_train_enh, y_train)\n",
    "\n",
    "#\n",
    "print(f\"Original Accuracy: {model_orig.score(X_test_orig, y_test):.4f}\")\n",
    "print(f\"Enhanced Accuracy: {model_enh.score(X_test_enh, y_test):.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Loading and preparing data\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target_names[iris.target]\n",
    "\n",
    "#binary classification (versicolor vs virginica)\n",
    "df_binary = df[df['species'].isin(['versicolor', 'virginica'])]\n",
    "\n",
    "####\n",
    "df_binary['petal_sepal_ratio'] = df_binary['petal length (cm)'] / df_binary['sepal length (cm)']\n",
    "df_binary['sepal_petal_area'] = df_binary['sepal length (cm)'] * df_binary['petal length (cm)']\n",
    "\n",
    "#Converting to NumPy arrays\n",
    "# Original features\n",
    "X_orig = df_binary[['sepal length (cm)', 'petal length (cm)']].values\n",
    "# Enhanced features\n",
    "X_enh = df_binary[['sepal length (cm)', 'petal length (cm)', \n",
    "                  'petal_sepal_ratio', 'sepal_petal_area']].values\n",
    "y = df_binary['species'].values\n",
    "\n",
    "#Splitting data (75% train, 25% validation)\n",
    "# For original features\n",
    "X_train_orig, X_val_orig, y_train, y_val = train_test_split(\n",
    "    X_orig, y, \n",
    "    test_size=0.25,  # 25% validation\n",
    "    random_state=42,  # for reproducibility\n",
    "    stratify=y       # maintain class balance\n",
    ")\n",
    "\n",
    "# For enhanced features\n",
    "X_train_enh, X_val_enh, _, _ = train_test_split(\n",
    "    X_enh, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "##\n",
    "print(\"Original features:\")\n",
    "print(f\"Train: {X_train_orig.shape}, Validation: {X_val_orig.shape}\\n\")\n",
    "\n",
    "print(\"Enhanced features:\")\n",
    "\n",
    "print(f\"Train: {X_train_enh.shape}, Validation: {X_val_enh.shape}\")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_evaluate(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"Helper function to standardize and evaluate models\"\"\"\n",
    "    # \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    \n",
    "    #\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # \n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # \n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))\n",
    "    val_acc = accuracy_score(y_val, model.predict(X_val_scaled))\n",
    "    \n",
    "    return train_acc, val_acc\n",
    "\n",
    "# Evaluating original features\n",
    "orig_train_acc, orig_val_acc = train_and_evaluate(X_train_orig, X_val_orig, y_train, y_val)\n",
    "\n",
    "# Evaluating enhanced features\n",
    "enh_train_acc, enh_val_acc = train_and_evaluate(X_train_enh, X_val_enh, y_train, y_val)\n",
    "\n",
    "print(\"Original Features (2D):\")\n",
    "print(f\"  Training Accuracy: {orig_train_acc:.4f}\")\n",
    "print(f\"  Validation Accuracy: {orig_val_acc:.4f}\\n\")\n",
    "\n",
    "print(\"Enhanced Features (4D):\")\n",
    "print(f\"  Training Accuracy: {enh_train_acc:.4f}\")\n",
    "print(f\"  Validation Accuracy: {enh_val_acc:.4f}\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def evaluate_knn(X_train, X_val, y_train, y_val, feature_set_name):\n",
    "    \"\"\"Evaluate KNN with different neighbor values\"\"\"\n",
    "    print(f\"\\nEvaluating {feature_set_name} features:\")\n",
    "    \n",
    "    for n in [1, 3, 5]:\n",
    "        # \n",
    "        knn = KNeighborsClassifier(n_neighbors=n)\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        #predictions\n",
    "        y_pred = knn.predict(X_val)\n",
    "        \n",
    "        #accuracy\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        print(f\"{n}-NN Accuracy: {acc:.4f}\")\n",
    "\n",
    "#\n",
    "scaler_orig = StandardScaler().fit(X_train_orig)\n",
    "X_train_orig_std = scaler_orig.transform(X_train_orig)\n",
    "X_val_orig_std = scaler_orig.transform(X_val_orig)\n",
    "\n",
    "scaler_enh = StandardScaler().fit(X_train_enh)\n",
    "X_train_enh_std = scaler_enh.transform(X_train_enh)\n",
    "X_val_enh_std = scaler_enh.transform(X_val_enh)\n",
    "\n",
    "#\n",
    "evaluate_knn(X_train_orig_std, X_val_orig_std, y_train, y_val, \"Original (2D)\")\n",
    "evaluate_knn(X_train_enh_std, X_val_enh_std, y_train, y_val, \"Enhanced (4D)\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, confusion_matrix)\n",
    "\n",
    "def evaluate_model(model, X_val, y_val, positive_class='virginica'):\n",
    "    \"\"\"Calculate and print all evaluation metrics\"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculating metrics\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    prec = precision_score(y_val, y_pred, pos_label=positive_class)\n",
    "    rec = recall_score(y_val, y_pred, pos_label=positive_class)\n",
    "    f1 = f1_score(y_val, y_pred, pos_label=positive_class)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    # results\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Evaluating 3-NN model \n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_enh_std, y_train)\n",
    "print(\"Evaluation for 3-NN with Enhanced Features:\")\n",
    "cm = evaluate_model(knn, X_val_enh_std, y_val)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#\n",
    "def decision_region(X, y, model, step=0.01, title='decision region', \n",
    "                   xlabel='xlabel', ylabel='ylabel', \n",
    "                   target_names=['versicolor', 'virginica']):\n",
    "    \"\"\"\n",
    "    Draw the decision region of a binary classification model with two-dimensional features.\n",
    "    \"\"\"\n",
    "    #markers and colors\n",
    "    scatter_color = ['red', 'blue']\n",
    "    contourf_color = ['pink', 'skyblue']\n",
    "    n_class = 2\n",
    "\n",
    "    # decision boundary\n",
    "    f0_min, f0_max = X[:, 0].min()-0.5, X[:, 0].max()+0.5\n",
    "    f1_min, f1_max = X[:, 1].min()-0.5, X[:, 1].max()+0.5\n",
    "    xx, yy = np.meshgrid(np.arange(f0_min, f0_max, step),\n",
    "                         np.arange(f1_min, f1_max, step))\n",
    "    \n",
    "    # \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = np.where(Z == 'versicolor', 0, 1).reshape(xx.shape)\n",
    "    \n",
    "    # decision regions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=ListedColormap(contourf_color))\n",
    "    plt.contour(xx, yy, Z, colors='y', linewidths=1, alpha=0.5)\n",
    "    \n",
    "    # \n",
    "    for i, class_val in enumerate(['versicolor', 'virginica']):\n",
    "        plt.scatter(X[y == class_val][:, 0], X[y == class_val][:, 1], \n",
    "                    color=scatter_color[i], label=target_names[i], \n",
    "                    edgecolor='black', s=60)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using only sepal length and sepal width\n",
    "y = iris.target_names[iris.target]\n",
    "\n",
    "# versicolor (1) and virginica (2)\n",
    "mask = (y == 'versicolor') | (y == 'virginica')\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Splitting into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, \n",
    "                                                random_state=42, stratify=y)\n",
    "\n",
    "# \n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "\n",
    "#KNN model (3-NN)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "# decision regions\n",
    "print(\"Training Data Decision Regions:\")\n",
    "decision_region(X_train_std, y_train, knn,\n",
    "               title='3-NN Decision Regions (Training Data)',\n",
    "               xlabel='Standardized Sepal Length',\n",
    "               ylabel='Standardized Sepal Width')\n",
    "\n",
    "print(\"\\nValidation Data Decision Regions:\")\n",
    "decision_region(X_val_std, y_val, knn,\n",
    "               title='3-NN Decision Regions (Validation Data)',\n",
    "               xlabel='Standardized Sepal Length',\n",
    "               ylabel='Standardized Sepal Width')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, confusion_matrix)\n",
    "\n",
    "# Loading data\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using sepal length and width\n",
    "y = iris.target_names[iris.target]\n",
    "X = X[(y == 'versicolor') | (y == 'virginica')]\n",
    "y = y[(y == 'versicolor') | (y == 'virginica')]\n",
    "\n",
    "# \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# \n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_val, y_pred),\n",
    "        'Precision': precision_score(y_val, y_pred, pos_label='virginica'),\n",
    "        'Recall': recall_score(y_val, y_pred, pos_label='virginica'),\n",
    "        'F1': f1_score(y_val, y_pred, pos_label='virginica')\n",
    "    }\n",
    "\n",
    "# \n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    results[name] = evaluate_model(model, X_val, y_val)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure()\n",
    "    decision_region(X_val, y_val, model, \n",
    "                   title=f'{name} Decision Regions',\n",
    "                   xlabel='Sepal Length (standardized)',\n",
    "                   ylabel='Sepal Width (standardized)')\n",
    "    plt.show()\n",
    "\n",
    "# \n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # sepal length and width\n",
    "y = iris.target_names[iris.target]\n",
    "X = X[(y == 'versicolor') | (y == 'virginica')]\n",
    "y = y[(y == 'versicolor') | (y == 'virginica')]\n",
    "\n",
    "#\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "#\n",
    "results = {}\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Without standardization\n",
    "    model.fit(X_train, y_train)\n",
    "    acc_raw = accuracy_score(y_val, model.predict(X_val))\n",
    "    \n",
    "    # With standardization\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    acc_scaled = accuracy_score(y_val, model.predict(X_val_scaled))\n",
    "    \n",
    "    results[name] = {\n",
    "        'Raw Accuracy': acc_raw,\n",
    "        'Scaled Accuracy': acc_scaled,\n",
    "        'Difference': acc_scaled - acc_raw\n",
    "    }\n",
    "\n",
    "#comparison table\n",
    "comparison = pd.DataFrame(results).T.sort_values('Difference', ascending=False)\n",
    "print(comparison)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, confusion_matrix, \n",
    "                           classification_report)\n",
    "\n",
    "#\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using sepal length and width for visualization\n",
    "y = iris.target_names[iris.target]\n",
    "\n",
    "#\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "\n",
    "#\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(decision_function_shape='ovo'),  # One-vs-One for multiclass\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Multiclass evaluation function\n",
    "def evaluate_multiclass(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    #metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_val, y_pred),\n",
    "        'Precision (Macro)': precision_score(y_val, y_pred, average='macro'),\n",
    "        'Recall (Macro)': recall_score(y_val, y_pred, average='macro'),\n",
    "        'F1 (Macro)': f1_score(y_val, y_pred, average='macro'),\n",
    "        'Precision (Micro)': precision_score(y_val, y_pred, average='micro'),\n",
    "        'Recall (Micro)': recall_score(y_val, y_pred, average='micro'),\n",
    "        'F1 (Micro)': f1_score(y_val, y_pred, average='micro')\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    return metrics, cm\n",
    "\n",
    "#\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_std, y_train)\n",
    "    metrics, cm = evaluate_multiclass(model, X_val_std, y_val)\n",
    "    results[name] = metrics\n",
    "    \n",
    "    #\n",
    "    print(f\"\\n{name} Classification Report:\")\n",
    "    print(classification_report(y_val, model.predict(X_val_std)))\n",
    "    \n",
    "    #\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', \n",
    "                xticklabels=iris.target_names,\n",
    "                yticklabels=iris.target_names)\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "print(results_df[['Accuracy', 'F1 (Macro)', 'F1 (Micro)']])\n",
    "\n",
    "#\n",
    "def plot_multiclass_decision_regions(X, y, model, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = np.array([np.where(iris.target_names == z)[0][0] for z in Z]).reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.Paired)\n",
    "    \n",
    "    #data points\n",
    "    for i, species in enumerate(iris.target_names):\n",
    "        plt.scatter(X[y == species][:, 0], X[y == species][:, 1], \n",
    "                    color=plt.cm.Paired(i/3.), \n",
    "                    label=species,\n",
    "                    edgecolor='black', s=60)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Sepal Length (standardized)')\n",
    "    plt.ylabel('Sepal Width (standardized)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# \n",
    "for name, model in models.items():\n",
    "    plot_multiclass_decision_regions(\n",
    "        X_val_std, y_val, model,\n",
    "        title=f'{name} Decision Regions'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
