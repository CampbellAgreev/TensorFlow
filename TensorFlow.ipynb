{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "1.Data Loading and Preprocessing\n",
    "\n",
    "   I had to read the dataset. \n",
    "\n",
    "   I needed to clean the data.\n",
    "\n",
    "   I had to select features and target variables.\n",
    "\n",
    "   I needed to convert data types.  \n",
    "\n",
    "   I had to split the data into training, validation, and test sets.  \n",
    "\n",
    "2.Parameter Initialization \n",
    "\n",
    "   I needed to initialize weight matrices and bias vectors. \n",
    "\n",
    "   I had to set appropriate dimensions for each layer.  \n",
    "\n",
    "   I needed to use small random numbers for initialization.  \n",
    "\n",
    "3.Forward Propagation \n",
    "\n",
    "   I had to compute the linear combination for each layer.\n",
    "\n",
    "   I needed to apply activation functions.  \n",
    "\n",
    "   I had to store intermediate values for backpropagation.  \n",
    "\n",
    "4.Loss Function Calculation \n",
    "\n",
    "   I needed to compute the loss between predictions and true labels.  \n",
    "\n",
    "   I had to choose an appropriate loss function.  \n",
    "\n",
    "5.Backpropagation \n",
    "\n",
    "   I needed to calculate gradients using the chain rule.  \n",
    "\n",
    "   I had to propagate errors backward through the network.  \n",
    "\n",
    "6.Parameter Update  \n",
    "\n",
    "   I needed to adjust weights and biases using gradients. \n",
    "\n",
    "   I had to apply the learning rate to updates.  \n",
    "\n",
    "7.Epoch Loop  \n",
    "\n",
    "   I needed to iterate over the entire dataset multiple times. \n",
    "\n",
    "   I had to shuffle data between epochs.  \n",
    "\n",
    "8.Mini-batch Processing  \n",
    "\n",
    "   I needed to split data into smaller batches.  \n",
    "\n",
    "   I had to perform forward and backward passes per batch.  \n",
    "\n",
    "9.Prediction \n",
    "\n",
    "   I had to make predictions on new data. \n",
    "\n",
    "   I needed to run forward propagation without training.  \n",
    "\n",
    "10.Evaluation\n",
    "\n",
    "   I needed to calculate performance metrics.  \n",
    "   \n",
    "   I had to compare results on validation and test sets.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow sample code implementations\n",
    "\n",
    "Data Loading and Preprocessing:\n",
    "\n",
    " Done using pandas (pd.read_csv, filtering) and numpy (type conversion, reshaping) and sklearn (train_test_split) before the TensorFlow graph definition (Lines 14-32).\n",
    "\n",
    "Parameter Initialization:\n",
    "\n",
    " Handled within the example_net function using tf.Variable(tf.random_normal(...)) for weights (weights) and biases (biases) (Lines 103-111). tf.Variable declares trainable parameters within the TensorFlow graph.\n",
    "\n",
    "Forward Propagation:\n",
    "\n",
    " Defined symbolically within the example_net function using TensorFlow operations like tf.matmul (matrix multiplication), tf.add (or +), and tf.nn.relu (activation function) (Lines 113-117). The actual calculation happens later during sess.run(). The definition logits = example_net(X) (Line 120) connects the input placeholder X to the network structure.\n",
    "\n",
    "Loss Function Calculation: \n",
    "\n",
    "Defined symbolically using tf.nn.sigmoid_cross_entropy_with_logits and tf.reduce_mean, assigned to the loss_op variable (Line 123). The calculation is performed during sess.run([loss_op, ...]).\n",
    "\n",
    "Backpropagation: \n",
    "\n",
    "Implicitly handled by TensorFlow. When optimizer.minimize(loss_op) is defined (Line 126), TensorFlow automatically builds the necessary graph nodes to compute gradients.\n",
    "\n",
    "Parameter Update :\n",
    "\n",
    "Also implicitly handled by TensorFlow. Defining the optimizer (e.g., tf.train.AdamOptimizer, Line 125) and the train_op = optimizer.minimize(loss_op) (Line 126) sets up the mechanism. Running sess.run(train_op, ...) executes both backpropagation and the parameter updates according to the chosen optimizer's logic.\n",
    "\n",
    "Epoch Loop: \n",
    "\n",
    "Implemented using a standard Python for loop: for epoch in range(num_epochs): (Line 136).\n",
    "\n",
    "Mini-batch Processing: \n",
    "\n",
    "Achieved using the custom GetMiniBatch class (Lines 34-67) which acts as an iterator, and a nested Python for loop: for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train): (Line 141). Data is fed into the graph using the feed_dict argument in sess.run() (Lines 143, 144).\n",
    "\n",
    "Prediction/Inference: \n",
    "\n",
    "The forward pass is run by calling sess.run(logits, feed_dict={X: ...}) or implicitly when calculating accuracy/loss on validation/test data (sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val}), Line 146). The actual predicted class would require applying tf.sigmoid and thresholding (or tf.argmax for multi-class).\n",
    "\n",
    "Evaluation: \n",
    "\n",
    "Defined symbolically using tf.equal, tf.sign, tf.cast, tf.reduce_mean to create the accuracy operation (Lines 129-131). Calculated by running sess.run(accuracy, feed_dict={...}) on the appropriate data splits (Lines 144, 146, 148).\n",
    "\n",
    "Placeholders: \n",
    "\n",
    "tf.placeholder (Lines 91, 92) defines inputs (X, Y) where actual data (mini-batches) will be fed during execution.\n",
    "\n",
    "Initialization: \n",
    "\n",
    "tf.global_variables_initializer() (Line 134) creates an operation to initialize all tf.Variables. This must be run once at the beginning of the session using sess.run(init) (Line 137).\n",
    "\n",
    "Session: \n",
    "\n",
    "tf.Session() (Line 136) creates the runtime environment to execute the graph operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shapes:\n",
      "X_train_scaled: (105, 4)\n",
      "y_train_one_hot: (105, 3)\n",
      "\n",
      "Network Parameters:\n",
      "Epoch   0, Train Loss: 1.0326, Val Loss: 0.9705, Val Acc: 0.591\n",
      "Epoch   1, Train Loss: 0.9255, Val Loss: 0.8769, Val Acc: 0.682\n",
      "Epoch   2, Train Loss: 0.8352, Val Loss: 0.7885, Val Acc: 0.682\n",
      "Epoch   3, Train Loss: 0.7465, Val Loss: 0.7010, Val Acc: 0.682\n",
      "Epoch   4, Train Loss: 0.6619, Val Loss: 0.6216, Val Acc: 0.682\n",
      "Epoch   5, Train Loss: 0.5863, Val Loss: 0.5536, Val Acc: 0.773\n",
      "Epoch   6, Train Loss: 0.5241, Val Loss: 0.4993, Val Acc: 0.864\n",
      "Epoch   7, Train Loss: 0.4760, Val Loss: 0.4582, Val Acc: 0.909\n",
      "Epoch   8, Train Loss: 0.4391, Val Loss: 0.4253, Val Acc: 0.955\n",
      "Epoch   9, Train Loss: 0.4086, Val Loss: 0.3953, Val Acc: 0.955\n",
      "Epoch  10, Train Loss: 0.3815, Val Loss: 0.3683, Val Acc: 0.955\n",
      "Epoch  11, Train Loss: 0.3558, Val Loss: 0.3413, Val Acc: 0.955\n",
      "Epoch  12, Train Loss: 0.3307, Val Loss: 0.3174, Val Acc: 0.955\n",
      "Epoch  13, Train Loss: 0.3065, Val Loss: 0.2936, Val Acc: 0.955\n",
      "Epoch  14, Train Loss: 0.2837, Val Loss: 0.2726, Val Acc: 0.955\n",
      "Epoch  15, Train Loss: 0.2626, Val Loss: 0.2536, Val Acc: 0.955\n",
      "Epoch  16, Train Loss: 0.2434, Val Loss: 0.2371, Val Acc: 0.955\n",
      "Epoch  17, Train Loss: 0.2260, Val Loss: 0.2219, Val Acc: 0.955\n",
      "Epoch  18, Train Loss: 0.2104, Val Loss: 0.2087, Val Acc: 0.955\n",
      "Epoch  19, Train Loss: 0.1964, Val Loss: 0.1970, Val Acc: 0.955\n",
      "Epoch  20, Train Loss: 0.1837, Val Loss: 0.1868, Val Acc: 0.955\n",
      "Epoch  21, Train Loss: 0.1723, Val Loss: 0.1774, Val Acc: 0.955\n",
      "Epoch  22, Train Loss: 0.1621, Val Loss: 0.1692, Val Acc: 0.955\n",
      "Epoch  23, Train Loss: 0.1528, Val Loss: 0.1616, Val Acc: 0.955\n",
      "Epoch  24, Train Loss: 0.1445, Val Loss: 0.1546, Val Acc: 0.955\n",
      "Epoch  25, Train Loss: 0.1366, Val Loss: 0.1501, Val Acc: 0.955\n",
      "Epoch  26, Train Loss: 0.1300, Val Loss: 0.1460, Val Acc: 0.955\n",
      "Epoch  27, Train Loss: 0.1242, Val Loss: 0.1412, Val Acc: 0.955\n",
      "Epoch  28, Train Loss: 0.1190, Val Loss: 0.1370, Val Acc: 0.955\n",
      "Epoch  29, Train Loss: 0.1144, Val Loss: 0.1332, Val Acc: 0.955\n",
      "Epoch  30, Train Loss: 0.1102, Val Loss: 0.1297, Val Acc: 0.955\n",
      "Epoch  31, Train Loss: 0.1065, Val Loss: 0.1265, Val Acc: 0.955\n",
      "Epoch  32, Train Loss: 0.1031, Val Loss: 0.1235, Val Acc: 0.955\n",
      "Epoch  33, Train Loss: 0.1000, Val Loss: 0.1202, Val Acc: 0.955\n",
      "Epoch  34, Train Loss: 0.0972, Val Loss: 0.1178, Val Acc: 0.955\n",
      "Epoch  35, Train Loss: 0.0945, Val Loss: 0.1155, Val Acc: 0.955\n",
      "Epoch  36, Train Loss: 0.0922, Val Loss: 0.1136, Val Acc: 0.955\n",
      "Epoch  37, Train Loss: 0.0900, Val Loss: 0.1116, Val Acc: 0.955\n",
      "Epoch  38, Train Loss: 0.0882, Val Loss: 0.1099, Val Acc: 0.955\n",
      "Epoch  39, Train Loss: 0.0863, Val Loss: 0.1081, Val Acc: 0.955\n",
      "Epoch  40, Train Loss: 0.0848, Val Loss: 0.1065, Val Acc: 0.955\n",
      "Epoch  41, Train Loss: 0.0832, Val Loss: 0.1052, Val Acc: 0.955\n",
      "Epoch  42, Train Loss: 0.0818, Val Loss: 0.1036, Val Acc: 0.955\n",
      "Epoch  43, Train Loss: 0.0806, Val Loss: 0.1027, Val Acc: 0.955\n",
      "Epoch  44, Train Loss: 0.0794, Val Loss: 0.1011, Val Acc: 0.955\n",
      "Epoch  45, Train Loss: 0.0783, Val Loss: 0.0996, Val Acc: 0.955\n",
      "Epoch  46, Train Loss: 0.0772, Val Loss: 0.0991, Val Acc: 0.955\n",
      "Epoch  47, Train Loss: 0.0763, Val Loss: 0.0977, Val Acc: 0.955\n",
      "Epoch  48, Train Loss: 0.0755, Val Loss: 0.0963, Val Acc: 0.955\n",
      "Epoch  49, Train Loss: 0.0745, Val Loss: 0.0962, Val Acc: 0.955\n",
      "Epoch  50, Train Loss: 0.0738, Val Loss: 0.0946, Val Acc: 0.955\n",
      "Epoch  51, Train Loss: 0.0731, Val Loss: 0.0934, Val Acc: 0.955\n",
      "Epoch  52, Train Loss: 0.0723, Val Loss: 0.0935, Val Acc: 0.955\n",
      "Epoch  53, Train Loss: 0.0716, Val Loss: 0.0925, Val Acc: 0.955\n",
      "Epoch  54, Train Loss: 0.0711, Val Loss: 0.0919, Val Acc: 0.955\n",
      "Epoch  55, Train Loss: 0.0705, Val Loss: 0.0916, Val Acc: 0.955\n",
      "Epoch  56, Train Loss: 0.0700, Val Loss: 0.0902, Val Acc: 0.955\n",
      "Epoch  57, Train Loss: 0.0695, Val Loss: 0.0896, Val Acc: 0.955\n",
      "Epoch  58, Train Loss: 0.0690, Val Loss: 0.0892, Val Acc: 0.955\n",
      "Epoch  59, Train Loss: 0.0685, Val Loss: 0.0888, Val Acc: 0.955\n",
      "Epoch  60, Train Loss: 0.0680, Val Loss: 0.0887, Val Acc: 0.955\n",
      "Epoch  61, Train Loss: 0.0677, Val Loss: 0.0877, Val Acc: 0.955\n",
      "Epoch  62, Train Loss: 0.0672, Val Loss: 0.0877, Val Acc: 0.955\n",
      "Epoch  63, Train Loss: 0.0668, Val Loss: 0.0879, Val Acc: 0.955\n",
      "Epoch  64, Train Loss: 0.0664, Val Loss: 0.0855, Val Acc: 0.955\n",
      "Epoch  65, Train Loss: 0.0665, Val Loss: 0.0835, Val Acc: 0.955\n",
      "Epoch  66, Train Loss: 0.0659, Val Loss: 0.0842, Val Acc: 0.909\n",
      "Epoch  67, Train Loss: 0.0656, Val Loss: 0.0845, Val Acc: 0.955\n",
      "Epoch  68, Train Loss: 0.0650, Val Loss: 0.0856, Val Acc: 0.909\n",
      "Epoch  69, Train Loss: 0.0648, Val Loss: 0.0840, Val Acc: 0.955\n",
      "Epoch  70, Train Loss: 0.0646, Val Loss: 0.0834, Val Acc: 0.909\n",
      "Epoch  71, Train Loss: 0.0643, Val Loss: 0.0835, Val Acc: 0.909\n",
      "Epoch  72, Train Loss: 0.0638, Val Loss: 0.0837, Val Acc: 0.909\n",
      "Epoch  73, Train Loss: 0.0636, Val Loss: 0.0837, Val Acc: 0.909\n",
      "Epoch  74, Train Loss: 0.0634, Val Loss: 0.0824, Val Acc: 0.909\n",
      "Epoch  75, Train Loss: 0.0633, Val Loss: 0.0810, Val Acc: 0.909\n",
      "Epoch  76, Train Loss: 0.0630, Val Loss: 0.0814, Val Acc: 0.909\n",
      "Epoch  77, Train Loss: 0.0627, Val Loss: 0.0817, Val Acc: 0.909\n",
      "Epoch  78, Train Loss: 0.0624, Val Loss: 0.0828, Val Acc: 0.909\n",
      "Epoch  79, Train Loss: 0.0623, Val Loss: 0.0810, Val Acc: 0.909\n",
      "Epoch  80, Train Loss: 0.0622, Val Loss: 0.0799, Val Acc: 0.909\n",
      "Epoch  81, Train Loss: 0.0620, Val Loss: 0.0798, Val Acc: 0.909\n",
      "Epoch  82, Train Loss: 0.0617, Val Loss: 0.0807, Val Acc: 0.909\n",
      "Epoch  83, Train Loss: 0.0615, Val Loss: 0.0814, Val Acc: 0.909\n",
      "Epoch  84, Train Loss: 0.0613, Val Loss: 0.0806, Val Acc: 0.909\n",
      "Epoch  85, Train Loss: 0.0612, Val Loss: 0.0796, Val Acc: 0.909\n",
      "Epoch  86, Train Loss: 0.0610, Val Loss: 0.0802, Val Acc: 0.909\n",
      "Epoch  87, Train Loss: 0.0607, Val Loss: 0.0808, Val Acc: 0.909\n",
      "Epoch  88, Train Loss: 0.0606, Val Loss: 0.0788, Val Acc: 0.909\n",
      "Epoch  89, Train Loss: 0.0606, Val Loss: 0.0769, Val Acc: 0.955\n",
      "Epoch  90, Train Loss: 0.0603, Val Loss: 0.0770, Val Acc: 0.955\n",
      "Epoch  91, Train Loss: 0.0603, Val Loss: 0.0777, Val Acc: 0.955\n",
      "Epoch  92, Train Loss: 0.0599, Val Loss: 0.0789, Val Acc: 0.909\n",
      "Epoch  93, Train Loss: 0.0598, Val Loss: 0.0793, Val Acc: 0.955\n",
      "Epoch  94, Train Loss: 0.0597, Val Loss: 0.0771, Val Acc: 0.955\n",
      "Epoch  95, Train Loss: 0.0599, Val Loss: 0.0751, Val Acc: 0.955\n",
      "Epoch  96, Train Loss: 0.0595, Val Loss: 0.0757, Val Acc: 0.909\n",
      "Epoch  97, Train Loss: 0.0594, Val Loss: 0.0761, Val Acc: 0.955\n",
      "Epoch  98, Train Loss: 0.0591, Val Loss: 0.0772, Val Acc: 0.955\n",
      "Epoch  99, Train Loss: 0.0591, Val Loss: 0.0778, Val Acc: 0.955\n",
      "-----\n",
      "Final Test Accuracy: 0.957\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Multi-class classification of Iris dataset (3 species) using neural network implementation in TensorFlow \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) \n",
    "\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\Iris.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Iris.csv not found.\")\n",
    "    exit()\n",
    "\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "X = np.array(X)\n",
    "y_labels = np.array(y)\n",
    "label_map = {\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2}\n",
    "y_integers = np.array([label_map[label] for label in y_labels]).reshape(-1, 1)\n",
    "\n",
    "# \n",
    "X_train, X_temp, y_train_int, y_temp_int = train_test_split(\n",
    "    X, y_integers, test_size=0.3, random_state=0, stratify=y_integers)\n",
    "X_val, X_test, y_val_int, y_test_int = train_test_split(\n",
    "    X_temp, y_temp_int, test_size=0.5, random_state=0, stratify=y_temp_int)\n",
    "\n",
    "# \n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train_int)\n",
    "y_val_one_hot = enc.transform(y_val_int)\n",
    "y_test_one_hot = enc.transform(y_test_int)\n",
    "\n",
    "#\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data Shapes:\")\n",
    "print(\"X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"y_train_one_hot:\", y_train_one_hot.shape)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train_scaled.shape[1]\n",
    "n_samples = X_train_scaled.shape[0]\n",
    "n_classes = y_train_one_hot.shape[1]\n",
    "\n",
    "print(\"\\nNetwork Parameters:\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train_scaled, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    initializer = tf.glorot_uniform_initializer() \n",
    "\n",
    "    weights = {\n",
    "        'w1': tf.Variable(initializer([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(initializer([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(initializer([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "logits = example_net(X)\n",
    "\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, axis=1), tf.argmax(Y, axis=1))\n",
    "\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss * mini_batch_x.shape[0]\n",
    "\n",
    "        avg_train_loss = total_loss / n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val_scaled, Y: y_val_one_hot})\n",
    "        print(\"Epoch {:>3}, Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.3f}\".format(\n",
    "            epoch, avg_train_loss, val_loss, val_acc))\n",
    "\n",
    "    # \n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test_scaled, Y: y_test_one_hot})\n",
    "    print(\"-----\")\n",
    "    print(\"Final Test Accuracy: {:.3f}\".format(test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shapes:\n",
      "X_train_scaled: (1022, 2)\n",
      "y_train_log: (1022, 1)\n",
      "X_val_scaled: (219, 2)\n",
      "y_val_log: (219, 1)\n",
      "X_test_scaled: (219, 2)\n",
      "y_test_log: (219, 1)\n",
      "Input features: 2\n",
      "Hidden Layer 1 Nodes: 50\n",
      "Hidden Layer 2 Nodes: 100\n",
      "Output Nodes: 1\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 16\n",
      "Epochs: 100\n",
      "Epoch   0, Train Loss (MSE): 114.7908, Val Loss (MSE): 63.8812, Val RMSE: 7.9926\n",
      "Epoch   1, Train Loss (MSE): 16.6778, Val Loss (MSE): 2.8445, Val RMSE: 1.6866\n",
      "Epoch   2, Train Loss (MSE): 2.3029, Val Loss (MSE): 1.8991, Val RMSE: 1.3781\n",
      "Epoch   3, Train Loss (MSE): 1.4513, Val Loss (MSE): 1.0761, Val RMSE: 1.0374\n",
      "Epoch   4, Train Loss (MSE): 0.7478, Val Loss (MSE): 0.5011, Val RMSE: 0.7079\n",
      "Epoch   5, Train Loss (MSE): 0.3439, Val Loss (MSE): 0.2216, Val RMSE: 0.4707\n",
      "Epoch   6, Train Loss (MSE): 0.1656, Val Loss (MSE): 0.1081, Val RMSE: 0.3289\n",
      "Epoch   7, Train Loss (MSE): 0.0935, Val Loss (MSE): 0.0634, Val RMSE: 0.2519\n",
      "Epoch   8, Train Loss (MSE): 0.0654, Val Loss (MSE): 0.0461, Val RMSE: 0.2146\n",
      "Epoch   9, Train Loss (MSE): 0.0547, Val Loss (MSE): 0.0394, Val RMSE: 0.1984\n",
      "Epoch  10, Train Loss (MSE): 0.0507, Val Loss (MSE): 0.0369, Val RMSE: 0.1920\n",
      "Epoch  11, Train Loss (MSE): 0.0492, Val Loss (MSE): 0.0359, Val RMSE: 0.1893\n",
      "Epoch  12, Train Loss (MSE): 0.0486, Val Loss (MSE): 0.0354, Val RMSE: 0.1881\n",
      "Epoch  13, Train Loss (MSE): 0.0482, Val Loss (MSE): 0.0351, Val RMSE: 0.1873\n",
      "Epoch  14, Train Loss (MSE): 0.0479, Val Loss (MSE): 0.0347, Val RMSE: 0.1862\n",
      "Epoch  15, Train Loss (MSE): 0.0475, Val Loss (MSE): 0.0344, Val RMSE: 0.1856\n",
      "Epoch  16, Train Loss (MSE): 0.0472, Val Loss (MSE): 0.0344, Val RMSE: 0.1854\n",
      "Epoch  17, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0343, Val RMSE: 0.1853\n",
      "Epoch  18, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0343, Val RMSE: 0.1853\n",
      "Epoch  19, Train Loss (MSE): 0.0468, Val Loss (MSE): 0.0343, Val RMSE: 0.1853\n",
      "Epoch  20, Train Loss (MSE): 0.0468, Val Loss (MSE): 0.0344, Val RMSE: 0.1854\n",
      "Epoch  21, Train Loss (MSE): 0.0468, Val Loss (MSE): 0.0344, Val RMSE: 0.1855\n",
      "Epoch  22, Train Loss (MSE): 0.0468, Val Loss (MSE): 0.0345, Val RMSE: 0.1858\n",
      "Epoch  23, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0347, Val RMSE: 0.1862\n",
      "Epoch  24, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0348, Val RMSE: 0.1866\n",
      "Epoch  25, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0350, Val RMSE: 0.1871\n",
      "Epoch  26, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0353, Val RMSE: 0.1879\n",
      "Epoch  27, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0355, Val RMSE: 0.1885\n",
      "Epoch  28, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0358, Val RMSE: 0.1893\n",
      "Epoch  29, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0362, Val RMSE: 0.1902\n",
      "Epoch  30, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0365, Val RMSE: 0.1910\n",
      "Epoch  31, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0368, Val RMSE: 0.1919\n",
      "Epoch  32, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0371, Val RMSE: 0.1927\n",
      "Epoch  33, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0374, Val RMSE: 0.1935\n",
      "Epoch  34, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0377, Val RMSE: 0.1943\n",
      "Epoch  35, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0379, Val RMSE: 0.1947\n",
      "Epoch  36, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0382, Val RMSE: 0.1953\n",
      "Epoch  37, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0383, Val RMSE: 0.1958\n",
      "Epoch  38, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0384, Val RMSE: 0.1961\n",
      "Epoch  39, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0385, Val RMSE: 0.1963\n",
      "Epoch  40, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0386, Val RMSE: 0.1964\n",
      "Epoch  41, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0384, Val RMSE: 0.1960\n",
      "Epoch  42, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0384, Val RMSE: 0.1960\n",
      "Epoch  43, Train Loss (MSE): 0.0468, Val Loss (MSE): 0.0384, Val RMSE: 0.1959\n",
      "Epoch  44, Train Loss (MSE): 0.0468, Val Loss (MSE): 0.0382, Val RMSE: 0.1955\n",
      "Epoch  45, Train Loss (MSE): 0.0467, Val Loss (MSE): 0.0382, Val RMSE: 0.1954\n",
      "Epoch  46, Train Loss (MSE): 0.0467, Val Loss (MSE): 0.0381, Val RMSE: 0.1951\n",
      "Epoch  47, Train Loss (MSE): 0.0467, Val Loss (MSE): 0.0381, Val RMSE: 0.1951\n",
      "Epoch  48, Train Loss (MSE): 0.0466, Val Loss (MSE): 0.0380, Val RMSE: 0.1950\n",
      "Epoch  49, Train Loss (MSE): 0.0466, Val Loss (MSE): 0.0379, Val RMSE: 0.1948\n",
      "Epoch  50, Train Loss (MSE): 0.0466, Val Loss (MSE): 0.0380, Val RMSE: 0.1949\n",
      "Epoch  51, Train Loss (MSE): 0.0466, Val Loss (MSE): 0.0380, Val RMSE: 0.1950\n",
      "Epoch  52, Train Loss (MSE): 0.0466, Val Loss (MSE): 0.0381, Val RMSE: 0.1952\n",
      "Epoch  53, Train Loss (MSE): 0.0466, Val Loss (MSE): 0.0383, Val RMSE: 0.1957\n",
      "Epoch  54, Train Loss (MSE): 0.0466, Val Loss (MSE): 0.0385, Val RMSE: 0.1961\n",
      "Epoch  55, Train Loss (MSE): 0.0466, Val Loss (MSE): 0.0387, Val RMSE: 0.1966\n",
      "Epoch  56, Train Loss (MSE): 0.0467, Val Loss (MSE): 0.0388, Val RMSE: 0.1971\n",
      "Epoch  57, Train Loss (MSE): 0.0467, Val Loss (MSE): 0.0393, Val RMSE: 0.1981\n",
      "Epoch  58, Train Loss (MSE): 0.0468, Val Loss (MSE): 0.0394, Val RMSE: 0.1986\n",
      "Epoch  59, Train Loss (MSE): 0.0468, Val Loss (MSE): 0.0397, Val RMSE: 0.1993\n",
      "Epoch  60, Train Loss (MSE): 0.0468, Val Loss (MSE): 0.0399, Val RMSE: 0.1998\n",
      "Epoch  61, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0402, Val RMSE: 0.2005\n",
      "Epoch  62, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0404, Val RMSE: 0.2011\n",
      "Epoch  63, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0407, Val RMSE: 0.2017\n",
      "Epoch  64, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0409, Val RMSE: 0.2022\n",
      "Epoch  65, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0410, Val RMSE: 0.2025\n",
      "Epoch  66, Train Loss (MSE): 0.0469, Val Loss (MSE): 0.0411, Val RMSE: 0.2026\n",
      "Epoch  67, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0413, Val RMSE: 0.2033\n",
      "Epoch  68, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0413, Val RMSE: 0.2033\n",
      "Epoch  69, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0412, Val RMSE: 0.2030\n",
      "Epoch  70, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0413, Val RMSE: 0.2032\n",
      "Epoch  71, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0412, Val RMSE: 0.2030\n",
      "Epoch  72, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0413, Val RMSE: 0.2031\n",
      "Epoch  73, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0413, Val RMSE: 0.2031\n",
      "Epoch  74, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0411, Val RMSE: 0.2028\n",
      "Epoch  75, Train Loss (MSE): 0.0470, Val Loss (MSE): 0.0412, Val RMSE: 0.2029\n",
      "Epoch  76, Train Loss (MSE): 0.0471, Val Loss (MSE): 0.0413, Val RMSE: 0.2031\n",
      "Epoch  77, Train Loss (MSE): 0.0472, Val Loss (MSE): 0.0412, Val RMSE: 0.2031\n",
      "Epoch  78, Train Loss (MSE): 0.0472, Val Loss (MSE): 0.0413, Val RMSE: 0.2031\n",
      "Epoch  79, Train Loss (MSE): 0.0472, Val Loss (MSE): 0.0413, Val RMSE: 0.2032\n",
      "Epoch  80, Train Loss (MSE): 0.0472, Val Loss (MSE): 0.0415, Val RMSE: 0.2038\n",
      "Epoch  81, Train Loss (MSE): 0.0473, Val Loss (MSE): 0.0415, Val RMSE: 0.2037\n",
      "Epoch  82, Train Loss (MSE): 0.0474, Val Loss (MSE): 0.0417, Val RMSE: 0.2043\n",
      "Epoch  83, Train Loss (MSE): 0.0475, Val Loss (MSE): 0.0416, Val RMSE: 0.2040\n",
      "Epoch  84, Train Loss (MSE): 0.0474, Val Loss (MSE): 0.0419, Val RMSE: 0.2046\n",
      "Epoch  85, Train Loss (MSE): 0.0475, Val Loss (MSE): 0.0419, Val RMSE: 0.2047\n",
      "Epoch  86, Train Loss (MSE): 0.0476, Val Loss (MSE): 0.0422, Val RMSE: 0.2054\n",
      "Epoch  87, Train Loss (MSE): 0.0477, Val Loss (MSE): 0.0421, Val RMSE: 0.2052\n",
      "Epoch  88, Train Loss (MSE): 0.0477, Val Loss (MSE): 0.0424, Val RMSE: 0.2058\n",
      "Epoch  89, Train Loss (MSE): 0.0477, Val Loss (MSE): 0.0424, Val RMSE: 0.2059\n",
      "Epoch  90, Train Loss (MSE): 0.0478, Val Loss (MSE): 0.0424, Val RMSE: 0.2059\n",
      "Epoch  91, Train Loss (MSE): 0.0478, Val Loss (MSE): 0.0426, Val RMSE: 0.2064\n",
      "Epoch  92, Train Loss (MSE): 0.0478, Val Loss (MSE): 0.0427, Val RMSE: 0.2066\n",
      "Epoch  93, Train Loss (MSE): 0.0479, Val Loss (MSE): 0.0429, Val RMSE: 0.2071\n",
      "Epoch  94, Train Loss (MSE): 0.0479, Val Loss (MSE): 0.0431, Val RMSE: 0.2076\n",
      "Epoch  95, Train Loss (MSE): 0.0479, Val Loss (MSE): 0.0430, Val RMSE: 0.2075\n",
      "Epoch  96, Train Loss (MSE): 0.0478, Val Loss (MSE): 0.0431, Val RMSE: 0.2076\n",
      "Epoch  97, Train Loss (MSE): 0.0479, Val Loss (MSE): 0.0431, Val RMSE: 0.2076\n",
      "Epoch  98, Train Loss (MSE): 0.0479, Val Loss (MSE): 0.0433, Val RMSE: 0.2081\n",
      "Epoch  99, Train Loss (MSE): 0.0479, Val Loss (MSE): 0.0433, Val RMSE: 0.2081\n",
      "Final Test Loss (MSE): 0.0714\n",
      "Final Test RMSE:       0.2672\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Regression of House Prices dataset using neural network implementation in TensorFlow 1.x\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) \n",
    "\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\train.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv House Prices dataset not found.\")\n",
    "    exit()\n",
    "\n",
    "# \n",
    "features = [\"GrLivArea\", \"YearBuilt\"]\n",
    "target = \"SalePrice\"\n",
    "\n",
    "df_clean = df[features + [target]].dropna()\n",
    "\n",
    "X = df_clean[features].values\n",
    "y = df_clean[target].values\n",
    "\n",
    "\n",
    "y_log = np.log(y).astype(np.float32) \n",
    "\n",
    "\n",
    "y_log = y_log.reshape(-1, 1)\n",
    "\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train_log, y_temp_log = train_test_split(\n",
    "    X, y_log, test_size=0.3, random_state=0) \n",
    "X_val, X_test, y_val_log, y_test_log = train_test_split(\n",
    "    X_temp, y_temp_log, test_size=0.5, random_state=0) \n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data Shapes:\")\n",
    "print(\"X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"y_train_log:\", y_train_log.shape)\n",
    "print(\"X_val_scaled:\", X_val_scaled.shape)\n",
    "print(\"y_val_log:\", y_val_log.shape)\n",
    "print(\"X_test_scaled:\", X_test_scaled.shape)\n",
    "print(\"y_test_log:\", y_test_log.shape)\n",
    "\n",
    "\n",
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 16     \n",
    "num_epochs = 100  \n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train_scaled.shape[1] \n",
    "n_samples = X_train_scaled.shape[0]\n",
    "n_classes = 1 \n",
    "\n",
    "\n",
    "print(\"Input features:\", n_input)\n",
    "print(\"Hidden Layer 1 Nodes:\", n_hidden1)\n",
    "print(\"Hidden Layer 2 Nodes:\", n_hidden2)\n",
    "print(\"Output Nodes:\", n_classes) \n",
    "print(\"Learning Rate:\", learning_rate)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Epochs:\", num_epochs)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes]) \n",
    "\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train_scaled, y_train_log, batch_size=batch_size)\n",
    "\n",
    "# \n",
    "def example_net(x):\n",
    "    initializer = tf.glorot_uniform_initializer()\n",
    "    weights = {\n",
    "        'w1': tf.Variable(initializer([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(initializer([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(initializer([n_hidden2, n_classes])) \n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.zeros([n_classes])) \n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # \n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# \n",
    "logits = example_net(X)\n",
    "\n",
    "# \n",
    "loss_op = tf.reduce_mean(tf.square(Y - logits))\n",
    "\n",
    "rmse_op = tf.sqrt(loss_op)\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            \n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            \n",
    "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss * mini_batch_x.shape[0] \n",
    "\n",
    "        avg_train_loss = total_loss / n_samples\n",
    "\n",
    "        #\n",
    "        val_loss, val_rmse = sess.run([loss_op, rmse_op], feed_dict={X: X_val_scaled, Y: y_val_log})\n",
    "\n",
    "        \n",
    "        print(\"Epoch {:>3}, Train Loss (MSE): {:.4f}, Val Loss (MSE): {:.4f}, Val RMSE: {:.4f}\".format(\n",
    "            epoch, avg_train_loss, val_loss, val_rmse))\n",
    "\n",
    "    \n",
    "    test_loss, test_rmse = sess.run([loss_op, rmse_op], feed_dict={X: X_test_scaled, Y: y_test_log})\n",
    "    \n",
    "    # Report final test metrics\n",
    "    print(\"Final Test Loss (MSE): {:.4f}\".format(test_loss))\n",
    "    print(\"Final Test RMSE:       {:.4f}\".format(test_rmse))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original MNIST shapes:\n",
      "X_train_orig: (60000, 28, 28)\n",
      "y_train_orig: (60000,)\n",
      "X_test: (10000, 28, 28)\n",
      "y_test: (10000,)\n",
      "X_train: (48000, 784)\n",
      "y_train_one_hot: (48000, 10)\n",
      "X_val: (12000, 784)\n",
      "y_val_one_hot: (12000, 10)\n",
      "X_test_norm: (10000, 784)\n",
      "y_test_one_hot: (10000, 10)\n",
      "Input features: 784\n",
      "Hidden Layer 1 Nodes: 256\n",
      "Hidden Layer 2 Nodes: 128\n",
      "Output Classes: 10\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 128\n",
      "Epochs: 20\n",
      "Epoch   0, Train Loss: 0.2793, Val Loss: 0.1464, Val Acc: 0.9572\n",
      "Epoch   1, Train Loss: 0.1036, Val Loss: 0.1089, Val Acc: 0.9665\n",
      "Epoch   2, Train Loss: 0.0628, Val Loss: 0.0939, Val Acc: 0.9718\n",
      "Epoch   3, Train Loss: 0.0411, Val Loss: 0.0913, Val Acc: 0.9721\n",
      "Epoch   4, Train Loss: 0.0272, Val Loss: 0.0914, Val Acc: 0.9718\n",
      "Epoch   5, Train Loss: 0.0189, Val Loss: 0.0899, Val Acc: 0.9737\n",
      "Epoch   6, Train Loss: 0.0130, Val Loss: 0.0922, Val Acc: 0.9757\n",
      "Epoch   7, Train Loss: 0.0104, Val Loss: 0.0892, Val Acc: 0.9777\n",
      "Epoch   8, Train Loss: 0.0088, Val Loss: 0.0989, Val Acc: 0.9764\n",
      "Epoch   9, Train Loss: 0.0073, Val Loss: 0.0960, Val Acc: 0.9769\n",
      "Epoch  10, Train Loss: 0.0057, Val Loss: 0.1183, Val Acc: 0.9733\n",
      "Epoch  11, Train Loss: 0.0053, Val Loss: 0.1126, Val Acc: 0.9762\n",
      "Epoch  12, Train Loss: 0.0043, Val Loss: 0.1022, Val Acc: 0.9783\n",
      "Epoch  13, Train Loss: 0.0039, Val Loss: 0.1099, Val Acc: 0.9768\n",
      "Epoch  14, Train Loss: 0.0036, Val Loss: 0.1031, Val Acc: 0.9787\n",
      "Epoch  15, Train Loss: 0.0027, Val Loss: 0.1098, Val Acc: 0.9774\n",
      "Epoch  16, Train Loss: 0.0025, Val Loss: 0.1147, Val Acc: 0.9783\n",
      "Epoch  17, Train Loss: 0.0028, Val Loss: 0.1109, Val Acc: 0.9793\n",
      "Epoch  18, Train Loss: 0.0028, Val Loss: 0.1209, Val Acc: 0.9765\n",
      "Epoch  19, Train Loss: 0.0017, Val Loss: 0.1201, Val Acc: 0.9775\n",
      "Final Test Accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "MNIST Digit Classification using neural network implementation in TensorFlow \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior() \n",
    "from tensorflow.keras.datasets import mnist\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) \n",
    "\n",
    "(X_train_orig, y_train_orig), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"Original MNIST shapes:\")\n",
    "print(\"X_train_orig:\", X_train_orig.shape) \n",
    "print(\"y_train_orig:\", y_train_orig.shape) \n",
    "print(\"X_test:\", X_test.shape)       \n",
    "print(\"y_test:\", y_test.shape)      \n",
    "\n",
    "X_train_flat = X_train_orig.reshape(-1, 784)\n",
    "X_test_flat = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train_norm = (X_train_flat / 255.0).astype(np.float32)\n",
    "X_test_norm = (X_test_flat / 255.0).astype(np.float32)\n",
    "\n",
    "y_train_col = y_train_orig.reshape(-1, 1).astype(np.int64) \n",
    "y_test_col = y_test.reshape(-1, 1).astype(np.int64)   \n",
    "\n",
    "\n",
    "X_train, X_val, y_train_int, y_val_int = train_test_split(\n",
    "    X_train_norm, y_train_col, test_size=0.2, random_state=0, stratify=y_train_col) \n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train_int)\n",
    "y_val_one_hot = enc.transform(y_val_int)\n",
    "y_test_one_hot = enc.transform(y_test_col)\n",
    "\n",
    "\n",
    "\n",
    "print(\"X_train:\", X_train.shape) \n",
    "print(\"y_train_one_hot:\", y_train_one_hot.shape) \n",
    "print(\"X_val:\", X_val.shape)  \n",
    "print(\"y_val_one_hot:\", y_val_one_hot.shape) \n",
    "print(\"X_test_norm:\", X_test_norm.shape)   \n",
    "print(\"y_test_one_hot:\", y_test_one_hot.shape) \n",
    "\n",
    "\n",
    "# \n",
    "class GetMiniBatch:\n",
    "    \n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# \n",
    "learning_rate = 0.001\n",
    "batch_size = 128     \n",
    "num_epochs = 20      \n",
    "\n",
    "# \n",
    "n_hidden1 = 256      \n",
    "n_hidden2 = 128      \n",
    "n_input = X_train.shape[1] \n",
    "n_samples = X_train.shape[0] \n",
    "n_classes = y_train_one_hot.shape[1] \n",
    "\n",
    "print(\"Input features:\", n_input)\n",
    "print(\"Hidden Layer 1 Nodes:\", n_hidden1)\n",
    "print(\"Hidden Layer 2 Nodes:\", n_hidden2)\n",
    "print(\"Output Classes:\", n_classes)\n",
    "print(\"Learning Rate:\", learning_rate)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Epochs:\", num_epochs)\n",
    "\n",
    "# \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "X = tf.placeholder(\"float\", [None, n_input]) \n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# \n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "# \n",
    "def example_net(x):\n",
    "    initializer = tf.glorot_uniform_initializer()\n",
    "    weights = {\n",
    "        'w1': tf.Variable(initializer([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(initializer([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(initializer([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    #\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# \n",
    "logits = example_net(X)\n",
    "\n",
    "#\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "# \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "#\n",
    "correct_pred = tf.equal(tf.argmax(logits, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss * mini_batch_x.shape[0] \n",
    "\n",
    "        avg_train_loss = total_loss / n_samples\n",
    "\n",
    "        # \n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "\n",
    "        print(\"Epoch {:>3}, Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}\".format( \n",
    "            epoch, avg_train_loss, val_loss, val_acc))\n",
    "\n",
    "    #\n",
    "    #\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test_norm, Y: y_test_one_hot})\n",
    "    print(\"Final Test Accuracy: {:.4f}\".format(test_acc)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
