{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Scratch implementation of logistic regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      Number of iterations\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    bias : bool\n",
    "      False if no bias term is included\n",
    "    verbose : bool\n",
    "      True to output the learning process\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : The following form of ndarray, shape (n_features,)\n",
    "      Parameters\n",
    "    self.loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record losses on training data\n",
    "    self.val_loss : The following form of ndarray, shape (self.iter,)\n",
    "      Record loss on validation data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False):\n",
    "        #hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        #arrays to record loss\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid function\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Add bias term to features\"\"\"\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    def _initialize_weights(self, n_features):\n",
    "        \"\"\"Initialize weights with small random values\"\"\"\n",
    "        return np.random.randn(n_features) * 0.01\n",
    "\n",
    "    def _compute_loss(self, y, y_pred):\n",
    "        \"\"\"Compute binary cross-entropy loss\"\"\"\n",
    "        epsilon = 1e-15  # to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn logistic regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "            if X_val is not None:\n",
    "                X_val = self._add_bias(X_val)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coef_ = self._initialize_weights(n_features)\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            # \n",
    "            z = np.dot(X, self.coef_)\n",
    "            #sigmoid function\n",
    "            y_pred = self._sigmoid(z)\n",
    "            \n",
    "            #gradient\n",
    "            error = y_pred - y\n",
    "            gradient = np.dot(X.T, error) / n_samples\n",
    "            \n",
    "            # Update weights\n",
    "            self.coef_ -= self.lr * gradient\n",
    "            \n",
    "            # loss\n",
    "            self.loss[i] = self._compute_loss(y, y_pred)\n",
    "            \n",
    "            #validation loss if validation data is provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self._sigmoid(np.dot(X_val, self.coef_))\n",
    "                self.val_loss[i] = self._compute_loss(y_val, val_pred)\n",
    "            \n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\", end=\"\")\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    print(f\", Validation Loss = {self.val_loss[i]:.4f}\")\n",
    "                else:\n",
    "                    print()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the probability using logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples,)\n",
    "            Estimated probability by logistic regression\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "        return self._sigmoid(np.dot(X, self.coef_))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Estimate the label using logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        threshold : float\n",
    "            Threshold for classification (default 0.5)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples,)\n",
    "            Estimated result by logistic regression\n",
    "        \"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "    \n",
    "\n",
    "    #Problem 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Scratch implementation of logistic regression with gradient descent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      Number of iterations\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    bias : bool\n",
    "      False if no bias term is included\n",
    "    verbose : bool\n",
    "      True to output the learning process\n",
    "    lambda_ : float\n",
    "      Regularization parameter (default: 0, no regularization)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : ndarray, shape (n_features,)\n",
    "      Parameters\n",
    "    self.loss : ndarray, shape (self.iter,)\n",
    "      Record losses on training data\n",
    "    self.val_loss : ndarray, shape (self.iter,)\n",
    "      Record loss on validation data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False, lambda_=0):\n",
    "        #hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        self.lambda_ = lambda_\n",
    "        #\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid function\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Add bias term to features\"\"\"\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    def _initialize_weights(self, n_features):\n",
    "        \"\"\"Initialize weights with small random values\"\"\"\n",
    "        return np.random.randn(n_features) * 0.01\n",
    "\n",
    "    def _compute_loss(self, y, y_pred):\n",
    "        \"\"\"Compute binary cross-entropy loss with L2 regularization\"\"\"\n",
    "        epsilon = 1e-15  # to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        \n",
    "        #L2 regularization\n",
    "        if self.lambda_ > 0:\n",
    "            if self.bias:\n",
    "                reg_term = (self.lambda_ / (2 * len(y))) * np.sum(self.coef_[1:]**2)\n",
    "            else:\n",
    "                reg_term = (self.lambda_ / (2 * len(y))) * np.sum(self.coef_**2)\n",
    "            loss += reg_term\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _gradient_descent(self, X, y, y_pred):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent with L2 regularization\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Feature matrix\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Target values\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            Predicted probabilities\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        gradient : ndarray, shape (n_features,)\n",
    "            Gradient vector\n",
    "        \"\"\"\n",
    "        error = y_pred - y\n",
    "        gradient = np.dot(X.T, error) / len(y)\n",
    "        \n",
    "        #regularization term \n",
    "        if self.lambda_ > 0:\n",
    "            if self.bias:\n",
    "                reg_term = (self.lambda_ / len(y)) * self.coef_\n",
    "                reg_term[0] = 0  # Don't regularize bias term\n",
    "            else:\n",
    "                reg_term = (self.lambda_ / len(y)) * self.coef_\n",
    "            gradient += reg_term\n",
    "            \n",
    "        return gradient\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Learn logistic regression using gradient descent. \n",
    "        If validation data is provided, also calculate validation loss at each iteration.\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "            if X_val is not None:\n",
    "                X_val = self._add_bias(X_val)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coef_ = self._initialize_weights(n_features)\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            #predictions\n",
    "            z = np.dot(X, self.coef_)\n",
    "            y_pred = self._sigmoid(z)\n",
    "            \n",
    "            #gradient and weights\n",
    "            gradient = self._gradient_descent(X, y, y_pred)\n",
    "            self.coef_ -= self.lr * gradient\n",
    "            \n",
    "            #loss\n",
    "            self.loss[i] = self._compute_loss(y, y_pred)\n",
    "            \n",
    "            #validation loss if validation data is provided\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self._sigmoid(np.dot(X_val, self.coef_))\n",
    "                self.val_loss[i] = self._compute_loss(y_val, val_pred)\n",
    "            \n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\", end=\"\")\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    print(f\", Validation Loss = {self.val_loss[i]:.4f}\")\n",
    "                else:\n",
    "                    print()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the probability using logistic regression.\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "        return self._sigmoid(np.dot(X, self.coef_))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Estimate the label using logistic regression.\n",
    "        \"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "    \n",
    "\n",
    "    #Problem 3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Scratch implementation of logistic regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      Number of iterations\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    bias : bool\n",
    "      True if bias term is included\n",
    "    verbose : bool\n",
    "      True to output the learning process\n",
    "    threshold : float\n",
    "      Decision threshold for classification (default: 0.5)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : ndarray, shape (n_features,)\n",
    "      Learned weights\n",
    "    self.loss : ndarray, shape (self.iter,)\n",
    "      Training loss at each iteration\n",
    "    self.val_loss : ndarray, shape (self.iter,)\n",
    "      Validation loss at each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False, threshold=0.5):\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        self.threshold = threshold\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid function\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Add bias term (column of ones) to features\"\"\"\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"Training method (implementation omitted for brevity)\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estimate class probabilities using logistic regression hypothesis function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Input features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_samples,)\n",
    "            Estimated probabilities for class 1\n",
    "        \"\"\"\n",
    "        #\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "        \n",
    "        #\n",
    "        if self.coef_ is None:\n",
    "            raise ValueError(\"Model not trained yet. Call fit() first.\")\n",
    "        \n",
    "        #\n",
    "        z = np.dot(X, self.coef_)\n",
    "        \n",
    "        #\n",
    "        probabilities = self._sigmoid(z)\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels using learned logistic regression model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Input features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_samples,)\n",
    "            Predicted class labels (0 or 1)\n",
    "        \"\"\"\n",
    "        #probabilities from predict_proba\n",
    "        probabilities = self.predict_proba(X)\n",
    "        \n",
    "        #\n",
    "        predictions = (probabilities >= self.threshold).astype(int)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "\n",
    "    #Problem 4\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Scratch implementation of logistic regression with L2 regularization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      Number of iterations\n",
    "    lr : float\n",
    "      Learning rate\n",
    "    bias : bool\n",
    "      True if bias term is included\n",
    "    verbose : bool\n",
    "      True to output the learning process\n",
    "    lambda_ : float\n",
    "      Regularization strength (default: 0, no regularization)\n",
    "    threshold : float\n",
    "      Decision threshold for classification (default: 0.5)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : ndarray, shape (n_features,)\n",
    "      Learned weights\n",
    "    self.loss : ndarray, shape (self.iter,)\n",
    "      Training loss at each iteration\n",
    "    self.val_loss : ndarray, shape (self.iter,)\n",
    "      Validation loss at each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False, lambda_=0, threshold=0.5):\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        self.lambda_ = lambda_\n",
    "        self.threshold = threshold\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid function\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Add bias term (column of ones) to features\"\"\"\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    def _compute_loss(self, X, y, coef):\n",
    "        \"\"\"\n",
    "        Compute the logistic regression objective function with L2 regularization\n",
    "        \n",
    "        J(θ) = (1/m) * Σ[-yⁱlog(hθ(xⁱ)) - (1-yⁱ)log(1-hθ(xⁱ))] + (λ/2m) * Σθⱼ² (j=1 to n)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Input features\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Target labels\n",
    "        coef : ndarray, shape (n_features,)\n",
    "            Current model coefficients\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Computed loss value\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(np.dot(X, coef))\n",
    "        \n",
    "        #\n",
    "        epsilon = 1e-15  # to prevent log(0)\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        cross_entropy = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        \n",
    "        # L2 regularization term \n",
    "        if self.bias:\n",
    "            reg_term = (self.lambda_ / (2 * m)) * np.sum(coef[1:]**2)\n",
    "        else:\n",
    "            reg_term = (self.lambda_ / (2 * m)) * np.sum(coef**2)\n",
    "            \n",
    "        total_loss = cross_entropy + reg_term\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Train logistic regression model with gradient descent\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Training features\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Training labels\n",
    "        X_val : ndarray, shape (n_val_samples, n_features), optional\n",
    "            Validation features\n",
    "        y_val : ndarray, shape (n_val_samples,), optional\n",
    "            Validation labels\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "            if X_val is not None:\n",
    "                X_val = self._add_bias(X_val)\n",
    "\n",
    "        m, n = X.shape\n",
    "        self.coef_ = np.zeros(n)\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            #predictions\n",
    "            z = np.dot(X, self.coef_)\n",
    "            h = self._sigmoid(z)\n",
    "            \n",
    "            #gradient\n",
    "            error = h - y\n",
    "            gradient = np.dot(X.T, error) / m\n",
    "            \n",
    "            #regularization term\n",
    "            if self.lambda_ > 0:\n",
    "                reg_term = (self.lambda_ / m) * self.coef_\n",
    "                if self.bias:\n",
    "                    reg_term[0] = 0  # Don't regularize bias term\n",
    "                gradient += reg_term\n",
    "                \n",
    "            #coefficients\n",
    "            self.coef_ -= self.lr * gradient\n",
    "            \n",
    "            #training loss\n",
    "            self.loss[i] = self._compute_loss(X, y, self.coef_)\n",
    "            \n",
    "            #validation loss \n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.val_loss[i] = self._compute_loss(X_val, y_val, self.coef_)\n",
    "            \n",
    "            if self.verbose and i % 100 == 0:\n",
    "                msg = f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    msg += f\", Validation Loss = {self.val_loss[i]:.4f}\"\n",
    "                print(msg)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Implementation as before...\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Implementation as before...\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    #Problem 5\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"Our implementation from previous steps\"\"\"\n",
    "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False, lambda_=0, threshold=0.5):\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        self.lambda_ = lambda_\n",
    "        self.threshold = threshold\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    def _compute_loss(self, X, y, coef):\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(np.dot(X, coef))\n",
    "        epsilon = 1e-15\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        cross_entropy = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        if self.bias:\n",
    "            reg_term = (self.lambda_ / (2 * m)) * np.sum(coef[1:]**2)\n",
    "        else:\n",
    "            reg_term = (self.lambda_ / (2 * m)) * np.sum(coef**2)\n",
    "        return cross_entropy + reg_term\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "            if X_val is not None:\n",
    "                X_val = self._add_bias(X_val)\n",
    "\n",
    "        m, n = X.shape\n",
    "        self.coef_ = np.zeros(n)\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            z = np.dot(X, self.coef_)\n",
    "            h = self._sigmoid(z)\n",
    "            error = h - y\n",
    "            gradient = np.dot(X.T, error) / m\n",
    "            if self.lambda_ > 0:\n",
    "                reg_term = (self.lambda_ / m) * self.coef_\n",
    "                if self.bias:\n",
    "                    reg_term[0] = 0\n",
    "                gradient += reg_term\n",
    "            self.coef_ -= self.lr * gradient\n",
    "            self.loss[i] = self._compute_loss(X, y, self.coef_)\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.val_loss[i] = self._compute_loss(X_val, y_val, self.coef_)\n",
    "            if self.verbose and i % 100 == 0:\n",
    "                msg = f\"Iteration {i}: Loss = {self.loss[i]:.4f}\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    msg += f\", Val Loss = {self.val_loss[i]:.4f}\"\n",
    "                print(msg)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "        if self.coef_ is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        return self._sigmoid(np.dot(X, self.coef_))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= self.threshold).astype(int)\n",
    "\n",
    "# Load and prepare iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Filter for only virginica (2) and versicolor (1)\n",
    "mask = (y == 1) | (y == 2)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Convert to binary classification (1 for virginica, 0 for versicolor)\n",
    "y = (y == 2).astype(int)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train our scratch implementation\n",
    "our_lr = ScratchLogisticRegression(num_iter=2000, lr=0.1, lambda_=0.1, verbose=True)\n",
    "our_lr.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Train scikit-learn implementation\n",
    "sk_lr = LogisticRegression(penalty='l2', C=1/0.1, max_iter=2000, random_state=42)\n",
    "sk_lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "our_preds = our_lr.predict(X_test)\n",
    "sk_preds = sk_lr.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "def print_metrics(y_true, y_pred, model_name):\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
    "\n",
    "print_metrics(y_test, our_preds, \"Our Scratch Implementation\")\n",
    "print_metrics(y_test, sk_preds, \"Scikit-learn Implementation\")\n",
    "\n",
    "# Compare coefficients\n",
    "print(\"\\nCoefficient Comparison:\")\n",
    "print(\"Our coefficients:\", our_lr.coef_)\n",
    "print(\"Sklearn coefficients:\", np.concatenate([sk_lr.intercept_, sk_lr.coef_[0]]))\n",
    "\n",
    "\n",
    "\n",
    "#Problem 6\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "\n",
    "#learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(our_lr.loss, label='Training Loss', color='blue')\n",
    "plt.plot(our_lr.val_loss, label='Validation Loss', color='orange', linestyle='--')\n",
    "plt.title('Learning Curve of Scratch Logistic Regression')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#region where loss decreases significantly\n",
    "min_loss = min(min(our_lr.loss), min(our_lr.val_loss))\n",
    "max_loss = max(max(our_lr.loss[:10]), max(our_lr.val_loss[:10]))  # First 10 iterations\n",
    "plt.ylim(min_loss - 0.1, max_loss + 0.1)  # Set y-axis limits to see details\n",
    "\n",
    "#vertical line at point where loss stabilizes\n",
    "stable_iter = np.argmin(our_lr.val_loss)\n",
    "plt.axvline(x=stable_iter, color='red', linestyle=':', \n",
    "            label=f'Stabilization at iter {stable_iter}')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#final loss values\n",
    "print(f\"\\nFinal Training Loss: {our_lr.loss[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {our_lr.val_loss[-1]:.4f}\")\n",
    "print(f\"Minimum Validation Loss at iteration: {stable_iter}\")\n",
    "\n",
    "# Compare with sklearn's loss (log loss)\n",
    "from sklearn.metrics import log_loss\n",
    "sk_probs = sk_lr.predict_proba(X_test)[:, 1]\n",
    "sk_loss = log_loss(y_test, sk_probs)\n",
    "print(f\"\\nScikit-learn's Log Loss: {sk_loss:.4f}\")\n",
    "\n",
    "\n",
    "#Problem 7\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"Same implementation as before, kept for completeness\"\"\"\n",
    "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False, lambda_=0, threshold=0.5):\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        self.lambda_ = lambda_\n",
    "        self.threshold = threshold\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        self.coef_ = np.zeros(n)\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            z = np.dot(X, self.coef_)\n",
    "            h = self._sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / m\n",
    "            if self.lambda_ > 0:\n",
    "                reg_term = (self.lambda_ / m) * self.coef_\n",
    "                if self.bias:\n",
    "                    reg_term[0] = 0\n",
    "                gradient += reg_term\n",
    "            self.coef_ -= self.lr * gradient\n",
    "            self.loss[i] = -np.mean(y * np.log(h + 1e-15) + (1 - y) * np.log(1 - h + 1e-15))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "        return self._sigmoid(np.dot(X, self.coef_))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= self.threshold).astype(int)\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data[:, [1, 2]]  # Sepal width (index 1) and petal length (index 2)\n",
    "y = iris.target\n",
    "\n",
    "# Filter for only virginica (2) and versicolor (1)\n",
    "mask = (y == 1) | (y == 2)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Convert to binary classification (1 for virginica, 0 for versicolor)\n",
    "y = (y == 2).astype(int)\n",
    "\n",
    "#\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train model\n",
    "model = ScratchLogisticRegression(num_iter=1000, lr=0.1, lambda_=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "#\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict for each point in mesh grid\n",
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "#decision boundary and regions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], colors=['blue', 'orange'], alpha=0.3)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='red', linewidths=2)\n",
    "\n",
    "#data points\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label='versicolor', edgecolor='k')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='orange', label='virginica', edgecolor='k')\n",
    "\n",
    "# \n",
    "plt.xlabel('Sepal Width (standardized)')\n",
    "plt.ylabel('Petal Length (standardized)')\n",
    "plt.title('Decision Region for Logistic Regression (virginica vs versicolor)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Problem 8\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Logistic Regression implementation from scratch with:\n",
    "    - L2 regularization\n",
    "    - Learning curve tracking\n",
    "    - Decision boundary visualization\n",
    "    - Weight saving/loading (pickle and npz)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False, lambda_=0, threshold=0.5):\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        self.lambda_ = lambda_\n",
    "        self.threshold = threshold\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.coef_ = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid function\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Add bias term to features\"\"\"\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    def _compute_loss(self, X, y, coef):\n",
    "        \"\"\"Compute loss with L2 regularization\"\"\"\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(np.dot(X, coef))\n",
    "        epsilon = 1e-15\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        cross_entropy = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        \n",
    "        if self.bias:\n",
    "            reg_term = (self.lambda_ / (2 * m)) * np.sum(coef[1:]**2)\n",
    "        else:\n",
    "            reg_term = (self.lambda_ / (2 * m)) * np.sum(coef**2)\n",
    "            \n",
    "        return cross_entropy + reg_term\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "            if X_val is not None:\n",
    "                X_val = self._add_bias(X_val)\n",
    "\n",
    "        m, n = X.shape\n",
    "        self.coef_ = np.zeros(n)\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            # Forward pass\n",
    "            z = np.dot(X, self.coef_)\n",
    "            h = self._sigmoid(z)\n",
    "            \n",
    "            # Backward pass\n",
    "            error = h - y\n",
    "            gradient = np.dot(X.T, error) / m\n",
    "            \n",
    "            # Add regularization\n",
    "            if self.lambda_ > 0:\n",
    "                reg_term = (self.lambda_ / m) * self.coef_\n",
    "                if self.bias:\n",
    "                    reg_term[0] = 0\n",
    "                gradient += reg_term\n",
    "                \n",
    "            #weights\n",
    "            self.coef_ -= self.lr * gradient\n",
    "            \n",
    "            #losses\n",
    "            self.loss[i] = self._compute_loss(X, y, self.coef_)\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.val_loss[i] = self._compute_loss(X_val, y_val, self.coef_)\n",
    "            \n",
    "            if self.verbose and i % 100 == 0:\n",
    "                msg = f\"Iteration {i}: Loss = {self.loss[i]:.4f}\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    msg += f\", Val Loss = {self.val_loss[i]:.4f}\"\n",
    "                print(msg)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        if self.bias:\n",
    "            X = self._add_bias(X)\n",
    "        if self.coef_ is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        return self._sigmoid(np.dot(X, self.coef_))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        return (self.predict_proba(X) >= self.threshold).astype(int)\n",
    "\n",
    "    def plot_decision_boundary(self, X, y):\n",
    "        \"\"\"\n",
    "        Plot decision boundary for 2D feature space\n",
    "        Parameters:\n",
    "        X : ndarray, shape (n_samples, 2)\n",
    "            Features (should be exactly 2 dimensions)\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Target labels (0 or 1)\n",
    "        \"\"\"\n",
    "        if X.shape[1] != 2:\n",
    "            raise ValueError(\"Decision boundary plotting only works for 2D features\")\n",
    "            \n",
    "        #mesh grid\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                             np.linspace(y_min, y_max, 200))\n",
    "        \n",
    "        # Predict probabilities\n",
    "        Z = self.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        #\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], colors=['blue', 'orange'], alpha=0.3)\n",
    "        plt.contour(xx, yy, Z, levels=[0.5], colors='red', linewidths=2)\n",
    "        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label='Class 0', edgecolor='k')\n",
    "        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='orange', label='Class 1', edgecolor='k')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.title('Decision Boundary')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def save_weights_pickle(self, filename):\n",
    "        \"\"\"Save weights and parameters using pickle\"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'coef': self.coef_,\n",
    "                'params': {\n",
    "                    'num_iter': self.iter,\n",
    "                    'lr': self.lr,\n",
    "                    'bias': self.bias,\n",
    "                    'lambda_': self.lambda_,\n",
    "                    'threshold': self.threshold\n",
    "                }\n",
    "            }, f)\n",
    "\n",
    "    def load_weights_pickle(self, filename):\n",
    "        \"\"\"Load weights and parameters using pickle\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.coef_ = data['coef']\n",
    "        params = data['params']\n",
    "        self.iter = params['num_iter']\n",
    "        self.lr = params['lr']\n",
    "        self.bias = params['bias']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.threshold = params['threshold']\n",
    "\n",
    "    def save_weights_npz(self, filename):\n",
    "        \"\"\"Save weights and parameters using numpy's savez\"\"\"\n",
    "        np.savez(\n",
    "            filename,\n",
    "            coef=self.coef_,\n",
    "            num_iter=self.iter,\n",
    "            lr=self.lr,\n",
    "            bias=self.bias,\n",
    "            lambda_=self.lambda_,\n",
    "            threshold=self.threshold\n",
    "        )\n",
    "\n",
    "    def load_weights_npz(self, filename):\n",
    "        \"\"\"Load weights and parameters using numpy's load\"\"\"\n",
    "        data = np.load(filename, allow_pickle=True)\n",
    "        self.coef_ = data['coef']\n",
    "        self.iter = int(data['num_iter'])\n",
    "        self.lr = float(data['lr'])\n",
    "        self.bias = bool(data['bias'])\n",
    "        self.lambda_ = float(data['lambda_'])\n",
    "        self.threshold = float(data['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
