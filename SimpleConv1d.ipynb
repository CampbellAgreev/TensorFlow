{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    def __init__(self, F, initializer, optimizer, stride=1):\n",
    "        self.F = F  \n",
    "        self.W = initializer.W(F)  \n",
    "        self.B = initializer.B(1)  \n",
    "        self.optimizer = optimizer\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for 1D convolution.\n",
    "        Args:\n",
    "            x: Input array (1D, shape: [N_in])\n",
    "        Returns:\n",
    "            A: Output array (1D, shape: [N_out])\n",
    "        \"\"\"\n",
    "        N_in = len(x)\n",
    "        N_out = (N_in - self.F) // self.stride + 1\n",
    "        A = np.zeros(N_out)\n",
    "        for i in range(N_out):\n",
    "            start = i * self.stride\n",
    "            A[i] = np.sum(x[start:start+self.F] * self.W) + self.B\n",
    "        self.x = x  \n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"Backward pass.\n",
    "        Args:\n",
    "            dA: Gradient from next layer (shape: [N_out])\n",
    "        Returns:\n",
    "            dW: Gradient for weights (shape: [F])\n",
    "            dB: Gradient for bias (scalar)\n",
    "            dx: Gradient for input (shape: [N_in])\n",
    "        \"\"\"\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dB = np.sum(dA)\n",
    "        dx = np.zeros_like(self.x)\n",
    "        for i in range(len(dA)):\n",
    "            start = i * self.stride\n",
    "            dx[start:start+self.F] += dA[i] * self.W\n",
    "            dW += dA[i] * self.x[start:start+self.F]\n",
    "        self.optimizer.update(self, dW, dB)  # Update weights/bias\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_output_size(N_in, F, P=0, S=1):\n",
    "    \"\"\"\n",
    "    Calculate output size for 1D convolution.\n",
    "    \n",
    "    Args:\n",
    "        N_in (int): Input size (number of features).\n",
    "        F (int): Filter size.\n",
    "        P (int): Padding size (default=0).\n",
    "        S (int): Stride size (default=1).\n",
    "        \n",
    "    Returns:\n",
    "        int: Output size (number of features).\n",
    "    \"\"\"\n",
    "    N_out = (N_in + 2 * P - F) // S + 1\n",
    "    return N_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output: [35. 50.]\n",
      "Weight gradients: [ 50  80 110]\n",
      "Bias gradient: 30\n",
      "Input gradients: [ 70 190 130  60]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleConv1d:\n",
    "    def __init__(self, w, b, stride=1):\n",
    "        self.w = w  # Kernel weights\n",
    "        self.b = b  # Bias\n",
    "        self.stride = stride\n",
    "        self.x = None  # To store input for backprop\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for 1D convolution\"\"\"\n",
    "        self.x = x  # Store input for backprop\n",
    "        F = len(self.w)\n",
    "        N_out = (len(x) - F) // self.stride + 1\n",
    "        a = np.zeros(N_out)\n",
    "        \n",
    "        for i in range(N_out):\n",
    "            start = i * self.stride\n",
    "            window = x[start:start+F]\n",
    "            a[i] = np.sum(window * self.w) + self.b\n",
    "        return a\n",
    "    \n",
    "    def backward(self, delta_a):\n",
    "        \"\"\"Backward pass for 1D convolution\"\"\"\n",
    "        F = len(self.w)\n",
    "        delta_w = np.zeros_like(self.w)\n",
    "        delta_b = np.sum(delta_a)\n",
    "        delta_x = np.zeros_like(self.x)\n",
    "        \n",
    "        #weight gradients\n",
    "        for i in range(len(delta_a)):\n",
    "            start = i * self.stride\n",
    "            window = self.x[start:start+F]\n",
    "            delta_w += delta_a[i] * window\n",
    "        \n",
    "    \n",
    "        \n",
    "        expanded_delta = np.zeros(len(self.x) - F + 1)\n",
    "        for i in range(len(delta_a)):\n",
    "            pos = i * self.stride\n",
    "            expanded_delta[pos] = delta_a[i]\n",
    "        \n",
    "        #\n",
    "        for j in range(len(delta_x)):\n",
    "            for k in range(F):\n",
    "                if 0 <= j - k < len(expanded_delta):\n",
    "                    delta_x[j] += expanded_delta[j - k] * self.w[F - 1 - k]\n",
    "        \n",
    "        return delta_w, delta_b, delta_x\n",
    "\n",
    "# \n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "delta_a = np.array([10, 20])\n",
    "\n",
    "# \n",
    "conv = SimpleConv1d(w, b)\n",
    "a = conv.forward(x)\n",
    "delta_w, delta_b, delta_x = conv.backward(delta_a)\n",
    "\n",
    "print(\"Forward pass output:\", a)\n",
    "print(\"Weight gradients:\", delta_w)\n",
    "print(\"Bias gradient:\", delta_b)\n",
    "print(\"Input gradients:\", delta_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output:\n",
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n",
      "\n",
      "Weight gradients:\n",
      "[[[3. 5. 7.]\n",
      "  [5. 7. 9.]]\n",
      "\n",
      " [[3. 5. 7.]\n",
      "  [5. 7. 9.]]\n",
      "\n",
      " [[3. 5. 7.]\n",
      "  [5. 7. 9.]]]\n",
      "\n",
      "Bias gradients:\n",
      "[2. 2. 2.]\n",
      "\n",
      "Input gradients:\n",
      "[[3. 6. 6. 3.]\n",
      " [3. 6. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # \n",
    "        self.w = np.random.randn(out_channels, in_channels, kernel_size).astype(np.float64) * 0.1\n",
    "        self.b = np.random.randn(out_channels).astype(np.float64) * 0.1\n",
    "        \n",
    "        # \n",
    "        self.input_shape = None\n",
    "        self.input_padded = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (in_channels, num_features)\n",
    "        output shape: (out_channels, num_output_features)\n",
    "        \"\"\"\n",
    "        #\n",
    "        x = x.astype(np.float64)\n",
    "        \n",
    "        #\n",
    "        if self.padding > 0:\n",
    "            x_padded = np.pad(x, ((0,0), (self.padding, self.padding)), mode='constant')\n",
    "        else:\n",
    "            x_padded = x\n",
    "        \n",
    "        self.input_shape = x.shape\n",
    "        self.input_padded = x_padded\n",
    "        \n",
    "        num_features = x.shape[1]\n",
    "        num_output_features = ((num_features + 2*self.padding - self.kernel_size) // self.stride) + 1\n",
    "        \n",
    "        output = np.zeros((self.out_channels, num_output_features), dtype=np.float64)\n",
    "        \n",
    "        for out_ch in range(self.out_channels):\n",
    "            for i in range(num_output_features):\n",
    "                start = i * self.stride\n",
    "                window = x_padded[:, start:start+self.kernel_size]\n",
    "                output[out_ch, i] = np.sum(window * self.w[out_ch]) + self.b[out_ch]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, delta_a):\n",
    "        \"\"\"\n",
    "        delta_a shape: (out_channels, num_output_features)\n",
    "        Returns:\n",
    "        - delta_w: gradient for weights (same shape as self.w)\n",
    "        - delta_b: gradient for bias (same shape as self.b)\n",
    "        - delta_x: gradient for input (same shape as original input)\n",
    "        \"\"\"\n",
    "        delta_a = delta_a.astype(np.float64)\n",
    "        \n",
    "        delta_w = np.zeros_like(self.w)\n",
    "        delta_b = np.sum(delta_a, axis=1)  \n",
    "        delta_x_padded = np.zeros_like(self.input_padded)\n",
    "        \n",
    "        num_output_features = delta_a.shape[1]\n",
    "        \n",
    "        for out_ch in range(self.out_channels):\n",
    "            for i in range(num_output_features):\n",
    "                start = i * self.stride\n",
    "                window = self.input_padded[:, start:start+self.kernel_size]\n",
    "                \n",
    "                # Weight gradients\n",
    "                delta_w[out_ch] += delta_a[out_ch, i] * window\n",
    "                \n",
    "                # Input gradients\n",
    "                delta_x_padded[:, start:start+self.kernel_size] += delta_a[out_ch, i] * self.w[out_ch]\n",
    "        \n",
    "        # Remove padding from input gradient \n",
    "        if self.padding > 0:\n",
    "            delta_x = delta_x_padded[:, self.padding:-self.padding]\n",
    "        else:\n",
    "            delta_x = delta_x_padded\n",
    "        \n",
    "        return delta_w, delta_b, delta_x\n",
    "\n",
    "########################\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]], dtype=np.float64)  \n",
    "w = np.ones((3, 2, 3), dtype=np.float64)  \n",
    "b = np.array([1, 2, 3], dtype=np.float64)  \n",
    "\n",
    "conv = Conv1d(in_channels=2, out_channels=3, kernel_size=3)\n",
    "conv.w = w  \n",
    "conv.b = b\n",
    "\n",
    "# \n",
    "a = conv.forward(x)\n",
    "print(\"Forward pass output:\")\n",
    "print(a)\n",
    "\n",
    "# Backward pass test\n",
    "delta_a = np.ones_like(a)  \n",
    "delta_w, delta_b, delta_x = conv.backward(delta_a)\n",
    "\n",
    "print(\"\\nWeight gradients:\")\n",
    "print(delta_w)\n",
    "\n",
    "print(\"\\nBias gradients:\")\n",
    "print(delta_b)\n",
    "\n",
    "print(\"\\nInput gradients:\")\n",
    "print(delta_x)\n",
    "\n",
    "# \n",
    "assert np.allclose(a, [[16,22], [17,23], [18,24]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Zero Padding ===\n",
      "Forward output (zero padding):\n",
      " [[14. 24. 30. 22.]\n",
      " [14. 24. 30. 22.]\n",
      " [14. 24. 30. 22.]]\n",
      "\n",
      "=== Test 2: Same Padding ===\n",
      "Forward output (same padding):\n",
      " [[14. 24. 30. 22.]\n",
      " [14. 24. 30. 22.]\n",
      " [14. 24. 30. 22.]]\n",
      "\n",
      "=== Test 3: Edge Padding ===\n",
      "Forward output (edge padding):\n",
      " [[20. 24. 30. 34.]\n",
      " [20. 24. 30. 34.]\n",
      " [20. 24. 30. 34.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding='valid'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            padding: 'valid' (no padding), \n",
    "                    'same' (pad to maintain input size),\n",
    "                    'zeros' (zero padding with specified size),\n",
    "                    'edge' (replicate edge values)\n",
    "        \"\"\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding_mode = padding\n",
    "        \n",
    "        # \n",
    "        self.w = np.random.randn(out_channels, in_channels, kernel_size) * 0.1\n",
    "        self.b = np.random.randn(out_channels) * 0.1\n",
    "        \n",
    "        # \n",
    "        self.input_shape = None\n",
    "        self.pad_width = None\n",
    "    \n",
    "    def _apply_padding(self, x):\n",
    "        \"\"\"Apply padding based on padding mode\"\"\"\n",
    "        if self.padding_mode == 'valid':\n",
    "            return x, 0\n",
    "        \n",
    "        if self.padding_mode == 'same':\n",
    "            pad_total = (self.kernel_size - 1)\n",
    "            pad_left = pad_total // 2\n",
    "            pad_right = pad_total - pad_left\n",
    "            self.pad_width = ((0, 0), (pad_left, pad_right))\n",
    "            return np.pad(x, self.pad_width, mode='constant'), pad_left\n",
    "        \n",
    "        elif isinstance(self.padding_mode, int):\n",
    "            self.pad_width = ((0, 0), (self.padding_mode, self.padding_mode))\n",
    "            return np.pad(x, self.pad_width, mode='constant'), self.padding_mode\n",
    "        \n",
    "        elif self.padding_mode == 'edge':\n",
    "            pad_total = (self.kernel_size - 1)\n",
    "            pad_left = pad_total // 2\n",
    "            pad_right = pad_total - pad_left\n",
    "            self.pad_width = ((0, 0), (pad_left, pad_right))\n",
    "            return np.pad(x, self.pad_width, mode='edge'), pad_left\n",
    "        \n",
    "        raise ValueError(f\"Unsupported padding mode: {self.padding_mode}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with padding support\"\"\"\n",
    "        x_padded, pad_left = self._apply_padding(x)\n",
    "        self.input_shape = x.shape\n",
    "        self.x_padded = x_padded\n",
    "        \n",
    "        num_features = x.shape[1]\n",
    "        if self.padding_mode == 'same':\n",
    "            num_output_features = num_features\n",
    "        else:\n",
    "            num_output_features = ((num_features + 2*pad_left - self.kernel_size) // self.stride) + 1\n",
    "        \n",
    "        output = np.zeros((self.out_channels, num_output_features))\n",
    "        \n",
    "        for out_ch in range(self.out_channels):\n",
    "            for i in range(num_output_features):\n",
    "                start = i * self.stride\n",
    "                window = x_padded[:, start:start+self.kernel_size]\n",
    "                output[out_ch, i] = np.sum(window * self.w[out_ch]) + self.b[out_ch]\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, delta_a):\n",
    "        \"\"\"Backward pass with padding support\"\"\"\n",
    "        delta_w = np.zeros_like(self.w)\n",
    "        delta_b = np.sum(delta_a, axis=1)\n",
    "        \n",
    "        # Initialize gradient with respect to padded input\n",
    "        delta_x_padded = np.zeros_like(self.x_padded)\n",
    "        \n",
    "        num_output_features = delta_a.shape[1]\n",
    "        \n",
    "        for out_ch in range(self.out_channels):\n",
    "            for i in range(num_output_features):\n",
    "                start = i * self.stride\n",
    "                window = self.x_padded[:, start:start+self.kernel_size]\n",
    "                \n",
    "                # Weight gradients\n",
    "                delta_w[out_ch] += delta_a[out_ch, i] * window\n",
    "                \n",
    "                # Input gradients\n",
    "                delta_x_padded[:, start:start+self.kernel_size] += delta_a[out_ch, i] * self.w[out_ch]\n",
    "        \n",
    "        # \n",
    "        if self.padding_mode == 'valid' or isinstance(self.padding_mode, int):\n",
    "            pad_left = self.pad_width[1][0] if hasattr(self, 'pad_width') else 0\n",
    "            delta_x = delta_x_padded[:, pad_left:delta_x_padded.shape[1]-pad_left]\n",
    "        else:\n",
    "            delta_x = delta_x_padded\n",
    "        \n",
    "        return delta_w, delta_b, delta_x\n",
    "\n",
    "# Test \n",
    "print(\"=== Test 1: Zero Padding ===\")\n",
    "x = np.array([[1,2,3,4], [5,6,7,8]], dtype=np.float64)\n",
    "conv = Conv1d(2, 3, kernel_size=3, padding=1)  \n",
    "conv.w = np.ones_like(conv.w)\n",
    "conv.b = np.zeros_like(conv.b)\n",
    "output = conv.forward(x)\n",
    "print(\"Forward output (zero padding):\\n\", output)\n",
    "\n",
    "print(\"\\n=== Test 2: Same Padding ===\")\n",
    "conv_same = Conv1d(2, 3, kernel_size=3, padding='same') \n",
    "conv_same.w = np.ones_like(conv_same.w)\n",
    "conv_same.b = np.zeros_like(conv_same.b)\n",
    "output_same = conv_same.forward(x)\n",
    "print(\"Forward output (same padding):\\n\", output_same)\n",
    "assert output_same.shape[1] == x.shape[1]  \n",
    "\n",
    "print(\"\\n=== Test 3: Edge Padding ===\")\n",
    "conv_edge = Conv1d(2, 3, kernel_size=3, padding='edge')  \n",
    "conv_edge.w = np.ones_like(conv_edge.w)\n",
    "conv_edge.b = np.zeros_like(conv_edge.b)\n",
    "output_edge = conv_edge.forward(x)\n",
    "print(\"Forward output (edge padding):\\n\", output_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Mini-batch Test ===\n",
      "Forward output shape: (3, 3, 4)\n",
      "First sample output:\n",
      " [[14. 24. 30. 22.]\n",
      " [14. 24. 30. 22.]\n",
      " [14. 24. 30. 22.]]\n",
      "\n",
      "Weight gradients shape: (3, 2, 3)\n",
      "Bias gradients: [12. 12. 12.]\n",
      "Input gradients shape: (3, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        #\n",
    "        self.w = np.random.randn(out_channels, in_channels, kernel_size) * 0.1\n",
    "        self.b = np.random.randn(out_channels) * 0.1\n",
    "        \n",
    "        #\n",
    "        self.input_shape = None\n",
    "        self.x_padded = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with mini-batch support\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # \n",
    "        if self.padding > 0:\n",
    "            self.x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')\n",
    "        else:\n",
    "            self.x_padded = x\n",
    "        \n",
    "        self.input_shape = x.shape\n",
    "        \n",
    "        num_features = x.shape[2]\n",
    "        num_output_features = ((num_features + 2*self.padding - self.kernel_size) // self.stride) + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, self.out_channels, num_output_features))\n",
    "        \n",
    "        # Vectorized implementation\n",
    "        for i in range(num_output_features):\n",
    "            start = i * self.stride\n",
    "            window = self.x_padded[:, :, start:start+self.kernel_size]  # (batch, in_ch, kernel)\n",
    "            \n",
    "            #\n",
    "            output[:, :, i] = np.tensordot(window, self.w, axes=([1,2], [1,2])) + self.b\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, delta_a):\n",
    "        \"\"\"Backward pass with mini-batch support\"\"\"\n",
    "        batch_size = delta_a.shape[0]\n",
    "        delta_w = np.zeros_like(self.w)\n",
    "        delta_b = np.sum(delta_a, axis=(0, 2))  \n",
    "        \n",
    "        # \n",
    "        delta_x_padded = np.zeros_like(self.x_padded)\n",
    "        \n",
    "        num_output_features = delta_a.shape[2]\n",
    "        \n",
    "        for i in range(num_output_features):\n",
    "            start = i * self.stride\n",
    "            window = self.x_padded[:, :, start:start+self.kernel_size]  # (batch, in_ch, kernel)\n",
    "            \n",
    "            #\n",
    "            delta_w += np.tensordot(delta_a[:, :, i].T, window, axes=([1], [0]))\n",
    "            \n",
    "            #\n",
    "            delta_x_padded[:, :, start:start+self.kernel_size] += np.einsum(\n",
    "                'bo,oik->bik', \n",
    "                delta_a[:, :, i], \n",
    "                self.w\n",
    "            )\n",
    "        \n",
    "        #\n",
    "        if self.padding > 0:\n",
    "            delta_x = delta_x_padded[:, :, self.padding:-self.padding]\n",
    "        else:\n",
    "            delta_x = delta_x_padded\n",
    "        \n",
    "        return delta_w, delta_b, delta_x\n",
    "\n",
    "#####\n",
    "print(\"=== Mini-batch Test ===\")\n",
    "x_batch = np.array([\n",
    "    [[1,2,3,4], [5,6,7,8]],   \n",
    "    [[2,3,4,5], [6,7,8,9]],    \n",
    "    [[3,4,5,6], [7,8,9,10]]    \n",
    "], dtype=np.float64)  \n",
    "\n",
    "conv = Conv1d(in_channels=2, out_channels=3, kernel_size=3, padding=1)\n",
    "conv.w = np.ones_like(conv.w)  \n",
    "conv.b = np.zeros_like(conv.b)  \n",
    "\n",
    "# \n",
    "output = conv.forward(x_batch)\n",
    "print(\"Forward output shape:\", output.shape)\n",
    "print(\"First sample output:\\n\", output[0])\n",
    "\n",
    "# \n",
    "delta_a = np.ones_like(output)\n",
    "delta_w, delta_b, delta_x = conv.backward(delta_a)\n",
    "\n",
    "print(\"\\nWeight gradients shape:\", delta_w.shape)\n",
    "print(\"Bias gradients:\", delta_b)\n",
    "print(\"Input gradients shape:\", delta_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Stride=1 ===\n",
      "Output shape (stride=1): (1, 3, 4)\n",
      "\n",
      "=== Test 2: Stride=2 ===\n",
      "Output shape (stride=2): (1, 3, 3)\n",
      "\n",
      "=== Test 3: Stride=3 ===\n",
      "Output shape (stride=3): (1, 3, 2)\n",
      "\n",
      "=== Backward Pass Test ===\n",
      "Weight gradients shape: (3, 2, 3)\n",
      "Bias gradients: [2. 2. 2.]\n",
      "Input gradients shape: (1, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            out_channels: Number of output channels\n",
    "            kernel_size: Size of the convolution kernel\n",
    "            stride: Stride of the convolution (can be any positive integer)\n",
    "            padding: Number of zeros to pad on each side\n",
    "        \"\"\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # \n",
    "        self.w = np.random.randn(out_channels, in_channels, kernel_size) * 0.1\n",
    "        self.b = np.random.randn(out_channels) * 0.1\n",
    "        \n",
    "        # \n",
    "        self.input_shape = None\n",
    "        self.x_padded = None\n",
    "        self.windows = None \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with arbitrary stride support\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # \n",
    "        if self.padding > 0:\n",
    "            self.x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')\n",
    "        else:\n",
    "            self.x_padded = x\n",
    "        \n",
    "        self.input_shape = x.shape\n",
    "        \n",
    "        num_features = x.shape[2]\n",
    "        num_output_features = ((num_features + 2*self.padding - self.kernel_size) // self.stride) + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, self.out_channels, num_output_features))\n",
    "        self.windows = np.zeros((batch_size, num_output_features, self.in_channels, self.kernel_size))\n",
    "        \n",
    "        # \n",
    "        for i in range(num_output_features):\n",
    "            start = i * self.stride\n",
    "            window = self.x_padded[:, :, start:start+self.kernel_size]\n",
    "            self.windows[:, i, :, :] = window\n",
    "            \n",
    "            #\n",
    "            output[:, :, i] = np.tensordot(window, self.w, axes=([1,2], [1,2])) + self.b\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, delta_a):\n",
    "        \"\"\"Backward pass with arbitrary stride support\"\"\"\n",
    "        batch_size = delta_a.shape[0]\n",
    "        delta_w = np.zeros_like(self.w)\n",
    "        delta_b = np.sum(delta_a, axis=(0, 2))\n",
    "        \n",
    "        #\n",
    "        delta_x_padded = np.zeros_like(self.x_padded)\n",
    "        \n",
    "        num_output_features = delta_a.shape[2]\n",
    "        \n",
    "        #\n",
    "        delta_w = np.tensordot(\n",
    "            delta_a.transpose(1, 0, 2).reshape(self.out_channels, -1),\n",
    "            self.windows.reshape(-1, self.in_channels * self.kernel_size),\n",
    "            axes=([1], [0])\n",
    "        ).reshape(self.w.shape)\n",
    "        \n",
    "        #\n",
    "        for i in range(num_output_features):\n",
    "            start = i * self.stride\n",
    "            # \n",
    "            delta_x_padded[:, :, start:start+self.kernel_size] += np.einsum(\n",
    "                'bo,oik->bik', \n",
    "                delta_a[:, :, i], \n",
    "                self.w\n",
    "            )\n",
    "        \n",
    "        #\n",
    "        if self.padding > 0:\n",
    "            delta_x = delta_x_padded[:, :, self.padding:-self.padding]\n",
    "        else:\n",
    "            delta_x = delta_x_padded\n",
    "        \n",
    "        return delta_w, delta_b, delta_x\n",
    "\n",
    "#\n",
    "print(\"=== Test 1: Stride=1 ===\")\n",
    "x1 = np.array([[[1,2,3,4], [5,6,7,8]]], dtype=np.float64)  \n",
    "conv1 = Conv1d(2, 3, kernel_size=3, stride=1, padding=1)\n",
    "conv1.w = np.ones_like(conv1.w)\n",
    "conv1.b = np.zeros_like(conv1.b)\n",
    "out1 = conv1.forward(x1)\n",
    "print(\"Output shape (stride=1):\", out1.shape)\n",
    "\n",
    "print(\"\\n=== Test 2: Stride=2 ===\")\n",
    "x2 = np.array([[[1,2,3,4,5], [6,7,8,9,10]]], dtype=np.float64)  \n",
    "conv2 = Conv1d(2, 3, kernel_size=3, stride=2, padding=1)\n",
    "conv2.w = np.ones_like(conv2.w)\n",
    "conv2.b = np.zeros_like(conv2.b)\n",
    "out2 = conv2.forward(x2)\n",
    "print(\"Output shape (stride=2):\", out2.shape)\n",
    "\n",
    "print(\"\\n=== Test 3: Stride=3 ===\")\n",
    "x3 = np.array([[[1,2,3,4,5,6], [7,8,9,10,11,12]]], dtype=np.float64)  \n",
    "conv3 = Conv1d(2, 3, kernel_size=3, stride=3, padding=1)\n",
    "conv3.w = np.ones_like(conv3.w)\n",
    "conv3.b = np.zeros_like(conv3.b)\n",
    "out3 = conv3.forward(x3)\n",
    "print(\"Output shape (stride=3):\", out3.shape)\n",
    "\n",
    "\n",
    "print(\"\\n=== Backward Pass Test ===\")\n",
    "delta_a = np.ones_like(out3)\n",
    "delta_w, delta_b, delta_x = conv3.backward(delta_a)\n",
    "print(\"Weight gradients shape:\", delta_w.shape)\n",
    "print(\"Bias gradients:\", delta_b)\n",
    "print(\"Input gradients shape:\", delta_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 0.0982\n",
      "Epoch 2, Accuracy: 0.1135\n",
      "Epoch 3, Accuracy: 0.1010\n",
      "Epoch 4, Accuracy: 0.1135\n",
      "Epoch 5, Accuracy: 0.1010\n",
      "\n",
      "Final Test Accuracy: 0.1010\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 1, 28*28).astype(np.float32) / 255.0  # (60000, 1, 784)\n",
    "X_test = X_test.reshape(-1, 1, 28*28).astype(np.float32) / 255.0     # (10000, 1, 784)\n",
    "\n",
    "# \n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "#\n",
    "class HeInitializer:\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2/n_nodes1)\n",
    "    def B(self, n_nodes2):\n",
    "        return np.zeros(n_nodes2)\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "#\n",
    "class Conv1d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        #\n",
    "        self.w = np.random.randn(out_channels, in_channels, kernel_size) * 0.1\n",
    "        self.b = np.random.randn(out_channels) * 0.1\n",
    "        \n",
    "        #\n",
    "        self.input_shape = None\n",
    "        self.x_padded = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Apply padding\n",
    "        if self.padding > 0:\n",
    "            self.x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding)), mode='constant')\n",
    "        else:\n",
    "            self.x_padded = x\n",
    "        \n",
    "        self.input_shape = x.shape\n",
    "        \n",
    "        num_features = x.shape[2]\n",
    "        num_output_features = ((num_features + 2*self.padding - self.kernel_size) // self.stride) + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, self.out_channels, num_output_features))\n",
    "        \n",
    "        for i in range(num_output_features):\n",
    "            start = i * self.stride\n",
    "            window = self.x_padded[:, :, start:start+self.kernel_size]\n",
    "            output[:, :, i] = np.sum(window[:, np.newaxis, :, :] * self.w[np.newaxis, :, :, :], axis=(2,3)) + self.b\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, delta_a):\n",
    "        batch_size = delta_a.shape[0]\n",
    "        delta_w = np.zeros_like(self.w)\n",
    "        delta_b = np.sum(delta_a, axis=(0, 2))\n",
    "        \n",
    "        delta_x_padded = np.zeros_like(self.x_padded)\n",
    "        num_output_features = delta_a.shape[2]\n",
    "        \n",
    "        for i in range(num_output_features):\n",
    "            start = i * self.stride\n",
    "            window = self.x_padded[:, :, start:start+self.kernel_size]\n",
    "            \n",
    "            # Weight gradients\n",
    "            delta_w += np.sum(delta_a[:, :, i][:, :, np.newaxis, np.newaxis] * window[:, np.newaxis, :, :], axis=0)\n",
    "            \n",
    "            # Input gradients\n",
    "            delta_x_padded[:, :, start:start+self.kernel_size] += np.sum(\n",
    "                delta_a[:, :, i][:, :, np.newaxis] * self.w[np.newaxis, :, :, :], axis=1)\n",
    "        \n",
    "        # Remove padding\n",
    "        if self.padding > 0:\n",
    "            delta_x = delta_x_padded[:, :, self.padding:-self.padding]\n",
    "        else:\n",
    "            delta_x = delta_x_padded\n",
    "        \n",
    "        return delta_w, delta_b, delta_x\n",
    "\n",
    "#\n",
    "class FC:\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.optimizer = optimizer\n",
    "        self.X = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return X @ self.W + self.B\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        dZ = dA @ self.W.T\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = self.X.T @ dA\n",
    "        self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "#\n",
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.maximum(0, A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * (self.A > 0)\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.Z = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        batch_size = Y.shape[0]\n",
    "        return self.Z[:batch_size] - Y\n",
    "\n",
    "# \n",
    "class CNNClassifier:\n",
    "    def __init__(self, learning_rate=0.01, batch_size=100):\n",
    "        self.batch_size = batch_size\n",
    "        self.conv = Conv1d(in_channels=1, out_channels=4, kernel_size=7, stride=1, padding=3)  # Changed stride to 1\n",
    "        self.fc1 = FC(4*784, 128, HeInitializer(), SGD(learning_rate))  # Changed to 4*784\n",
    "        self.fc2 = FC(128, 10, HeInitializer(), SGD(learning_rate))\n",
    "        self.relu = ReLU()\n",
    "        self.softmax = Softmax()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Conv1d forward\n",
    "        conv_out = self.conv.forward(X)  # Output shape: (batch, 4, 784)\n",
    "        \n",
    "        # Flatten for FC layers\n",
    "        flattened = conv_out.reshape(X.shape[0], -1)  # Flatten all but batch dimension\n",
    "        \n",
    "        # FC layers\n",
    "        A1 = self.fc1.forward(flattened)\n",
    "        Z1 = self.relu.forward(A1)\n",
    "        A2 = self.fc2.forward(Z1)\n",
    "        Z2 = self.softmax.forward(A2)\n",
    "        return Z2\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        batch_size = Y.shape[0]\n",
    "        dA2 = self.softmax.backward(Y)\n",
    "        dZ1 = self.fc2.backward(dA2)\n",
    "        dA1 = self.relu.backward(dZ1)\n",
    "        dX = self.fc1.backward(dA1)\n",
    "        \n",
    "        # Reshape back to conv output dimensions\n",
    "        dX_reshaped = dX.reshape(batch_size, 4, 784)\n",
    "        return dX_reshaped\n",
    "    \n",
    "    def train(self, X, y, epochs=5):\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            indices = np.random.permutation(len(X))\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(0, len(X), self.batch_size):\n",
    "                batch_X = X_shuffled[i:i+self.batch_size]\n",
    "                batch_y = y_shuffled[i:i+self.batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.forward(batch_X)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(batch_y)\n",
    "            \n",
    "            # Calculate accuracy after each epoch\n",
    "            pred = self.predict(X_test)\n",
    "            acc = accuracy_score(y_test, pred)\n",
    "            print(f\"Epoch {epoch+1}, Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.forward(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "# Initialize and train\n",
    "cnn = CNNClassifier(learning_rate=0.01, batch_size=100)\n",
    "cnn.train(X_train, y_train_onehot, epochs=5)\n",
    "\n",
    "# \n",
    "y_pred = cnn.predict(X_test)\n",
    "final_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nFinal Test Accuracy: {final_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
