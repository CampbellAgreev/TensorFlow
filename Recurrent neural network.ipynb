{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Forward Propagation with Small Array ---\n",
      "Manual Calculation Steps:\n",
      "\n",
      "Time step t=0\n",
      "  Input x_t: [[0.01 0.02]]\n",
      "  Previous h_-1: [[0. 0. 0. 0.]]\n",
      "  x_t @ Wx = [[0.0007 0.0013 0.0019 0.0023]]\n",
      "  h_-1 @ Wh = [[0. 0. 0. 0.]]\n",
      "  Bias B = [1 1 1 1]\n",
      "  Pre-activation a_t: [[1.0007 1.0013 1.0019 1.0023]]\n",
      "  Activation h_t = tanh(a_t): [[0.76188798 0.76213958 0.76239095 0.76255841]]\n",
      "\n",
      "Time step t=1\n",
      "  Input x_t: [[0.02 0.03]]\n",
      "  Previous h_0: [[0.76188798 0.76213958 0.76239095 0.76255841]]\n",
      "  x_t @ Wx = [[0.0011 0.0021 0.0031 0.0038]]\n",
      "  h_0 @ Wh = [[0.07623574 0.13721527 0.19819481 0.25155044]]\n",
      "  Bias B = [1 1 1 1]\n",
      "  Pre-activation a_t: [[1.07733574 1.13931527 1.20129481 1.25535044]]\n",
      "  Activation h_t = tanh(a_t): [[0.792209   0.8141834  0.83404912 0.84977719]]\n",
      "\n",
      "Time step t=2\n",
      "  Input x_t: [[0.03 0.04]]\n",
      "  Previous h_1: [[0.792209   0.8141834  0.83404912 0.84977719]]\n",
      "  x_t @ Wx = [[0.0015 0.0029 0.0043 0.0053]]\n",
      "  h_1 @ Wh = [[0.08321832 0.14902269 0.21482707 0.27229095]]\n",
      "  Bias B = [1 1 1 1]\n",
      "  Pre-activation a_t: [[1.08471832 1.15192269 1.21912707 1.27759095]]\n",
      "  Activation h_t = tanh(a_t): [[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "\n",
      "--- Final Hidden State (Manual Calculation) ---\n",
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "\n",
      "--- Expected Output from Text ---\n",
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "\n",
      "--- Verifying with Class Method (using fixed weights) ---\n",
      "\n",
      "--- Final Hidden State (Class Method _forward_propagation) ---\n",
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "\n",
      "--- Comparison ---\n",
      "Match: True\n",
      "Match with Expected: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScratchSimpleRNNClassifier:\n",
    "    \"\"\"\n",
    "    Simple Recurrent Neural Network Classifier from scratch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes : int\n",
    "        Number of nodes (neurons) in the RNN hidden layer.\n",
    "    n_output : int\n",
    "        Number of output classes.\n",
    "    n_epochs : int\n",
    "        Number of training epochs.\n",
    "    learning_rate : float\n",
    "        Learning rate for weight updates (alpha).\n",
    "    batch_size : int\n",
    "        Number of samples per batch during training.\n",
    "    sigma : float\n",
    "        Standard deviation for weight initialization (Gaussian distribution).\n",
    "    verbose : bool\n",
    "        If True, print loss during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes=50, n_output=1, n_epochs=10, learning_rate=0.01,\n",
    "                 batch_size=20, sigma=0.01, verbose=False):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_output = n_output \n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.Wx = None \n",
    "        self.Wh = None \n",
    "        self.B = None  \n",
    "        self.Wy = None \n",
    "        self.By = None \n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "    def _tanh(self, a):\n",
    "        \"\"\"Tanh activation function.\"\"\"\n",
    "        return np.tanh(a)\n",
    "\n",
    "    def _softmax(self, a):\n",
    "        \"\"\"Softmax activation function.\"\"\"\n",
    "    \n",
    "        a = a - np.max(a, axis=-1, keepdims=True)\n",
    "        exp_a = np.exp(a)\n",
    "        return exp_a / np.sum(exp_a, axis=-1, keepdims=True)\n",
    "\n",
    "    def _cross_entropy_loss(self, y_pred_proba, y_true_one_hot):\n",
    "        \"\"\"Cross-entropy loss calculation.\"\"\"\n",
    "        # Avoid log(0)\n",
    "        epsilon = 1e-12\n",
    "        y_pred_proba = np.clip(y_pred_proba, epsilon, 1. - epsilon)\n",
    "        log_likelihood = -np.sum(y_true_one_hot * np.log(y_pred_proba), axis=1)\n",
    "        loss = np.mean(log_likelihood)\n",
    "        return loss\n",
    "\n",
    "    def _initialize_weights(self, n_features):\n",
    "        \"\"\"Initialize weights using Gaussian distribution.\"\"\"\n",
    "        self.Wx = self.sigma * np.random.randn(n_features, self.n_nodes)\n",
    "        self.Wh = self.sigma * np.random.randn(self.n_nodes, self.n_nodes)\n",
    "        self.B = np.zeros(self.n_nodes)\n",
    "        self.Wy = self.sigma * np.random.randn(self.n_nodes, self.n_output)\n",
    "        self.By = np.zeros(self.n_output)\n",
    "\n",
    "    def _forward_propagation(self, X_batch):\n",
    "        \"\"\"\n",
    "        Perform forward propagation for one batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_batch : ndarray, shape (batch_size, n_sequences, n_features)\n",
    "            Input data for the batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        h_states : ndarray, shape (batch_size, n_sequences + 1, n_nodes)\n",
    "            Hidden states at each time step (including initial h0).\n",
    "        a_states : ndarray, shape (batch_size, n_sequences, n_nodes)\n",
    "            Pre-activation states at each time step.\n",
    "        final_h : ndarray, shape (batch_size, n_nodes)\n",
    "            Hidden state after the last sequence.\n",
    "        output_a : ndarray, shape (batch_size, n_output)\n",
    "             Pre-activation state of the output layer.\n",
    "        y_pred_proba : ndarray, shape (batch_size, n_output)\n",
    "            Predicted probabilities after softmax.\n",
    "        \"\"\"\n",
    "        batch_size, n_sequences, n_features = X_batch.shape\n",
    "\n",
    "        #\n",
    "        h_prev = np.zeros((batch_size, self.n_nodes))\n",
    "\n",
    "        #\n",
    "        h_states = np.zeros((batch_size, n_sequences + 1, self.n_nodes))\n",
    "        a_states = np.zeros((batch_size, n_sequences, self.n_nodes))\n",
    "        h_states[:, 0, :] = h_prev # Store initial h0\n",
    "\n",
    "        # RNN forward pass through time\n",
    "        for t in range(n_sequences):\n",
    "            xt = X_batch[:, t, :] # Input at time t (batch_size, n_features)\n",
    "            # Calculate pre-activation state 'a' at time t\n",
    "            at = xt @ self.Wx + h_prev @ self.Wh + self.B\n",
    "            # Calculate hidden state 'h' at time t using tanh\n",
    "            ht = self._tanh(at)\n",
    "\n",
    "            # Store states\n",
    "            a_states[:, t, :] = at\n",
    "            h_states[:, t + 1, :] = ht\n",
    "\n",
    "            # Update previous hidden state for next iteration\n",
    "            h_prev = ht\n",
    "\n",
    "        # Fully connected output layer \n",
    "        final_h = h_states[:, -1, :] \n",
    "        output_a = final_h @ self.Wy + self.By \n",
    "\n",
    "        # Apply softmax activation for classification output\n",
    "        y_pred_proba = self._softmax(output_a)\n",
    "\n",
    "        return h_states, a_states, final_h, output_a, y_pred_proba\n",
    "\n",
    "    def _backward_propagation(self, X_batch, y_true_one_hot, h_states, a_states, y_pred_proba):\n",
    "        \"\"\"\n",
    "        Perform backward propagation (BPTT) for one batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_batch : ndarray, shape (batch_size, n_sequences, n_features)\n",
    "            Input data for the batch.\n",
    "        y_true_one_hot : ndarray, shape (batch_size, n_output)\n",
    "            True labels in one-hot encoded format.\n",
    "        h_states : ndarray, shape (batch_size, n_sequences + 1, n_nodes)\n",
    "            Hidden states from forward pass.\n",
    "        a_states : ndarray, shape (batch_size, n_sequences, n_nodes)\n",
    "            Pre-activation states from forward pass.\n",
    "        y_pred_proba : ndarray, shape (batch_size, n_output)\n",
    "            Predicted probabilities from forward pass.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dWx : ndarray, shape (n_features, n_nodes)\n",
    "            Gradient of loss w.r.t. Wx.\n",
    "        dWh : ndarray, shape (n_nodes, n_nodes)\n",
    "            Gradient of loss w.r.t. Wh.\n",
    "        dB : ndarray, shape (n_nodes,)\n",
    "            Gradient of loss w.r.t. B.\n",
    "        dWy : ndarray, shape (n_nodes, n_output)\n",
    "            Gradient of loss w.r.t. Wy.\n",
    "        dBy : ndarray, shape (n_output,)\n",
    "            Gradient of loss w.r.t. By.\n",
    "        \"\"\"\n",
    "        batch_size, n_sequences, n_features = X_batch.shape\n",
    "\n",
    "        # Initialize gradients\n",
    "        dWx = np.zeros_like(self.Wx)\n",
    "        dWh = np.zeros_like(self.Wh)\n",
    "        dB = np.zeros_like(self.B)\n",
    "        dWy = np.zeros_like(self.Wy)\n",
    "        dBy = np.zeros_like(self.By)\n",
    "\n",
    "        \n",
    "        #\n",
    "        delta_out = y_pred_proba - y_true_one_hot \n",
    "\n",
    "        # Gradient of loss w.r.t. Wy (dL/dWy)\n",
    "        final_h = h_states[:, -1, :] \n",
    "        dWy = final_h.T @ delta_out \n",
    "\n",
    "        # Gradient of loss w.r.t. By (dL/dBy)\n",
    "        dBy = np.sum(delta_out, axis=0) \n",
    "\n",
    "        # \n",
    "        dh_next = delta_out @ self.Wy.T \n",
    "\n",
    "        #Backpropagation Through Time (BPTT)\n",
    "        \n",
    "        for t in reversed(range(n_sequences)):\n",
    "           \n",
    "            ht = h_states[:, t + 1, :] \n",
    "            at = a_states[:, t, :]   \n",
    "            tanh_derivative = 1 - ht**2 \n",
    "            \n",
    "            delta_a = dh_next * tanh_derivative \n",
    "\n",
    "            #\n",
    "            \n",
    "            dB += np.sum(delta_a, axis=0) \n",
    "\n",
    "            # dL/dWx_t = x_t.T @ (dL/da_t)\n",
    "            xt = X_batch[:, t, :] \n",
    "            dWx += xt.T @ delta_a \n",
    "\n",
    "            # dL/dWh_t = h_{t-1}.T @ (dL/da_t)\n",
    "            h_prev = h_states[:, t, :] \n",
    "            dWh += h_prev.T @ delta_a \n",
    "\n",
    "            \n",
    "            # dL/dh_{t-1} = (dL/da_t) @ Wh.T\n",
    "            dh_next = delta_a @ self.Wh.T # (batch_size, n_nodes) @ (n_nodes, n_nodes) -> (batch_size, n_nodes)\n",
    "\n",
    "        \n",
    "\n",
    "        return dWx, dWh, dB, dWy, dBy\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the RNN classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_sequences, n_features)\n",
    "            Training data.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Target labels (integers from 0 to n_output-1).\n",
    "        \"\"\"\n",
    "        n_samples, n_sequences, n_features = X.shape\n",
    "\n",
    "        # Initialize weights \n",
    "        if self.Wx is None:\n",
    "            self._initialize_weights(n_features)\n",
    "\n",
    "        # One-hot encode target labels\n",
    "        y_true_one_hot = np.eye(self.n_output)[y] \n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.n_epochs):\n",
    "            epoch_loss = 0\n",
    "            # Mini-batch processing\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_true_one_hot_shuffled = y_true_one_hot[permutation]\n",
    "\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                \n",
    "                X_batch = X_shuffled[i : i + self.batch_size]\n",
    "                y_batch_one_hot = y_true_one_hot_shuffled[i : i + self.batch_size]\n",
    "                current_batch_size = X_batch.shape[0] \n",
    "\n",
    "                # \n",
    "                h_states, a_states, _, _, y_pred_proba = self._forward_propagation(X_batch)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self._cross_entropy_loss(y_pred_proba, y_batch_one_hot)\n",
    "                epoch_loss += loss * current_batch_size # Accumulate total loss for epoch avg\n",
    "\n",
    "                # Backward propagation\n",
    "                dWx, dWh, dB, dWy, dBy = self._backward_propagation(\n",
    "                    X_batch, y_batch_one_hot, h_states, a_states, y_pred_proba\n",
    "                )\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.Wx -= self.lr * dWx / current_batch_size \n",
    "                self.Wh -= self.lr * dWh / current_batch_size\n",
    "                self.B  -= self.lr * dB / current_batch_size\n",
    "                self.Wy -= self.lr * dWy / current_batch_size\n",
    "                self.By -= self.lr * dBy / current_batch_size\n",
    "\n",
    "            # Calculate average loss for the epoch\n",
    "            average_epoch_loss = epoch_loss / n_samples\n",
    "            self.loss_history.append(average_epoch_loss)\n",
    "\n",
    "            if self.verbose:\n",
    "                 print(f\"Epoch {epoch+1}/{self.n_epochs}, Loss: {average_epoch_loss:.4f}\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for input samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_sequences, n_features)\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred_proba : ndarray, shape (n_samples, n_output)\n",
    "            Predicted probabilities.\n",
    "        \"\"\"\n",
    "        # \n",
    "        _, _, _, _, y_pred_proba = self._forward_propagation(X)\n",
    "        return y_pred_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for input samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_sequences, n_features)\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        # \n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "#Verification with the small array \n",
    "print(\"--- Verifying Forward Propagation with Small Array ---\")\n",
    "\n",
    "#\n",
    "x_small = np.array([[[1, 2], [2, 3], [3, 4]]]) / 100 \n",
    "w_x_small = np.array([[1, 3, 5, 7], [3, 5, 7, 8]]) / 100 \n",
    "w_h_small = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]]) / 100 \n",
    "b_small = np.array([1, 1, 1, 1]) \n",
    "\n",
    "# Get dimensions\n",
    "batch_size_small = x_small.shape[0]\n",
    "n_sequences_small = x_small.shape[1]\n",
    "n_features_small = x_small.shape[2]\n",
    "n_nodes_small = w_x_small.shape[1]\n",
    "\n",
    "# Initialize h0\n",
    "h_prev_small = np.zeros((batch_size_small, n_nodes_small)) \n",
    "\n",
    "# \n",
    "print(\"Manual Calculation Steps:\")\n",
    "h_states_manual = np.zeros((batch_size_small, n_sequences_small + 1, n_nodes_small))\n",
    "h_states_manual[:, 0, :] = h_prev_small\n",
    "\n",
    "for t in range(n_sequences_small):\n",
    "    xt_small = x_small[:, t, :] \n",
    "    print(f\"\\nTime step t={t}\")\n",
    "    print(f\"  Input x_t: {xt_small}\")\n",
    "    print(f\"  Previous h_{t-1}: {h_prev_small}\")\n",
    "\n",
    "    # Calculate a_t = x_t @ Wx + h_{t-1} @ Wh + B\n",
    "    term1 = xt_small @ w_x_small\n",
    "    term2 = h_prev_small @ w_h_small\n",
    "    a_t_small = term1 + term2 + b_small\n",
    "    print(f\"  x_t @ Wx = {term1}\")\n",
    "    print(f\"  h_{t-1} @ Wh = {term2}\")\n",
    "    print(f\"  Bias B = {b_small}\")\n",
    "    print(f\"  Pre-activation a_t: {a_t_small}\")\n",
    "\n",
    "    # Calculate h_t = tanh(a_t)\n",
    "    h_t_small = np.tanh(a_t_small)\n",
    "    print(f\"  Activation h_t = tanh(a_t): {h_t_small}\")\n",
    "\n",
    "    # Update h_prev for the next step\n",
    "    h_prev_small = h_t_small\n",
    "    h_states_manual[:, t + 1, :] = h_t_small\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Hidden State (Manual Calculation) ---\")\n",
    "final_h_manual = h_prev_small\n",
    "print(final_h_manual)\n",
    "\n",
    "print(\"\\n--- Expected Output from Text ---\")\n",
    "expected_h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])\n",
    "print(expected_h)\n",
    "\n",
    "print(\"\\n--- Verifying with Class Method (using fixed weights) ---\")\n",
    "# \n",
    "rnn_verifier = ScratchSimpleRNNClassifier(n_nodes=n_nodes_small, n_output=1)\n",
    "\n",
    "# \n",
    "rnn_verifier.Wx = w_x_small\n",
    "rnn_verifier.Wh = w_h_small\n",
    "rnn_verifier.B = b_small\n",
    "# \n",
    "rnn_verifier.Wy = np.zeros((n_nodes_small, 1))\n",
    "rnn_verifier.By = np.zeros(1)\n",
    "\n",
    "#\n",
    "h_states_class, a_states_class, final_h_class, _, _ = rnn_verifier._forward_propagation(x_small)\n",
    "\n",
    "print(\"\\n--- Final Hidden State (Class Method _forward_propagation) ---\")\n",
    "print(final_h_class)\n",
    "\n",
    "#\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Match: {np.allclose(final_h_manual, final_h_class)}\") # Should be True\n",
    "print(f\"Match with Expected: {np.allclose(final_h_class, expected_h)}\") # Should be True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
