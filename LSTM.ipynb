{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SimpleRNN IMDb data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build SimpleRNN model...\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2593025 (9.89 MB)\n",
      "Trainable params: 2593025 (9.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Compile SimpleRNN model...\n",
      "Train SimpleRNN model...\n",
      "782/782 [==============================] - 98s 121ms/step - loss: 0.6934 - accuracy: 0.5302 - val_loss: 0.6663 - val_accuracy: 0.5818\n",
      "Evaluate SimpleRNN model...\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.6663 - accuracy: 0.5818\n",
      "Test score: 0.6663140654563904\n",
      "Test accuracy: 0.5817599892616272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "print(\"Loading SimpleRNN IMDb data...\")\n",
    "max_features = 20000\n",
    "maxlen = 80 \n",
    "batch_size = 32\n",
    "epochs = 1 \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build SimpleRNN model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128)) \n",
    "model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Compile SimpleRNN model...')\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train SimpleRNN model...')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Evaluate SimpleRNN model...')\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LSTM IMDb data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build LSTM model...\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2691713 (10.27 MB)\n",
      "Trainable params: 2691713 (10.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Compile LSTM model...\n",
      "Train LSTM model...\n",
      "782/782 [==============================] - 208s 262ms/step - loss: 0.4254 - accuracy: 0.8021 - val_loss: 0.3546 - val_accuracy: 0.8424\n",
      "Evaluate LSTM model...\n",
      "782/782 [==============================] - 24s 30ms/step - loss: 0.3546 - accuracy: 0.8424\n",
      "Test score: 0.35461699962615967\n",
      "Test accuracy: 0.8423600196838379\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM \n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "print(\"Loading LSTM IMDb data...\")\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "epochs = 1 \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build LSTM model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Compile LSTM model...')\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train LSTM model...')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Evaluate LSTM model...')\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting parameters...\n",
      "Loading GRU IMDb data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time) to a fixed length...\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build GRU model...\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2659201 (10.14 MB)\n",
      "Trainable params: 2659201 (10.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Compile GRU model...\n",
      "\n",
      "Train GRU model...\n",
      "782/782 [==============================] - 207s 261ms/step - loss: 0.4542 - accuracy: 0.7781 - val_loss: 0.3668 - val_accuracy: 0.8386\n",
      "\n",
      "Evaluate GRU model...\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.3668 - accuracy: 0.8386\n",
      "\n",
      "Test score: 0.3667726516723633\n",
      "Test accuracy: 0.8386399745941162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU\n",
    "from keras.datasets import imdb\n",
    "\n",
    "print(\"Setting parameters...\")\n",
    "max_features = 20000  \n",
    "maxlen = 80         \n",
    "batch_size = 32\n",
    "epochs = 1           \n",
    "\n",
    "print(\"Loading GRU IMDb data...\")\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time) to a fixed length...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "# \n",
    "print('Build GRU model...')\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding Layer\n",
    "model.add(Embedding(max_features, 128)) \n",
    "\n",
    "# GRU Layer\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2)) \n",
    "\n",
    "# Dense Output Layer\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# \n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# \n",
    "print('\\nCompile GRU model...')\n",
    "# \n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',           \n",
    "              metrics=['accuracy'])       \n",
    "\n",
    "\n",
    "print('\\nTrain GRU model...')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "print('\\nEvaluate GRU model...')\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "print('\\nTest score:', score)\n",
    "print('Test accuracy:', acc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video data...\n",
      "Video data generated.\n",
      "noisy_movies shape: (1200, 15, 40, 40, 1)\n",
      "shifted_movies shape: (1200, 15, 40, 40, 1)\n",
      "Building ConvLSTM2D model...\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, None, 40, 40, 40   59200     \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, None, 40, 40, 40   160       \n",
      " chNormalization)            )                                   \n",
      "                                                                 \n",
      " conv_lstm2d_5 (ConvLSTM2D)  (None, None, 40, 40, 40   115360    \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, None, 40, 40, 40   160       \n",
      " chNormalization)            )                                   \n",
      "                                                                 \n",
      " conv_lstm2d_6 (ConvLSTM2D)  (None, None, 40, 40, 40   115360    \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, None, 40, 40, 40   160       \n",
      " chNormalization)            )                                   \n",
      "                                                                 \n",
      " conv_lstm2d_7 (ConvLSTM2D)  (None, None, 40, 40, 40   115360    \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, None, 40, 40, 40   160       \n",
      " chNormalization)            )                                   \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, None, 40, 40, 1)   1081      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407001 (1.55 MB)\n",
      "Trainable params: 406681 (1.55 MB)\n",
      "Non-trainable params: 320 (1.25 KB)\n",
      "_________________________________________________________________\n",
      "Training ConvLSTM2D model (using first 100 samples)...\n",
      "10/10 [==============================] - 294s 26s/step - loss: 0.8386 - val_loss: 0.6914\n",
      "ConvLSTM2D training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, ConvLSTM2D, BatchNormalization\n",
    "import numpy as np\n",
    "import pylab as plt \n",
    "\n",
    "print(\"Generating video data...\")\n",
    "def generate_movies(n_samples=1200, n_frames=15):\n",
    "    row = 80\n",
    "    col = 80\n",
    "\n",
    "    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float32)\n",
    "    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        \n",
    "        n = np.random.randint(3, 8)\n",
    "        for j in range(n):\n",
    "            xstart = np.random.randint(20, 60)\n",
    "            ystart = np.random.randint(20, 60)\n",
    "            directionx = np.random.randint(0, 3) - 1\n",
    "            directiony = np.random.randint(0, 3) - 1\n",
    "            \n",
    "            w = np.random.randint(2, 4)\n",
    "            for t in range(n_frames):\n",
    "                x_shift = xstart + directionx * t\n",
    "                y_shift = ystart + directiony * t\n",
    "                noisy_movies[i, t, x_shift - w: x_shift + w,\n",
    "                             y_shift - w: y_shift + w, 0] += 1\n",
    "                if np.random.randint(0, 2):\n",
    "                    noise_f = (-1)**np.random.randint(0, 2)\n",
    "                    noisy_movies[i, t,\n",
    "                                 x_shift - w - 1: x_shift + w + 1,\n",
    "                                 y_shift - w - 1: y_shift + w + 1, 0] += noise_f * 0.1\n",
    "                \n",
    "                x_shift = xstart + directionx * (t + 1)\n",
    "                y_shift = ystart + directiony * (t + 1)\n",
    "                shifted_movies[i, t, x_shift - w: x_shift + w,\n",
    "                               y_shift - w: y_shift + w, 0] += 1\n",
    "    \n",
    "    noisy_movies = noisy_movies[:, :, 20:60, 20:60, :]\n",
    "    shifted_movies = shifted_movies[:, :, 20:60, 20:60, :]\n",
    "    noisy_movies[noisy_movies >= 1] = 1\n",
    "    shifted_movies[shifted_movies >= 1] = 1\n",
    "    return noisy_movies, shifted_movies\n",
    "\n",
    "\n",
    "noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n",
    "print(\"Video data generated.\")\n",
    "print(\"noisy_movies shape:\", noisy_movies.shape) \n",
    "print(\"shifted_movies shape:\", shifted_movies.shape)\n",
    "\n",
    "print(\"Building ConvLSTM2D model...\")\n",
    "seq = Sequential()\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   input_shape=(None, 40, 40, 1), \n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last'))\n",
    "\n",
    "\n",
    "seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
    "seq.summary()\n",
    "\n",
    "print(\"Training ConvLSTM2D model (using first 100 samples)...\")\n",
    "seq.fit(noisy_movies[:100], shifted_movies[:100], batch_size=10,\n",
    "        epochs=1, validation_split=0.05) \n",
    "\n",
    "print(\"ConvLSTM2D training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Reuters data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "46 classes\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (8982, 100)\n",
      "x_test shape: (2246, 100)\n",
      "Convert class vectors to binary class matrices (one-hot encoding)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n",
      "Build LSTM model for Reuters...\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, None, 128)         1280000   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 46)                5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1417518 (5.41 MB)\n",
      "Trainable params: 1417518 (5.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Compile LSTM model for Reuters...\n",
      "Train LSTM model on Reuters...\n",
      "Epoch 1/5\n",
      "281/281 [==============================] - 114s 375ms/step - loss: 2.1859 - accuracy: 0.4295 - val_loss: 1.7673 - val_accuracy: 0.5423\n",
      "Epoch 2/5\n",
      "281/281 [==============================] - 117s 418ms/step - loss: 1.6588 - accuracy: 0.5689 - val_loss: 1.6279 - val_accuracy: 0.5993\n",
      "Epoch 3/5\n",
      "281/281 [==============================] - 98s 347ms/step - loss: 1.4131 - accuracy: 0.6381 - val_loss: 1.5114 - val_accuracy: 0.6309\n",
      "Epoch 4/5\n",
      "281/281 [==============================] - 84s 298ms/step - loss: 1.1668 - accuracy: 0.6959 - val_loss: 1.4448 - val_accuracy: 0.6362\n",
      "Epoch 5/5\n",
      "281/281 [==============================] - 83s 295ms/step - loss: 1.0182 - accuracy: 0.7352 - val_loss: 1.4079 - val_accuracy: 0.6723\n",
      "Evaluate LSTM model on Reuters...\n",
      "71/71 [==============================] - 4s 49ms/step - loss: 1.4079 - accuracy: 0.6723\n",
      "Test score: 1.4079194068908691\n",
      "Test accuracy: 0.6723062992095947\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.datasets import reuters \n",
    "from keras.utils import to_categorical \n",
    "\n",
    "print(\"Loading Reuters data...\")\n",
    "max_features = 10000 \n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "epochs = 5 \n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vectors to binary class matrices (one-hot encoding)')\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('Build LSTM model for Reuters...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128)) \n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(num_classes, activation='softmax')) \n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Compile LSTM model for Reuters...')\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train LSTM model on Reuters...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Evaluate LSTM model on Reuters...')\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`RNN`: \n",
    "\n",
    "This is the fundamental base class for all recurrent layers in Keras (like `SimpleRNN`, `LSTM`, `GRU`). It provides the core structure and methods needed to process sequences step-by-step, manage states, and apply the recurrent logic defined by specific cell types. You typically don't use this class directly unless you are creating a completely custom recurrent layer.\n",
    "\n",
    "`SimpleRNNCell`: \n",
    "\n",
    "This class contains the actual computation logic performed within a single time step of a `SimpleRNN` layer. It defines how the input at the current time step and the hidden state from the previous time step are combined to produce the output and the new hidden state for the current time step. The `SimpleRNN` layer acts as a wrapper that iterates this cell's logic over the entire input sequence.\n",
    "\n",
    "`GRUCell`: \n",
    "\n",
    "Similar to `SimpleRNNCell`, this defines the computation logic for a single time step of a `GRU` layer. This includes the calculations for its specific gates (update and reset gates) which control the information flow. The `GRU` layer wraps this cell and applies it sequentially.\n",
    "\n",
    "`LSTMCell`: \n",
    "\n",
    "This defines the computation logic for a single time step of an `LSTM` layer. It includes the calculations for the three gates (forget, input, output) and the update mechanism for the internal cell state ($C_t$). The `LSTM` layer wraps this cell. Using these individual `Cell` classes directly can be useful if you need more fine-grained control over the recurrent loop or want to build custom RNN structures.\n",
    "\n",
    "`StackedRNNCells`: \n",
    "\n",
    "This is a helper wrapper class. It takes a list of RNN cell instances e.g., `[LSTMCell(units), LSTMCell(units)]` and makes them behave like a single cell. This is useful for easily creating stacked RNNs, where the output of one layer feeds directly into the input of the next layer within the same time step, before the whole stack processes the next time step in the sequence.\n",
    "\n",
    "`CuDNNGRU` / `CuDNNLSTM`: \n",
    "\n",
    "These were highly optimized implementations of the `GRU` and `LSTM` layers specifically designed to leverage NVIDIA's cuDNN library for maximum speed on compatible NVIDIA GPUs. In more recent versions of TensorFlow/Keras, the standard `LSTM` and `GRU` layers often automatically detect and use cuDNN optimizations when possible, making these separate classes less frequently needed explicitly. However, they represent the underlying fast, GPU-accelerated implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
