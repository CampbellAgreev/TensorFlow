{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gini_impurity(y):\n",
    "    \"\"\"\n",
    "    Calculate Gini impurity for a node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        Array of class labels for samples in the node\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    impurity : float\n",
    "        Gini impurity of the node\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "        \n",
    "    #occurrences of each class\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    proportions = counts / len(y)\n",
    "    \n",
    "    #Gini impurity\n",
    "    impurity = 1 - np.sum(proportions ** 2)\n",
    "    \n",
    "    return impurity\n",
    "\n",
    "class ScratchDecesionTreeClassifierDepth1():\n",
    "    \"\"\"\n",
    "    Depth 1 decision tree classifier scratch implementation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose : bool\n",
    "      True to output the learning process\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=False):\n",
    "        #hyperparameters as attributes\n",
    "        self.verbose = verbose\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left_class = None\n",
    "        self.right_class = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the decision tree classifier\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize best parameters\n",
    "        best_gini = float('inf')\n",
    "        best_feature = 0\n",
    "        best_threshold = 0\n",
    "        \n",
    "        #features and possible thresholds\n",
    "        for feature in range(n_features):\n",
    "            #unique values in this feature to try as thresholds\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                # Split data\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                y_left = y[left_mask]\n",
    "                y_right = y[right_mask]\n",
    "                \n",
    "                # Skip if split results in empty node\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate weighted Gini impurity\n",
    "                gini_left = gini_impurity(y_left)\n",
    "                gini_right = gini_impurity(y_right)\n",
    "                \n",
    "                weighted_gini = (len(y_left) * gini_left + len(y_right) * gini_right) / n_samples\n",
    "                \n",
    "                # Update best split if this one is better\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    self.left_class = self._most_common_class(y_left)\n",
    "                    self.right_class = self._most_common_class(y_right)\n",
    "        \n",
    "        # Save the best split parameters\n",
    "        self.feature_index = best_feature\n",
    "        self.threshold = best_threshold\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Best split: Feature {best_feature} <= {best_threshold}\")\n",
    "            print(f\"Left node class: {self.left_class}, Right node class: {self.right_class}\")\n",
    "\n",
    "    def _most_common_class(self, y):\n",
    "        \"\"\"Helper function to find the most common class in a node\"\"\"\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return unique[np.argmax(counts)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the label using a decision tree classifier\n",
    "        \"\"\"\n",
    "        if self.feature_index is None or self.threshold is None:\n",
    "            raise Exception(\"Model not fitted yet\")\n",
    "            \n",
    "        # Initialize predictions\n",
    "        predictions = np.empty(X.shape[0], dtype=type(self.left_class))\n",
    "        \n",
    "        # Make predictions based on the learned split\n",
    "        left_mask = X[:, self.feature_index] <= self.threshold\n",
    "        predictions[left_mask] = self.left_class\n",
    "        predictions[~left_mask] = self.right_class\n",
    "        \n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(parent_y, left_y, right_y):\n",
    "    \"\"\"\n",
    "    Calculate information gain for a split using Gini impurity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_y : ndarray\n",
    "        Array of class labels in parent node\n",
    "    left_y : ndarray\n",
    "        Array of class labels in left child node\n",
    "    right_y : ndarray\n",
    "        Array of class labels in right child node\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    gain : float\n",
    "        Information gain of the split\n",
    "    \"\"\"\n",
    "    #parent impurity\n",
    "    parent_impurity = gini_impurity(parent_y)\n",
    "    \n",
    "    #child impurities\n",
    "    left_impurity = gini_impurity(left_y)\n",
    "    right_impurity = gini_impurity(right_y)\n",
    "    \n",
    "    #weights\n",
    "    n_parent = len(parent_y)\n",
    "    n_left = len(left_y)\n",
    "    n_right = len(right_y)\n",
    "    \n",
    "    #weighted average of child impurities\n",
    "    weighted_child_impurity = (n_left * left_impurity + n_right * right_impurity) / n_parent\n",
    "    \n",
    "    # Information gain is the reduction in impurity\n",
    "    gain = parent_impurity - weighted_child_impurity\n",
    "    \n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "class ScratchDecesionTreeClassifierDepth1():\n",
    "    \"\"\"\n",
    "    Depth 1 decision tree classifier scratch implementation \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left_class = None\n",
    "        self.right_class = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the decision tree by finding the best split that maximizes information gain\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        #parent impurity \n",
    "        parent_impurity = gini_impurity(y)\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            #unique values as potential thresholds\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                # Split data\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                y_left = y[left_mask]\n",
    "                y_right = y[right_mask]\n",
    "                \n",
    "                # Skip if split would create empty nodes\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate information gain\n",
    "                current_gain = information_gain(y, y_left, y_right)\n",
    "                \n",
    "                # Update best split if this one is better\n",
    "                if current_gain > best_gain:\n",
    "                    best_gain = current_gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    self.left_class = self._most_common_class(y_left)\n",
    "                    self.right_class = self._most_common_class(y_right)\n",
    "        \n",
    "        # Save the best split parameters\n",
    "        self.feature_index = best_feature\n",
    "        self.threshold = best_threshold\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Best split found: Feature {best_feature} <= {best_threshold:.4f}\")\n",
    "            print(f\"Information gain: {best_gain:.4f}\")\n",
    "            print(f\"Left node class: {self.left_class}, Right node class: {self.right_class}\")\n",
    "    \n",
    "    def _most_common_class(self, y):\n",
    "        \"\"\"Helper function to find the most common class in a node\"\"\"\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return unique[np.argmax(counts)]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using the learned decision tree\"\"\"\n",
    "        if self.feature_index is None or self.threshold is None:\n",
    "            raise Exception(\"Model not fitted yet\")\n",
    "            \n",
    "        predictions = np.empty(X.shape[0], dtype=int)\n",
    "        left_mask = X[:, self.feature_index] <= self.threshold\n",
    "        predictions[left_mask] = self.left_class\n",
    "        predictions[~left_mask] = self.right_class\n",
    "        return predictions\n",
    "    \n",
    "    def visualize_decision_boundary(self, X, y, feature_names=None):\n",
    "        \"\"\"\n",
    "        Visualize the decision boundary for 2D data (for demonstration purposes)\n",
    "        Note: Only works when we have exactly 2 features\n",
    "        \"\"\"\n",
    "        if X.shape[1] != 2:\n",
    "            print(\"Visualization only works for 2D data\")\n",
    "            return\n",
    "        \n",
    "        if self.feature_index is None:\n",
    "            print(\"Model not trained yet\")\n",
    "            return\n",
    "        \n",
    "        # \n",
    "        cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
    "        cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n",
    "        \n",
    "        #decision boundary\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        \n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                             np.arange(y_min, y_max, 0.02))\n",
    "        \n",
    "        # Predict for each point in meshgrid\n",
    "        Z = self.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "        \n",
    "        # Plot the training points\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "        \n",
    "        #decision boundary\n",
    "        if self.feature_index == 0:\n",
    "            plt.axvline(x=self.threshold, color='k', linestyle='--')\n",
    "            plt.text(self.threshold+0.1, y_max-0.1, \n",
    "                    f'X{self.feature_index} ≤ {self.threshold:.2f}',\n",
    "                    fontsize=12)\n",
    "        else:\n",
    "            plt.axhline(y=self.threshold, color='k', linestyle='--')\n",
    "            plt.text(x_max-2, self.threshold+0.1, \n",
    "                    f'X{self.feature_index} ≤ {self.threshold:.2f}',\n",
    "                    fontsize=12)\n",
    "        \n",
    "        if feature_names:\n",
    "            plt.xlabel(feature_names[0])\n",
    "            plt.ylabel(feature_names[1])\n",
    "        else:\n",
    "            plt.xlabel('Feature 0')\n",
    "            plt.ylabel('Feature 1')\n",
    "        \n",
    "        plt.title(\"Decision boundary of depth-1 decision tree\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDecesionTreeClassifierDepth1():\n",
    "    \"\"\"\n",
    "    Depth 1 decision tree classifier scratch implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=False):\n",
    "        # Record hyperparameters as attributes\n",
    "        self.verbose = verbose\n",
    "        self.feature_index = None  # Index of feature used for splitting\n",
    "        self.threshold = None      # Threshold value for splitting\n",
    "        self.left_class = None     # Class label for left leaf node\n",
    "        self.right_class = None    # Class label for right leaf node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the decision tree classifier\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        #parent impurity once\n",
    "        parent_impurity = gini_impurity(y)\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            #unique values as potential thresholds\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                # Split data\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                y_left = y[left_mask]\n",
    "                y_right = y[right_mask]\n",
    "                \n",
    "                # Skip if split would create empty nodes\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate information gain\n",
    "                current_gain = information_gain(y, y_left, y_right)\n",
    "                \n",
    "                # Update best split if this one is better\n",
    "                if current_gain > best_gain:\n",
    "                    best_gain = current_gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    self.left_class = self._most_common_class(y_left)\n",
    "                    self.right_class = self._most_common_class(y_right)\n",
    "        \n",
    "        # Save the best split parameters\n",
    "        self.feature_index = best_feature\n",
    "        self.threshold = best_threshold\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Best split found: Feature {best_feature} <= {best_threshold:.4f}\")\n",
    "            print(f\"Information gain: {best_gain:.4f}\")\n",
    "            print(f\"Left node class: {self.left_class}, Right node class: {self.right_class}\")\n",
    "\n",
    "    def _most_common_class(self, y):\n",
    "        \"\"\"Helper function to find the most common class in a node\"\"\"\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return unique[np.argmax(counts)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate the label using the learned decision tree\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Input data to be classified\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        predictions : ndarray, shape (n_samples,)\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        if self.feature_index is None or self.threshold is None:\n",
    "            raise ValueError(\"Model has not been trained yet. Call fit() first.\")\n",
    "        \n",
    "        # \n",
    "        predictions = np.empty(X.shape[0], dtype=type(self.left_class))\n",
    "        \n",
    "        # \n",
    "        left_mask = X[:, self.feature_index] <= self.threshold\n",
    "        \n",
    "        #\n",
    "        predictions[left_mask] = self.left_class\n",
    "        predictions[~left_mask] = self.right_class\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Return the decision path information (for debugging/analysis)\n",
    "        Returns which leaf node each sample would reach\n",
    "        \"\"\"\n",
    "        if self.feature_index is None or self.threshold is None:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "            \n",
    "        decisions = np.where(X[:, self.feature_index] <= self.threshold, \n",
    "                           \"Left\", \"Right\")\n",
    "        return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#\n",
    "X, y = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    weights=[0.7, 0.3],\n",
    "    random_state=42,\n",
    "    flip_y=0.1  \n",
    ")\n",
    "\n",
    "#feature names \n",
    "feature_names = [\"Feature A\", \"Feature B\"]\n",
    "class_names = [\"Class 0\", \"Class 1\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# \n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', edgecolor='k', s=30)\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.title(\"Training Data Visualization\")\n",
    "plt.colorbar(label=\"Class\")\n",
    "plt.show()\n",
    "\n",
    "# Initialize and train scratch implementation\n",
    "scratch_tree = ScratchDecesionTreeClassifierDepth1(verbose=True)\n",
    "scratch_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "scratch_pred = scratch_tree.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "scratch_accuracy = accuracy_score(y_test, scratch_pred)\n",
    "scratch_precision = precision_score(y_test, scratch_pred)\n",
    "scratch_recall = recall_score(y_test, scratch_pred)\n",
    "\n",
    "print(\"\\nScratch Implementation Metrics:\")\n",
    "print(f\"Accuracy:  {scratch_accuracy:.4f}\")\n",
    "print(f\"Precision: {scratch_precision:.4f}\")\n",
    "print(f\"Recall:    {scratch_recall:.4f}\")\n",
    "\n",
    "# Initialize and train scikit-learn's implementation (max_depth=1)\n",
    "sklearn_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "sklearn_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "sklearn_pred = sklearn_tree.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_pred)\n",
    "sklearn_precision = precision_score(y_test, sklearn_pred)\n",
    "sklearn_recall = recall_score(y_test, sklearn_pred)\n",
    "\n",
    "print(\"\\nscikit-learn Implementation Metrics:\")\n",
    "print(f\"Accuracy:  {sklearn_accuracy:.4f}\")\n",
    "print(f\"Precision: {sklearn_precision:.4f}\")\n",
    "print(f\"Recall:    {sklearn_recall:.4f}\")\n",
    "\n",
    "# Compare the splits\n",
    "print(\"\\nComparison of Splits:\")\n",
    "print(f\"Scratch - Split on {feature_names[scratch_tree.feature_index]} <= {scratch_tree.threshold:.4f}\")\n",
    "print(f\"scikit-learn - Split on {feature_names[sklearn_tree.tree_.feature[0]]} <= {sklearn_tree.tree_.threshold[0]:.4f}\")\n",
    "\n",
    "# decision boundaries\n",
    "def plot_decision_boundary(clf, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='bwr')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', s=30)\n",
    "    plt.xlabel(feature_names[0])\n",
    "    plt.ylabel(feature_names[1])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "#both decision boundaries\n",
    "plot_decision_boundary(scratch_tree, X_train, y_train, \"Scratch Implementation Decision Boundary\")\n",
    "plot_decision_boundary(sklearn_tree, X_train, y_train, \"scikit-learn Implementation Decision Boundary\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    \"\"\"Node class for our decision tree\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None, current_depth=0):\n",
    "        self.max_depth = max_depth\n",
    "        self.current_depth = current_depth\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.class_label = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Recursively fit the decision tree\"\"\"\n",
    "        # Base cases\n",
    "        if len(np.unique(y)) == 1:  # All samples same class\n",
    "            self.class_label = y[0]\n",
    "            return\n",
    "        \n",
    "        if (self.max_depth is not None and \n",
    "            self.current_depth >= self.max_depth):\n",
    "            self.class_label = self._most_common_class(y)\n",
    "            return\n",
    "        \n",
    "        # Find best split\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = information_gain(y, y[left_mask], y[right_mask])\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        # If no good split found, become leaf node\n",
    "        if best_gain == -1:\n",
    "            self.class_label = self._most_common_class(y)\n",
    "            return\n",
    "        \n",
    "        # Store best split\n",
    "        self.feature_index = best_feature\n",
    "        self.threshold = best_threshold\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        #child nodes\n",
    "        self.left_child = DecisionTreeNode(\n",
    "            max_depth=self.max_depth,\n",
    "            current_depth=self.current_depth + 1\n",
    "        )\n",
    "        self.right_child = DecisionTreeNode(\n",
    "            max_depth=self.max_depth,\n",
    "            current_depth=self.current_depth + 1\n",
    "        )\n",
    "        \n",
    "        # \n",
    "        self.left_child.fit(X[left_mask], y[left_mask])\n",
    "        self.right_child.fit(X[right_mask], y[right_mask])\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Recursively predict class for a single sample\"\"\"\n",
    "        if self.class_label is not None:\n",
    "            return self.class_label\n",
    "            \n",
    "        if x[self.feature_index] <= self.threshold:\n",
    "            return self.left_child.predict(x)\n",
    "        else:\n",
    "            return self.right_child.predict(x)\n",
    "    \n",
    "    def _most_common_class(self, y):\n",
    "        \"\"\"Helper method to find majority class\"\"\"\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return unique[np.argmax(counts)]\n",
    "\n",
    "\n",
    "class ScratchDecesionTreeClassifierDepth2:\n",
    "    \"\"\"Depth 2 decision tree classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the decision tree with max_depth=2\"\"\"\n",
    "        self.root = DecisionTreeNode(max_depth=2, current_depth=0)\n",
    "        self.root.fit(X, y)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Decision tree trained with depth 2\")\n",
    "            self.print_tree()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples\"\"\"\n",
    "        return np.array([self.root.predict(x) for x in X])\n",
    "    \n",
    "    def print_tree(self, node=None, indent=\"\", feature_names=None):\n",
    "        \"\"\"Print the tree structure (for visualization)\"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "            if feature_names is None:\n",
    "                feature_names = [f\"Feature {i}\" for i in range(len(node.feature_index))] \\\n",
    "                    if hasattr(node, 'feature_index') and node.feature_index is not None else []\n",
    "        \n",
    "        if node.class_label is not None:\n",
    "            print(indent + \"Predict class\", node.class_label)\n",
    "        else:\n",
    "            feature_name = feature_names[node.feature_index] if feature_names else f\"Feature {node.feature_index}\"\n",
    "            print(indent + f\"{feature_name} <= {node.threshold:.2f}\")\n",
    "            self.print_tree(node.left_child, indent + \"  \", feature_names)\n",
    "            print(indent + f\"{feature_name} > {node.threshold:.2f}\")\n",
    "            self.print_tree(node.right_child, indent + \"  \", feature_names)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDecesionTreeClassifierDepthInf:\n",
    "    \"\"\"\n",
    "    Decision tree classifier with unlimited depth capability\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth : int or None, default=None\n",
    "        The maximum depth of the tree. If None, nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "    min_samples_split : int, default=2\n",
    "        The minimum number of samples required to split an internal node\n",
    "    verbose : bool, default=False\n",
    "        If True, prints training progress\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, verbose=False):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.verbose = verbose\n",
    "        self.root = None\n",
    "        self.feature_importances_ = None\n",
    "    \n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        \"\"\"\n",
    "        Build a decision tree classifier from the training set (X, y)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target values\n",
    "        feature_names : list of str, optional\n",
    "            Names of features for better visualization\n",
    "        \"\"\"\n",
    "        self.feature_names = feature_names if feature_names is not None else [\n",
    "            f\"Feature {i}\" for i in range(X.shape[1])\n",
    "        ]\n",
    "        \n",
    "        # \n",
    "        self.feature_importances_ = np.zeros(X.shape[1])\n",
    "        \n",
    "        # \n",
    "        self.root = self._build_tree(X, y, current_depth=0)\n",
    "        \n",
    "        # \n",
    "        self.feature_importances_ /= np.sum(self.feature_importances_)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Decision tree training completed\")\n",
    "            self.print_tree()\n",
    "    \n",
    "    def _build_tree(self, X, y, current_depth):\n",
    "        \"\"\"Recursively build the decision tree\"\"\"\n",
    "        node = {\n",
    "            'samples': len(y),\n",
    "            'value': np.bincount(y),\n",
    "            'impurity': gini_impurity(y),\n",
    "            'depth': current_depth\n",
    "        }\n",
    "        \n",
    "        # \n",
    "        if (len(np.unique(y)) == 1 or  # All samples same class\n",
    "            (self.max_depth is not None and current_depth >= self.max_depth) or\n",
    "            len(y) < self.min_samples_split):\n",
    "            node['class'] = np.argmax(node['value'])\n",
    "            return node\n",
    "        \n",
    "        # \n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split['gain'] <= 0:  # No improvement\n",
    "            node['class'] = np.argmax(node['value'])\n",
    "            return node\n",
    "        \n",
    "        # Store best split\n",
    "        node.update(best_split)\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, best_split['feature']] <= best_split['threshold']\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        # \n",
    "        node['left'] = self._build_tree(X[left_mask], y[left_mask], current_depth + 1)\n",
    "        node['right'] = self._build_tree(X[right_mask], y[right_mask], current_depth + 1)\n",
    "        \n",
    "        # \n",
    "        self.feature_importances_[best_split['feature']] += (\n",
    "            best_split['gain'] * best_split['samples']\n",
    "        )\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"Find the best split for a node\"\"\"\n",
    "        best_split = {'gain': -1}\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = information_gain(y, y[left_mask], y[right_mask])\n",
    "                \n",
    "                if gain > best_split['gain']:\n",
    "                    best_split = {\n",
    "                        'feature': feature,\n",
    "                        'threshold': threshold,\n",
    "                        'gain': gain,\n",
    "                        'samples': n_samples\n",
    "                    }\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class for X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The predicted classes\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_one(x) for x in X])\n",
    "    \n",
    "    def _predict_one(self, x, node=None):\n",
    "        \"\"\"Predict class for a single sample\"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        \n",
    "        if 'class' in node:\n",
    "            return node['class']\n",
    "        \n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self._predict_one(x, node['left'])\n",
    "        else:\n",
    "            return self._predict_one(x, node['right'])\n",
    "    \n",
    "    def print_tree(self, node=None, indent=\"\"):\n",
    "        \"\"\"Print the tree structure\"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        \n",
    "        if 'class' in node:\n",
    "            print(indent + f\"Class: {node['class']}\", \n",
    "                  f\"(samples={node['samples']}, value={node['value']}, impurity={node['impurity']:.3f})\")\n",
    "        else:\n",
    "            feature_name = self.feature_names[node['feature']]\n",
    "            print(indent + f\"{feature_name} <= {node['threshold']:.3f}\",\n",
    "                  f\"(samples={node['samples']}, value={node['value']}, impurity={node['impurity']:.3f}, gain={node['gain']:.3f})\")\n",
    "            self.print_tree(node['left'], indent + \"  \")\n",
    "            print(indent + f\"{feature_name} > {node['threshold']:.3f}\")\n",
    "            self.print_tree(node['right'], indent + \"  \")\n",
    "    \n",
    "    def get_depth(self, node=None):\n",
    "        \"\"\"Get the maximum depth of the tree\"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        \n",
    "        if 'class' in node:\n",
    "            return node['depth']\n",
    "        \n",
    "        return max(\n",
    "            self.get_depth(node['left']),\n",
    "            self.get_depth(node['right'])\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
