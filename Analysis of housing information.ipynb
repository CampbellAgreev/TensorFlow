{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the the dowloaded data\n",
    "df = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\train.csv\")\n",
    "\n",
    "#first few rows of the dataframe to check\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"House Prices: Advanced Regression Techniques\" dataset from Kaggle is designed for practicing feature engineering and applying advanced regression techniques to predict housing prices. It contains 79 explanatory variables that describe various aspects of residential homes in Ames, Iowa. These variables cover a wide range of features, including:\n",
    "\n",
    "- Structural: Overall quality, size, and condition of the house.\n",
    "- Functional: Design and layout aspects.\n",
    "- Exterior  : Materials and finish of the house's exterior.\n",
    "- Interior: Details about the interior finishes and features.\n",
    "- Location: Geographical and neighborhood information.\n",
    "- Utilities: Types of available utilities.\n",
    "- Garage: Garage size, condition, and features.\n",
    "- Basement: Information about the basement, if present.\n",
    "- Lot: Lot size and related features.\n",
    "- Pool: Presence and quality of a pool, if any.\n",
    "- Miscellaneous: Other features like outbuildings or additional amenities.\n",
    "\n",
    "This comprehensive dataset provides a rich foundation for practicing data preprocessing, feature selection, and applying various regression models to predict house sale prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the data types of each column\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.info() gives us an overview of the dataset, showing the data types of each column. For example, it will indicate if a column is numerical (like int64 or float64) or categorical (like object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays column names to identify the target variable\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.columns will return the list of all column names. Based on the problem description, we can identify that the column SalePrice is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays mean, standard deviation, and quartiles of numerical features\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.describe() will give us summary statistics for all numerical columns in the dataset. This includes the mean, standard deviation, minimum, maximum, and the 25th, 50th, and 75th percentiles (quartiles) for each numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install missingno\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\train.csv\")\n",
    "\n",
    "#Visualize missing values\n",
    "msno.matrix(df)  # Visualize missing values in the dataset\n",
    "\n",
    "#Calculate and display the percentage of missing values for each column\n",
    "missing_percentage = df.isnull().mean() * 100\n",
    "print(\"Percentage of missing values in each column:\")\n",
    "print(missing_percentage)\n",
    "\n",
    "#Drop columns with 5 or more missing values\n",
    "df_cleaned = df.dropna(axis=1, thresh=len(df) - 5)\n",
    "print(\"\\nColumns after removing those with 5 or more missing values:\")\n",
    "print(df_cleaned.columns)\n",
    "\n",
    "#Drop rows with any missing values\n",
    "df_cleaned = df_cleaned.dropna(axis=0)\n",
    "\n",
    "# Check if any missing values remain\n",
    "print(\"\\nRemaining missing values after dropping columns and rows:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis measures the \"tailedness\" of a distribution, showing whether data has outliers (heavy tails or light tails) and the peak's height.\n",
    "\n",
    "Skewness measures the asymmetry of a distribution, showing if the data tends to have a long tail on the right (positive skew) or left (negative skew).\n",
    "\n",
    "Both kurtosis and skewness provide valuable insights into the shape of a data distribution, helping in understanding the underlying nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import numpy as np\n",
    "\n",
    "# Loading dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\train.csv\")\n",
    "\n",
    "#plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Target variable\n",
    "target = df['SalePrice']\n",
    "\n",
    "#original distribution using displot and histplot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "#displot\n",
    "sns.displot(target, kde=True)\n",
    "plt.title(\"Original Distribution of SalePrice (displot)\")\n",
    "plt.xlabel(\"SalePrice\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "#histplot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(target, kde=True, bins=30)\n",
    "plt.title(\"Original Distribution of SalePrice (histplot)\")\n",
    "plt.xlabel(\"SalePrice\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Calculating kurtosis and skewness\n",
    "orig_kurtosis = kurtosis(target)\n",
    "orig_skewness = skew(target)\n",
    "print(f\"Original Kurtosis: {orig_kurtosis:.2f}\")\n",
    "print(f\"Original Skewness: {orig_skewness:.2f}\")\n",
    "\n",
    "#Log transformation\n",
    "log_target = np.log1p(target)  # log(1 + x) to avoid log(0)\n",
    "\n",
    "#log-transformed distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "#displot\n",
    "sns.displot(log_target, kde=True)\n",
    "plt.title(\"Log-Transformed Distribution of SalePrice (displot)\")\n",
    "plt.xlabel(\"Log(SalePrice)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "#histplot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(log_target, kde=True, bins=30)\n",
    "plt.title(\"Log-Transformed Distribution of SalePrice (histplot)\")\n",
    "plt.xlabel(\"Log(SalePrice)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "#kurtosis and skewness for log-transformed data\n",
    "log_kurtosis = kurtosis(log_target)\n",
    "log_skewness = skew(log_target)\n",
    "print(f\"Log-Transformed Kurtosis: {log_kurtosis:.2f}\")\n",
    "print(f\"Log-Transformed Skewness: {log_skewness:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before Log Transformation:\n",
    "\n",
    "The distribution of SalePrice is right-skewed (positively skewed).\n",
    "\n",
    "This means most house prices are lower, but a few very high-priced houses stretch the tail to the right.\n",
    "\n",
    "Skewness and kurtosis are both high, indicating the data is not normally distributed and has many outliers.\n",
    "\n",
    "After Log Transformation:\n",
    "\n",
    "The distribution becomes much closer to a normal distribution (bell-shaped curve).\n",
    "\n",
    "Skewness and kurtosis decrease significantly.\n",
    "\n",
    "This helps normalize the target variable, which improves the performance of many machine learning algorithms (especially linear regression) that assume normality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "#correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title(\"Correlation Heatmap (Numeric Features Only)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# correlations with SalePrice\n",
    "target_corr = corr_matrix['SalePrice'].abs().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "print(target_corr.head(11))\n",
    "\n",
    "#top 10 features excluding 'SalePrice' itself\n",
    "top_features = target_corr.index[1:11]\n",
    "print(\"\\nTop 10 features most correlated with SalePrice:\\n\", top_features.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# Correlation matrix for selected features\n",
    "selected_corr = df[top_features.tolist() + ['SalePrice']].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(selected_corr, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
    "plt.title(\"Top 10 Correlated Features + SalePrice Heatmap\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing What the Top Features Represent from Kaggle Data Description\n",
    "\n",
    "Feature             \tDescription\n",
    "\n",
    "OverallQual \t -      Overall material and finish quality\n",
    "\n",
    "GrLivArea\t     -      Above ground living area in square feet\n",
    "\n",
    "GarageCars\t     -      Size of garage in car capacity\n",
    "\n",
    "GarageArea\t     -      Size of garage in square feet\n",
    "\n",
    "TotalBsmtSF\t     -      Total basement area in square feet\n",
    "\n",
    "1stFlrSF\t     -      First Floor square feet\n",
    "\n",
    "FullBath\t     -      Full bathrooms above grade\n",
    "\n",
    "TotRmsAbvGrd\t -      Total rooms above grade (does not include bathrooms)\n",
    "\n",
    "YearBuilt\t     -      Original construction date\n",
    "\n",
    "YearRemodAdd\t -      Remodel date (same as construction date if no remodeling)\n",
    "\n",
    "These features mostly represent the size, quality, and age of the house — all factors that logically affect the SalePrice. For instance, higher overall quality, more square footage, and newer construction/remodel years typically command higher prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for \"high\" correlation\n",
    "threshold = 0.8\n",
    "\n",
    "#correlation matrix of the top features\n",
    "top_corr_pairs = selected_corr[top_features].unstack().sort_values(ascending=False)\n",
    "\n",
    "# Removing self-correlation and duplicate pairs\n",
    "top_corr_pairs = top_corr_pairs[top_corr_pairs < 1]\n",
    "top_corr_pairs = top_corr_pairs[::2]  # drop duplicates\n",
    "\n",
    "#top 3 correlated feature pairs\n",
    "high_corr_pairs = top_corr_pairs[top_corr_pairs > threshold]\n",
    "print(\"\\nHighly correlated feature pairs (r > 0.8):\")\n",
    "print(high_corr_pairs.head(3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
