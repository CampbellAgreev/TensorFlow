{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully Connected Layer\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "        Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "        Number of nodes in the next layer\n",
    "    initializer : instance of weight initialization class\n",
    "    optimizer : instance of optimizer class (e.g., SGD)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialize weights (W) and biases (B)\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through fully connected layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (batch_size, n_nodes1)\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of shape (batch_size, n_nodes2)\n",
    "        \"\"\"\n",
    "        self.X = X  \n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of shape (batch_size, n_nodes2)\n",
    "            Gradient from next layer\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray of shape (batch_size, n_nodes1)\n",
    "            Gradient to pass to previous layer\n",
    "        \"\"\"\n",
    "        # Gradients\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "\n",
    "        #weights\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray of shape (n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * self.sigma\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray of shape (n_nodes2,)\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent (SGD)\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a given layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : instance of FC (Fully Connected Layer)\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : updated layer instance\n",
    "        \"\"\"\n",
    "        #parameter update\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        dA[self.mask] = 0\n",
    "        return dA\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        self.out = 1 / (1 + np.exp(-X))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        return dA * self.out * (1 - self.out)\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, X):\n",
    "        self.out = np.tanh(X)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        return dA * (1 - self.out ** 2)\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        X = X - np.max(X, axis=1, keepdims=True)  # for stability\n",
    "        exp_X = np.exp(X)\n",
    "        self.out = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dA, Y):\n",
    "        \"\"\"\n",
    "        Y: one-hot true labels\n",
    "        Returns gradient of loss w.r.t input to softmax\n",
    "        \"\"\"\n",
    "        batch_size = Y.shape[0]\n",
    "        return (self.out - Y) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Apply ReLU activation function: f(x) = max(0, x)\n",
    "        Save mask of where input is less than or equal to zero for backward.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (batch_size, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        out : ndarray, shape (batch_size, n_features)\n",
    "        \"\"\"\n",
    "        self.mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward pass for ReLU.\n",
    "        Gradient only passes through where input was > 0.\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_features)\n",
    "            Gradient from next layer\n",
    "        Returns\n",
    "        -------\n",
    "        dZ : ndarray, shape (batch_size, n_features)\n",
    "        \"\"\"\n",
    "        dA[self.mask] = 0\n",
    "        return dA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavier (Glorot) Initializer for sigmoid/tanh\n",
    "    σ = 1 / sqrt(n)\n",
    "    \"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        sigma = 1 / np.sqrt(n_nodes1)\n",
    "        return np.random.randn(n_nodes1, n_nodes2) * sigma\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.zeros(n_nodes2)\n",
    "\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    He Initializer for ReLU\n",
    "    σ = sqrt(2 / n)\n",
    "    \"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        sigma = np.sqrt(2 / n_nodes1)\n",
    "        return np.random.randn(n_nodes1, n_nodes2) * sigma\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        return np.zeros(n_nodes2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad optimizer\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Initial learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.epsilon = 1e-7  # To avoid division by zero\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases using AdaGrad\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : instance of FC\n",
    "        \"\"\"\n",
    "        # Initialize accumulators\n",
    "        if not hasattr(layer, 'h_W'):\n",
    "            layer.h_W = np.zeros_like(layer.W)\n",
    "            layer.h_B = np.zeros_like(layer.B)\n",
    "\n",
    "        # Accumulate squared gradients\n",
    "        layer.h_W += layer.dW ** 2\n",
    "        layer.h_B += layer.dB ** 2\n",
    "\n",
    "        # Update with AdaGrad rule\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(layer.h_W) + self.epsilon)\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(layer.h_B) + self.epsilon)\n",
    "\n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    def __init__(self, n_epochs=10, batch_size=32, lr=0.01, sigma=0.01,\n",
    "                 n_nodes_list=[64, 32], n_output=10,\n",
    "                 initializer=None, optimizer=None,\n",
    "                 activations=None, verbose=True):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.n_nodes_list = n_nodes_list\n",
    "        self.n_output = n_output\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activations = activations\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.layers = []\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self._build_network(X.shape[1], y.shape[1])\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Shuffle the dataset\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X_array = X\n",
    "                y_array = y\n",
    "            else:\n",
    "                #\n",
    "                X_array = X.to_numpy()\n",
    "                y_array = y.to_numpy()\n",
    "\n",
    "            perm = np.random.permutation(len(X_array))\n",
    "            X_shuffled = X_array[perm] \n",
    "            y_shuffled = y_array[perm]  \n",
    "\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X), self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                output = self._forward(X_batch)\n",
    "\n",
    "                # Loss calculation\n",
    "                loss = self._cross_entropy(output, y_batch)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Backward pass\n",
    "                self._backward(output, y_batch)\n",
    "\n",
    "            self.loss_train.append(epoch_loss / (len(X) // self.batch_size))\n",
    "\n",
    "            # Validation\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_output = self._forward(X_val)\n",
    "                val_loss = self._cross_entropy(val_output, y_val)\n",
    "                self.loss_val.append(val_loss)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs}, Train Loss: {self.loss_train[-1]:.4f}\" + \n",
    "                      (f\", Val Loss: {self.loss_val[-1]:.4f}\" if self.loss_val else \"\"))\n",
    "\n",
    "        self._plot_learning_curve()\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self._forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def _build_network(self, n_features, n_classes):\n",
    "        self.layers = []\n",
    "        layer_sizes = [n_features] + self.n_nodes_list + [n_classes]\n",
    "\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            fc = FC(layer_sizes[i], layer_sizes[i+1], self.initializer, self.optimizer)\n",
    "            self.layers.append(fc)\n",
    "            ####\n",
    "            if i < len(self.n_nodes_list):\n",
    "                self.layers.append(self.activations[i])\n",
    "            else:\n",
    "                self.layers.append(Softmax())\n",
    "\n",
    "    def _forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def _backward(self, output, y_true):\n",
    "        grad = self.layers[-1].backward(output, y_true)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def _cross_entropy(self, y_pred, y_true):\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-7), axis=1))\n",
    "\n",
    "    def _plot_learning_curve(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.loss_train, label='Train Loss')\n",
    "        if self.loss_val:\n",
    "            plt.plot(self.loss_val, label='Validation Loss')\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "c:\\Users\\Admin\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 2 Layers (ReLU)...\n",
      "Epoch 1/10, Train Loss: 0.9787, Val Loss: 0.5473, Train Acc: 0.8666, Val Acc: 0.8639\n",
      "Epoch 2/10, Train Loss: 0.4590, Val Loss: 0.4180, Train Acc: 0.8910, Val Acc: 0.8886\n",
      "Epoch 3/10, Train Loss: 0.3786, Val Loss: 0.3709, Train Acc: 0.9026, Val Acc: 0.8991\n",
      "Epoch 4/10, Train Loss: 0.3410, Val Loss: 0.3422, Train Acc: 0.9089, Val Acc: 0.9059\n",
      "Epoch 5/10, Train Loss: 0.3177, Val Loss: 0.3235, Train Acc: 0.9142, Val Acc: 0.9119\n",
      "Epoch 6/10, Train Loss: 0.2989, Val Loss: 0.3088, Train Acc: 0.9172, Val Acc: 0.9154\n",
      "Epoch 7/10, Train Loss: 0.2853, Val Loss: 0.2979, Train Acc: 0.9206, Val Acc: 0.9183\n",
      "Epoch 8/10, Train Loss: 0.2730, Val Loss: 0.2903, Train Acc: 0.9235, Val Acc: 0.9200\n",
      "Epoch 9/10, Train Loss: 0.2626, Val Loss: 0.2771, Train Acc: 0.9271, Val Acc: 0.9234\n",
      "Epoch 10/10, Train Loss: 0.2529, Val Loss: 0.2694, Train Acc: 0.9294, Val Acc: 0.9256\n",
      "2 Layers (ReLU) - Test Accuracy: 0.9261\n",
      "\n",
      "Training 3 Layers (ReLU)...\n",
      "Epoch 1/10, Train Loss: 0.9076, Val Loss: 0.4570, Train Acc: 0.8842, Val Acc: 0.8782\n",
      "Epoch 2/10, Train Loss: 0.3808, Val Loss: 0.3496, Train Acc: 0.9064, Val Acc: 0.9000\n",
      "Epoch 3/10, Train Loss: 0.3109, Val Loss: 0.3168, Train Acc: 0.9138, Val Acc: 0.9072\n",
      "Epoch 4/10, Train Loss: 0.2743, Val Loss: 0.2815, Train Acc: 0.9260, Val Acc: 0.9194\n",
      "Epoch 5/10, Train Loss: 0.2498, Val Loss: 0.2620, Train Acc: 0.9309, Val Acc: 0.9255\n",
      "Epoch 6/10, Train Loss: 0.2309, Val Loss: 0.2498, Train Acc: 0.9348, Val Acc: 0.9272\n",
      "Epoch 7/10, Train Loss: 0.2136, Val Loss: 0.2285, Train Acc: 0.9421, Val Acc: 0.9344\n",
      "Epoch 8/10, Train Loss: 0.1993, Val Loss: 0.2180, Train Acc: 0.9464, Val Acc: 0.9385\n",
      "Epoch 9/10, Train Loss: 0.1873, Val Loss: 0.2113, Train Acc: 0.9495, Val Acc: 0.9395\n",
      "Epoch 10/10, Train Loss: 0.1767, Val Loss: 0.2002, Train Acc: 0.9520, Val Acc: 0.9432\n",
      "3 Layers (ReLU) - Test Accuracy: 0.9461\n",
      "\n",
      "Training 2 Layers (Tanh)...\n",
      "Epoch 1/10, Train Loss: 0.9291, Val Loss: 0.5665, Train Acc: 0.8606, Val Acc: 0.8590\n",
      "Epoch 2/10, Train Loss: 0.4855, Val Loss: 0.4449, Train Acc: 0.8838, Val Acc: 0.8793\n",
      "Epoch 3/10, Train Loss: 0.4066, Val Loss: 0.3963, Train Acc: 0.8942, Val Acc: 0.8895\n",
      "Epoch 4/10, Train Loss: 0.3681, Val Loss: 0.3678, Train Acc: 0.9012, Val Acc: 0.8971\n",
      "Epoch 5/10, Train Loss: 0.3441, Val Loss: 0.3494, Train Acc: 0.9059, Val Acc: 0.9014\n",
      "Epoch 6/10, Train Loss: 0.3263, Val Loss: 0.3368, Train Acc: 0.9092, Val Acc: 0.9059\n",
      "Epoch 7/10, Train Loss: 0.3129, Val Loss: 0.3242, Train Acc: 0.9126, Val Acc: 0.9088\n",
      "Epoch 8/10, Train Loss: 0.3016, Val Loss: 0.3150, Train Acc: 0.9154, Val Acc: 0.9109\n",
      "Epoch 9/10, Train Loss: 0.2921, Val Loss: 0.3064, Train Acc: 0.9181, Val Acc: 0.9134\n",
      "Epoch 10/10, Train Loss: 0.2839, Val Loss: 0.2990, Train Acc: 0.9201, Val Acc: 0.9152\n",
      "2 Layers (Tanh) - Test Accuracy: 0.9179\n",
      "\n",
      "Final Results:\n",
      "2 Layers (ReLU): Test Accuracy = 0.9261\n",
      "3 Layers (ReLU): Test Accuracy = 0.9461\n",
      "2 Layers (Tanh): Test Accuracy = 0.9179\n",
      "\n",
      "Visualizing predictions from best model: 3 Layers (ReLU)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#activation functions\n",
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    def gradient(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def gradient(self, x):\n",
    "        s = self.__call__(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        return np.tanh(x)\n",
    "    def gradient(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "#initializers\n",
    "class HeInitializer:\n",
    "    def __call__(self, shape):\n",
    "        fan_in = shape[0]\n",
    "        return np.random.randn(*shape) * np.sqrt(2.0 / fan_in)\n",
    "\n",
    "class XavierInitializer:\n",
    "    def __call__(self, shape):\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "        limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        return np.random.uniform(-limit, limit, size=shape)\n",
    "\n",
    "#optimizer\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "# NN Implementation\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activations, initializer=HeInitializer(), optimizer=SGD(), \n",
    "                 learning_rate=0.01, batch_size=32, epochs=10, verbose=True):\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            self.weights.append(self.initializer((self.layers[i], self.layers[i+1])))\n",
    "            self.biases.append(np.zeros((1, self.layers[i+1])))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache = {'A0': X}\n",
    "        A = X\n",
    "        for i in range(len(self.weights)-1):\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            A = self.activations[i](Z)\n",
    "            self.cache[f'Z{i+1}'] = Z\n",
    "            self.cache[f'A{i+1}'] = A\n",
    "        \n",
    "        # Output layer (softmax)\n",
    "        Z = np.dot(A, self.weights[-1]) + self.biases[-1]\n",
    "        A = self.softmax(Z)\n",
    "        self.cache[f'Z{len(self.weights)}'] = Z\n",
    "        self.cache[f'A{len(self.weights)}'] = A\n",
    "        return A\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-10)) / m\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        grads = {'weights': [], 'biases': []}\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dZ = self.cache[f'A{len(self.weights)}'] - y_true\n",
    "        grads['weights'].insert(0, np.dot(self.cache[f'A{len(self.weights)-1}'].T, dZ) / m)\n",
    "        grads['biases'].insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        # Hidden layers gradients\n",
    "        for i in reversed(range(len(self.weights)-1)):\n",
    "            dA = np.dot(dZ, self.weights[i+1].T)\n",
    "            dZ = dA * self.activations[i].gradient(self.cache[f'Z{i+1}'])\n",
    "            grads['weights'].insert(0, np.dot(self.cache[f'A{i}'].T, dZ) / m)\n",
    "            grads['biases'].insert(0, np.sum(dZ, axis=0, keepdims=True) / m)\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads['weights'][i]\n",
    "            self.biases[i] -= self.learning_rate * grads['biases'][i]\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        X = X.values if hasattr(X, 'values') else X\n",
    "        y = y.values if hasattr(y, 'values') else y\n",
    "        \n",
    "        self.initialize_parameters()\n",
    "        self.optimizer.lr = self.learning_rate\n",
    "        \n",
    "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle data\n",
    "            perm = np.random.permutation(len(X))\n",
    "            X_shuffled = X[perm]\n",
    "            y_shuffled = y[perm]\n",
    "            \n",
    "            batch_losses = []\n",
    "            for i in range(0, len(X), self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "                loss = self.compute_loss(y_batch, y_pred)\n",
    "                batch_losses.append(loss)\n",
    "                \n",
    "                # Backward pass\n",
    "                grads = self.backward(y_batch)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(grads)\n",
    "            \n",
    "            #metrics\n",
    "            train_loss = np.mean(batch_losses)\n",
    "            train_pred = np.argmax(self.forward(X), axis=1)\n",
    "            train_acc = accuracy_score(np.argmax(y, axis=1), train_pred)\n",
    "            \n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                val_pred = self.forward(X_val)\n",
    "                val_loss = self.compute_loss(y_val, val_pred)\n",
    "                val_acc = accuracy_score(np.argmax(y_val, axis=1), np.argmax(val_pred, axis=1))\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_acc'].append(val_acc)\n",
    "            \n",
    "            if self.verbose:\n",
    "                if X_val is not None:\n",
    "                    print(f\"Epoch {epoch+1}/{self.epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1}/{self.epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = X.values if hasattr(X, 'values') else X\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "#MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X = mnist.data.astype('float32') / 255.0\n",
    "y = mnist.target.astype('int')\n",
    "\n",
    "#\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Train/val/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "#network configurations\n",
    "configurations = [\n",
    "    {\n",
    "        'name': '2 Layers (ReLU)',\n",
    "        'layers': [784, 128, 10],\n",
    "        'activations': [ReLU(), ReLU()],\n",
    "        'initializer': HeInitializer(),\n",
    "        'optimizer': SGD(lr=0.01),\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64\n",
    "    },\n",
    "    {\n",
    "        'name': '3 Layers (ReLU)',\n",
    "        'layers': [784, 256, 128, 10],\n",
    "        'activations': [ReLU(), ReLU(), ReLU()],\n",
    "        'initializer': HeInitializer(),\n",
    "        'optimizer': SGD(lr=0.01),\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64\n",
    "    },\n",
    "    {\n",
    "        'name': '2 Layers (Tanh)',\n",
    "        'layers': [784, 128, 10],\n",
    "        'activations': [Tanh(), Tanh()],\n",
    "        'initializer': XavierInitializer(),\n",
    "        'optimizer': SGD(lr=0.01),\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train and evaluate each configuration\n",
    "results = []\n",
    "for config in configurations:\n",
    "    print(f\"\\nTraining {config['name']}...\")\n",
    "    nn = NeuralNetwork(\n",
    "        layers=config['layers'],\n",
    "        activations=config['activations'],\n",
    "        initializer=config['initializer'],\n",
    "        optimizer=config['optimizer'],\n",
    "        learning_rate=0.01,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    nn.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    #####\n",
    "    y_pred = nn.predict(X_test)\n",
    "    test_acc = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
    "    results.append((config['name'], test_acc))\n",
    "    print(f\"{config['name']} - Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#results\n",
    "print(\"\\nFinal Results:\")\n",
    "for name, acc in results:\n",
    "    print(f\"{name}: Test Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Visualize predictions \n",
    "best_idx = np.argmax([acc for _, acc in results])\n",
    "best_config = configurations[best_idx]\n",
    "print(f\"\\nVisualizing predictions from best model: {best_config['name']}\")\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    layers=best_config['layers'],\n",
    "    activations=best_config['activations'],\n",
    "    initializer=best_config['initializer'],\n",
    "    optimizer=best_config['optimizer'],\n",
    "    learning_rate=0.01,\n",
    "    batch_size=best_config['batch_size'],\n",
    "    epochs=best_config['epochs'],\n",
    "    verbose=False\n",
    ")\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "#some predictions\n",
    "sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    pred = nn.predict(X_test[idx].reshape(1, -1))[0]\n",
    "    true = np.argmax(y_test[idx])\n",
    "    plt.title(f\"P:{pred}\\nT:{true}\", fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
